{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LRR MIT6.036 hw08 colab notebook",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "v-sSs7N4mMiX",
        "Zh4u39OCjLza"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lrraymond13/collab/blob/master/LRR_MIT6_036_hw08_colab_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "_xIaEwCD406A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#MIT 6.036 Spring 2019: Homework 8#\n",
        "\n",
        "This colab notebook provides code and a framework for [homework 8](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week8/week8_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab.\n"
      ]
    },
    {
      "metadata": {
        "id": "2YM-_zLf9Bp-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf code_for_hw8* __MACOSX data .DS_Store\n",
        "!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw08/code_for_hw8.zip\n",
        "!unzip code_for_hw8.zip\n",
        "!mv code_for_hw8/* .\n",
        "\n",
        "from code_for_hw8_oop import *\n",
        "import numpy as np\n",
        "import math as m"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jDXfdkxJ0Kp5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# set random seed for question 3g\n",
        " np.random.seed(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "q9yVU74AuUU-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 1) Batch Normalization\n",
        "No code involved,  <a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week8/week8_homework/\">Refer to Catsoop.</a>"
      ]
    },
    {
      "metadata": {
        "id": "v-sSs7N4mMiX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2) Implementing Mini-batch Gradient Descent and Batch Normalization (OPTIONAL)\n",
        "\n",
        "** Note: You can click the arrow on the left of this text block to collapse/expand this optional section and all its code blocks **\n",
        "\n",
        "Last week we implemented a framework for building neural networks from scratch. We trained our models using *stochastic* gradient descent. In this problem, we explore how we can implement batch normalization as a module `BatchNorm` in our framework. It is the same module which you analyzed in problem 1. "
      ]
    },
    {
      "metadata": {
        "id": "HgxmIfXVmVwd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Key to the concept of batch normalization is the doing gradient descent on batches of data. So we instead of using last week's stochastic gradient descent, we will first implement the *mini-batch* gradient descent method `mini_gd`, which is a hybrid between *stochastic* gradient descent and *batch* gradient descent. The lecture notes on <a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week7/neural_networks_2/1?activate_block_id=block-v1%3AMITx%2B6.036%2B2019_Spring%2Btype%40vertical%2Bblock%40neural_networks_2_optimizing_neural_network_parameters_vert\"> optimizing neural network parameters</a> are helpful for this part.\n",
        "\n",
        "In *mini-batch* gradient descent, for a mini-batch of size $K$, we select $K$ distinct data points uniformly at random from the data set and update the network weights based only on their contributions to the gradient:\n",
        "$$W := W - \\eta\\sum_{i=1}^K \\nabla_W \\mathcal{L}(h(x^{(i)}; W), y^{(i)})\\;\\;.$$\n",
        "\n",
        "Our *mini-batch* method `mini_gd` will be implemented within the `Sequential` python class (see homework 7 problem 2) and will take the following as inputs:\n",
        "\n",
        "* `X`: a standard data array (d by n)\n",
        "* `y`: a standard labels row vector (1 by n)\n",
        "* `iters`: the number of updates to perform on weights $W$\n",
        "* `lrate`: the learning rate used\n",
        "* `K`: the mini-batch size to be used\n",
        "\n",
        "One call of `mini_gd` should call `Sequential.backward` for back-propagation and `Sequential.step` for updating the weights, for a total of `iters` times, using `lrate` as the learning rate. As in our implementation of `sgd` from homework 7, we compute the predicted output for a mini-batch of data with the `Sequential.forward` method. We compute the loss between our predictions and the true labels using the assigned `Sequential.loss` method. (Note that in homework 7, `Sequential.step` was called `Sequential.sgd_step`. While the functionality of the step function is the same, it has been renamed for convenience. The same is true for the `module.step` function of each module we implemented, where applicable.)\n",
        "\n",
        "For picking $K$ unique data points at random from our large data-set for each mini-batch, we will implement the following strategy: we will first shuffle our data points `X` (and associated labels `y`). Then, we get <math>\\frac{n}{k}</math> (rounded down to the nearest integer) different mini-batches by grouping each $K$ consecutive points from this shuffled array. If we end up iterating over all the points but need more mini-batches, we will repeat the shuffling and the batching process."
      ]
    },
    {
      "metadata": {
        "id": "Dr1kWI08mdo4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>2A)</b>You need to fill in the missing code below. We have implemented the shuffling of indices and have provided you with the outer and inner loops."
      ]
    },
    {
      "metadata": {
        "id": "h_lvmO9Z22bH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "** This OPTIONAL problem has you extend your homework 7 implementation for building neural networks. **\n",
        "### PLEASE COPY IN YOUR CODE FROM HOMEWORK 7 TO COMPLEMENT THE CLASSES GIVEN HERE\n",
        "\n",
        "Recall that your implementation from homework 7 included the following classes:\n",
        "    \n",
        "  * Module\n",
        "  * Linear \n",
        "  * Tanh \n",
        "  * ReLU \n",
        "  * SoftMax\n",
        "  * NLL\n",
        "  * Sequential"
      ]
    },
    {
      "metadata": {
        "id": "ovtbqrrSmezE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math as m \n",
        "\n",
        "class Sequential:\n",
        "    def __init__(self, modules, loss):            \n",
        "        self.modules = modules\n",
        "        self.loss = loss\n",
        "\n",
        "    def mini_gd(self, X, Y, iters, lrate, notif_each=None, K=10):\n",
        "        D, N = X.shape\n",
        "\n",
        "        np.random.seed(0)\n",
        "        num_updates = 0\n",
        "        indices = np.arange(N)\n",
        "        while num_updates < iters:\n",
        "\n",
        "            np.random.shuffle(indices)\n",
        "            X = None  # Your code\n",
        "            Y = None  # Your code\n",
        "\n",
        "            for j in range(m.floor(N/K)):\n",
        "                if num_updates >= iters: break\n",
        "\n",
        "                # Implement the main part of mini_gd here\n",
        "                # Your code\n",
        "                \n",
        "                num_updates += 1\n",
        "\n",
        "    def forward(self, Xt):                        \n",
        "        for m in self.modules: Xt = m.forward(Xt)\n",
        "        return Xt\n",
        "\n",
        "    def backward(self, delta):                   \n",
        "        for m in self.modules[::-1]: delta = m.backward(delta)\n",
        "\n",
        "    def step(self, lrate):    \n",
        "        for m in self.modules: m.step(lrate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1JZeeKXkm6YI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>2B)</b> We are now ready to implement batch normalization into our neural network framework! Our module `BatchNorm` will sit between consecutive layers of neurons, such as the $l^{th}$ and $(l+1)^{th}$ layers, acting as a \"corrector\" which allows $W^l$ to change freely, producing outputs $z^l$, but then the module corrects the covariate shift induced in the signals before they reach the $(l+1)^{th}$ layer, converting $z^l$ to $\\widehat{Z}^l$. \n",
        "\n",
        "The following is a summmary what is described in the <a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week7/neural_networks_2/2\">lecture notes</a>, and it should guide your implementation of the module. \n",
        "\n",
        "Any normalization between the $l^{th}$ and $(l+1)^{th}$ layers is done *separately* for each of the $n^l$ input connections leading to the $(l+1)^{th}$ layer. We handle a mini-batch of data of size $K$, and $Z^l$ is $n^l \\times K$, and the output $\\widehat{Z}^l$is of the same shape. \n",
        "\n",
        "We first compute $n^l$ *batchwise* means and\n",
        "standard deviations.  Let $\\mu^l$ be the $n^l \\times 1$ vector (`self.mus`) where\n",
        "$$\\mu^l_i = \\frac{1}{K} \\sum_{j = 1}^K Z^l_{ij}\\;\\;,$$\n",
        "and let $\\sigma^l$ be the $n^l \\times 1$ vector (`self.vars`) where \n",
        "$$\\sigma^l_i = \\sqrt{\\frac{1}{K} \\sum_{j = 1}^K (Z^l_{ij} - \\mu_i)^2}\\;\\;.$$\n",
        "\n",
        "The normalized data `self.norm` is the matrix $\\overline{Z}$, where\n",
        "$$\\overline{Z}^l_{ij} = \\frac{Z^l_{ij} - \\mu^l_i}{\\sigma^l_i + \\epsilon}\\;\\;,$$\n",
        "and where $\\epsilon$ is a very small constant to guard against division by\n",
        "zero. \n",
        "\n",
        "We define weights $G^l$ (`self.G`) and $B^l$ (`self.B`), each being an $n^l \\times 1$ vector, which we use to to shift and scale the outputs:\n",
        "$$\\widehat{Z}^l_{ij} = G^l_i \\overline{Z}^l_{ij} + B^l_i\\;\\;.$$\n",
        "\n",
        "The outputs are finally ready to be passed to the $(l+1)^{th}$ layer.\n",
        "\n",
        "A slight warning (that we will not worry about here) about `BatchNorm` is that during the *test* phase, if the test mini-batch size is too small (imagine we are deploying a neural network that deals with live video frames), then the lack of samples would cause the freshly-calculated $\\mu^l$ and $\\sigma^l$ to be far off from their true values that the module's parameters $G^l$ and $B^l$ were trained to be compatible with. To fix that, people usually compute a running average of $\\mu^l$ and $\\sigma^l$ during training, to be used at test time. We will assume our test mini-batches are large enough.\n",
        "\n",
        "In this problem we only implement the `BatchNorm.forward` and `BatchNorm.step` methods. We provide you with the implementation for `BatchNorm.backward` and the lecture notes contain the details of the derivations. You will need to fill in the missing code below."
      ]
    },
    {
      "metadata": {
        "id": "UlXP26plm8R7",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class BatchNorm(Module):    \n",
        "    def __init__(self, m):\n",
        "        np.random.seed(0)\n",
        "        self.eps = 1e-20\n",
        "        self.m = m  # number of input channels\n",
        "        \n",
        "        # Init learned shifts and scaling factors\n",
        "        self.B = np.zeros([self.m, 1])\n",
        "        self.G = np.random.normal(0, 1.0 * self.m ** (-.5), [self.m, 1])\n",
        "        \n",
        "    # Works on m x b matrices of m input channels and b different inputs\n",
        "    def forward(self, A):# A is m x K: m input channels and mini-batch size K\n",
        "        # Store last inputs and K for next backward() call\n",
        "        self.A = A\n",
        "        self.K = A.shape[1]\n",
        "        \n",
        "        self.mus = None  # Your Code\n",
        "        self.vars = None  # Your Code\n",
        "\n",
        "        # Normalize inputs using their mean and standard deviation\n",
        "        self.norm = None  # Your Code\n",
        "            \n",
        "        # Return scaled and shifted versions of self.norm\n",
        "        return None  # Your Code\n",
        "\n",
        "    def backward(self, dLdZ):\n",
        "        # Re-usable constants\n",
        "        std_inv = 1/np.sqrt(self.vars+self.eps)\n",
        "        A_min_mu = self.A-self.mus\n",
        "        \n",
        "        dLdnorm = dLdZ * self.G\n",
        "        dLdVar = np.sum(dLdnorm * A_min_mu * -0.5 * std_inv**3, axis=1, keepdims=True)\n",
        "        dLdMu = np.sum(dLdnorm*(-std_inv), axis=1, keepdims=True) + dLdVar * (-2/self.K) * np.sum(A_min_mu, axis=1, keepdims=True)\n",
        "        dLdX = (dLdnorm * std_inv) + (dLdVar * (2/self.K) * A_min_mu) + (dLdMu/self.K)\n",
        "        \n",
        "        self.dLdB = np.sum(dLdZ, axis=1, keepdims=True)\n",
        "        self.dLdG = np.sum(dLdZ * self.norm, axis=1, keepdims=True)\n",
        "        return dLdX\n",
        "\n",
        "    def step(self, lrate):\n",
        "        self.B = None  # Your Code\n",
        "        self.G = None  # Your Code\n",
        "        return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "65LKUAHD_77Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3) 2D Datasets\n",
        "\n",
        "For the 2D datasets, we have provided the following function:\n",
        "\n",
        "\n",
        "```\n",
        "run_keras_2d(data_name, layers, epochs, split=0.25, display=True, trials=5)\n",
        "```\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "data_name is a string, such as '1', '2', etc.\n",
        "layers is a list of Keras layer definitions for a Sequential model, e.g. \n",
        "```\n",
        "[Dense(input_dim=2, units=10, activation='relu'), Dense(units=2, activation='softmax')]\n",
        "```\n",
        "\n",
        "epochs is an integer indicating how many times to go through the data in training\n",
        "split is a fraction of the training data to use for validation if a validation set is not defined\n",
        "display whether to display result plots\n",
        "verbose whether to print loss and accuracy (percent correctly labeled) each epoch\n",
        "trials is an integer indicating how many times to perform the training and testing\n",
        "2D Data\n",
        "The two-class datasets have data_names: '1','2','3','4'. Target accuracies (percent correct) on the validation set are (99%, 90.5%, 96%, 94%).\n",
        "\n",
        "In this problem, try the following 5 architectures, specified by the number of units in the hidden layers:\n",
        "\n",
        "1: (0), 2: (10), 3: (100), 4: (10, 10), 5: (100, 100))\n",
        "You may find the archs function in the code file to be helpful here.\n",
        "Some of these questions ask for the \"simplest\" architecture; the list above is ordered starting with the simplest."
      ]
    },
    {
      "metadata": {
        "id": "Vp-RWPD7FUeo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from code_for_hw8_keras import *\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.optimizers import SGD, Adam\n",
        "from keras.layers import Conv1D, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "from keras.callbacks import Callback\n",
        "from keras.datasets import mnist\n",
        "from keras import backend as K\n",
        "from keras.initializers import VarianceScaling\n",
        "from matplotlib import pyplot as plt "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tOM8N6E-Pfju",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#]def archs(classes):\n",
        "    return [[Dense(input_dim=2, units=classes, activation=\"softmax\")],\n",
        "            [Dense(input_dim=2, units=10, activation='relu'),\n",
        "             Dense(units=classes, activation=\"softmax\")],\n",
        "            [Dense(input_dim=2, units=100, activation='relu'),\n",
        "             Dense(units=classes, activation=\"softmax\")],\n",
        "            [Dense(input_dim=2, units=10, activation='relu'),\n",
        "             Dense(units=10, activation='relu'),\n",
        "             Dense(units=classes, activation=\"softmax\")],\n",
        "            [Dense(input_dim=2, units=100, activation='relu'),\n",
        "             Dense(units=100, activation='relu'),\n",
        "             Dense(units=classes, activation=\"softmax\")]]"
      ]
    },
    {
      "metadata": {
        "id": "jCnuvXXp4gEG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 3A answer\n",
        "run_keras_2d(\"1\", archs(2)[0], 10, display=False, verbose=False, trials=5)\n",
        "run_keras_2d(\"2\", archs(2)[4], 10, display=False, verbose=False, trials=5)\n",
        "run_keras_2d(\"3\", archs(2)[1], 10, display=False, verbose=False, trials=5)\n",
        "run_keras_2d(\"4\", archs(2)[1], 10, display=False, verbose=False, trials=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rd2EnMo94tVl",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 3C \n",
        "run_keras_2d(\"3\", [\n",
        "    Dense(input_dim=2, units=200, activation=\"relu\"),\n",
        "    Dense(units=200, activation=\"relu\"),\n",
        "    Dense(units=2, activation=\"softmax\")\n",
        "], 100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dZKl5JRX4y30",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 3E average validation accuracy\n",
        "run_keras_2d(\"3class\", archs(3)[0], 10, split=0.5, display=False, verbose=False, trials=5)\n",
        "run_keras_2d(\"3class\", archs(3)[1], 10, split=0.5, display=False, verbose=False, trials=5)\n",
        "run_keras_2d(\"3class\", archs(3)[2], 10, split=0.5, display=False, verbose=False, trials=5)\n",
        "run_keras_2d(\"3class\", archs(3)[3], 10, split=0.5, display=False, verbose=False, trials=5)\n",
        "run_keras_2d(\"3class\", archs(3)[4], 10, split=0.5, display=False, verbose=False, trials=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_YWbWX47_9Tr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#example of run_keras_2d\n",
        "# this is a 2 class dataset\n",
        "\n",
        "# for dataset in [\"1\", \"2\", \"3\", \"4\"]:\n",
        "for dataset in [\"3class\"]:\n",
        "  for archnum in rAange(5):\n",
        "    print('Dataset ', dataset, \" Architecture \", archnum)\n",
        "    run_keras_2d(dataset, archs(3)[archnum], 10, split=0.5, display=False, verbose=False, trials=5)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ysXsXUe60X1J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# USE THE 3 class dataset and run for no hidden units with 1 trial\n",
        "linear_model = archs(3)[0]\n",
        "run_keras_2d('3class', linear_model, 10, split=0.5, display=False, verbose=False, trials=1)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vgRXa7j01GTv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# GET weights from the first layer of the model \n",
        "w, w0 = linear_model[0].get_weights()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BXOS4ATJ1S8p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# compute z values for each of the following points\n",
        "X = np.array([[-1,0], [1,0], [0,-11], [0,1], [-1,-1], [-1,1], [1,1], [1,-1]])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GM8mujBH1ZWe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "z = np.dot(w.T, X.T) + w0.reshape((-1,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sJcIBuYu1v6d",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# find indices with argmax\n",
        "np.argmax(z, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zh4u39OCjLza",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Weight sharing (OPTIONAL)\n",
        "\n",
        "** Note: You can click the arrow on the left of this text block to collapse/expand this optional section and all its code blocks **\n",
        "\n",
        "In the lab we designed a CNN that can count the number of objects in 1 dimensional images, where each black pixel is represented by a value of 0 and each white pixel is represented by a value of 1. Recall that an object is a consecutive sequence of black pixels ($0$'s). For example, the sequence $0100110$ contains three objects. \n",
        "\n",
        "Here we want to see how hard/easy it is to train such a network from data.  Our network architecture will be as follows: \n",
        "\n",
        "* The first layer is convolutional and you will implement it using the Keras `Conv1D` function, with a kernel of size 2 and stride of 1 with ReLu activation.\n",
        "\n",
        "* The second layer is a fully connected `Dense` layer which has a scalar output. \n",
        "\n",
        "Here is sample usage of the `Conv1D` and`Dense` layers. \n",
        "\n",
        "`layer1=keras.layers.Conv1D(filters=?, kernel_size=?, strides=?,use_bias=False, activation=?, batch_size=1, input_shape=?, padding='same')`\n",
        "\n",
        "`Dense(units=?, activation=?, use_bias=False)`\n",
        "\n",
        "You need to fill in the parameters marked with `?` based on the problem specifications. Note also that in Keras, depending on your implementation, you may be forced to use *three* layers to implement such a network, where one intermediary `Flatten` layer is used to flatten the output of the convolutional layer, before being passed to the dense layer.\n",
        "\n",
        "Refer to the <a href=\"https://keras.io/layers/convolutional/\">Conv 1D</a>, <a href=\"https://keras.io/layers/core/\">Dense</a> and <a href=\"https://keras.io/layers/core/#flatten\">Flatten</a> descriptions in the Keras documentation to see the available parameter options.\n",
        "\n",
        "In this exercise, we fix the structure and want to learn the best combination of weights from data. In the homework code, we have provided functions `train_neural_counter` and `get_image_data_1d`. You can use them to generate data and train the above neural network in Keras to answer the following questions. We assume that the images in our data set are randomly generated. The probability of a pixel being white is $0.1$. We work with mean squared error as the loss function for this problem. We have provided template code which you can fill in, to perform the training."
      ]
    },
    {
      "metadata": {
        "id": "yrcXA596yWek",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>4A)</b> We call a $2\\times 1$ filter an *edge detector* if it has weights $[-1,1]$ or $[1,-1]$ (i.e., it can detect a left or right edge in a given image). We call it an *averaging filter* if takes average of nearby pixels. \n",
        "\n",
        "Assuming that the second layer in our network is the sum function (i.e., all of its weights are equal to 1), which of the following filters gives a (nearly) local minima? You may assume that we are working with $1024\\times 1$ images. \n",
        "\n",
        "(Although you can answer the question without any coding, you may also empirically verify your claims. The function `set_weights` that can be applied to keras layers might be useful.)"
      ]
    },
    {
      "metadata": {
        "id": "wKa8iMv_j3ek",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>4B)</b> What is (approximately) the expected loss of the network on $1024\\times 1$ images if the convolutional layer is an averaging filter and second layer is the sum function (without a bias term)? (Note that you can answer the question theoretically or through coding, depending on your preference.)"
      ]
    },
    {
      "metadata": {
        "id": "IKxedp-qFXJe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.models import Sequential"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oKPcB588ok8a",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Code template if you would like to check 4B) through code\n",
        "\n",
        "imsize = 1024\n",
        "prob_white = 0.1\n",
        "\n",
        "num_filters = None  # Your code\n",
        "kernel_size = None  # Your code\n",
        "strides = None  # Your code\n",
        "activation_conv = None  # Your code\n",
        "\n",
        "(X_train,Y_train,X_val,Y_val,X_test,Y_test) = get_image_data_1d(1000,imsize,prob_white)\n",
        "\n",
        "layer1=keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, \\\n",
        "       strides=strides, use_bias=False, activation=activation_conv, batch_size=1, input_shape=(imsize,1), padding='same')\n",
        "\n",
        "activation_dense = None  # Your code\n",
        "num_units = None  # Your code\n",
        "layer3=Dense(units=num_units, activation=activation_dense, use_bias=False)\n",
        "\n",
        "layers=[layer1,Flatten(),layer3]\n",
        "\n",
        "# This is how we create the model using our layers\n",
        "model=Sequential()\n",
        "for layer in layers:\n",
        "    model.add(layer)\n",
        "        \n",
        "model.compile(loss='mse', optimizer=Adam()) \n",
        "\n",
        "# Set the weights of the layers to desired values\n",
        "# We give you the lines to use for this part\n",
        "model.layers[0].set_weights([np.array([1/2,1/2]).reshape(2,1,1)])\n",
        "model.layers[-1].set_weights([np.ones(imsize).reshape(imsize,1)])\n",
        "\n",
        "model.evaluate(X_test,Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "js3OYsbwj7Ms",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>4C)</b> Now suppose we add a bias term of $-10$ to the last layer. What is (approximately) the expected quadratic loss? (Note that you can answer the question theoretically or through coding, depending on your preference.)"
      ]
    },
    {
      "metadata": {
        "id": "AKynxhF1klga",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Edit code from 4B) with the bias"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3GCLr8qmj-Hk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>4D)</b> Averaging type filters are abundant and form a nearly flat valley of local minima for this problem. It is difficult for the network to find alternative solutions on its own. We need to force our way out of these bad minima and towards a better solution, i.e., an edge detector. To force the first layer to behave as an edge detector, we need to choose a proper **kernel regularizer**. Consider the following functions\n",
        "\n",
        "$f_1=\\sum_i |w_i|$, $f_2=\\sum_i |w_i^2|$, $f_3=|\\sum_{i} w_i|$. Which one of the choices is likely to guide the network to find an edge detector at the convolution layer?\n",
        "\n",
        "\n",
        "<a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week8/week8_homework/\">Refer to Catsoop.</a>"
      ]
    },
    {
      "metadata": {
        "id": "5aubU6Q6kwOI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Implement your choice of regularizers from above in the code (complete the function `filter_reg`). Do not allow any bias in the layers for the rest of the problem. The code generates some random test and training data sets and trains the model on these data. Run a few learning trials (5 or more) for each data set and answer the following questions based on the performance of your model.\n",
        "\n",
        "**IMPORTANT**: When implementing `filter_reg`, you should use the keras backend operations, imported as \"K\" in the code. So for example, `K.sum` and `K.abs`, rather than `np.sum` and `np.abs`. This is because the `weights` argument is NOT a numpy object, but rather an internal Keras object!"
      ]
    },
    {
      "metadata": {
        "id": "yOLZf_JsuTLn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Implement filter_reg\n",
        "\n",
        "def filter_reg(weights):\n",
        "    # We scale the output of the filter by lam\n",
        "    lam=1000\n",
        "    filter_result = None  # Your code\n",
        "    return lam * filter_result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pYRwd0eJkAdh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>4E)</b> For $1024\\times 1$ images and training set of size $1000$, is the network **without any regularization** likely to find models that have a mean square error lower than 8 on the test data?"
      ]
    },
    {
      "metadata": {
        "id": "1KiCbZmksXO6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Code template if you would like to check 4B) through code\n",
        "\n",
        "imsize = 1024\n",
        "prob_white = 0.1\n",
        "\n",
        "data=get_image_data_1d(1000, imsize, prob_white)\n",
        "trials=5\n",
        "for trial in range(trials):\n",
        "  \n",
        "    num_filters = None  # Your code\n",
        "    kernel_size = None  # Your code\n",
        "    strides = None  # Your code\n",
        "    activation_conv = None  # Your code\n",
        "\n",
        "    layer1=keras.layers.Conv1D(filters=num_filters, kernel_size=kernel_size, \\\n",
        "    strides=strides, use_bias=False, activation=activation_conv, batch_size=1, \\\n",
        "    input_shape=(imsize,1),padding='same')\n",
        "    \n",
        "    activation_dense = None  # Your code\n",
        "    num_units = None  # Your code\n",
        "\n",
        "    layer3=Dense(units=num_units, activation=activation_dense, use_bias=False)\n",
        "    \n",
        "    layers=[layer1,Flatten(),layer3]\n",
        "    model,err = train_neural_counter(layers, data, 'mse')\n",
        "    \n",
        "    model.layers[0].get_weights()[0]\n",
        "    np.mean(model.layers[-1].get_weights()[0])\n",
        "    print(err)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "g1vcUEL-vW9D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### For parts F) to J), simply edit your code from E) with the necessary changes."
      ]
    },
    {
      "metadata": {
        "id": "w_25ygQJkD5F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>4F)</b> Repeat the same experiment, but now with the regularizer you implemented. Try different regularization parameters. Which choice of regularization parameter gives the best prediction results?"
      ]
    },
    {
      "metadata": {
        "id": "vNAChIqylIlt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Edit code from 4E), using your filter as the kernel_regularizer in the Conv1D layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Rs44ze96kHZZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>4G)</b> With the above choice of regularization parameter, what is the mean square error of the best network that you find on the test data? Try a few trials (5 or more) for each data test and report the value of the best network. \n"
      ]
    },
    {
      "metadata": {
        "id": "IAN0k9wylOmz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### We expect the training to be easier when there are fewer parameters to learn. Consider images of size $128\\times 1$ for the rest of the problem."
      ]
    },
    {
      "metadata": {
        "id": "ZnktFwXRkKNF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>4H)</b> Instead of resorting to regularization again, we may instead find a way to reduce the number of parameters. What additional layer can you add to the output of the convolution layer to reduce the number of parameters to be learned without losing any relevant information?\n",
        "\n",
        "<a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week8/week8_homework/\">Refer to Catsoop.</a>"
      ]
    },
    {
      "metadata": {
        "id": "zXgOqKtRkNRP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>4I)</b> Add the layer you suggested above to your network and run some tests with data sets of size 1000 on $128\\times 1$ images.  How many parameters are left to learn with the new structure?"
      ]
    },
    {
      "metadata": {
        "id": "e8FRQawHkPG9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b>4J)</b> Mark your observations on the two structures (not using regularization).\n",
        "<a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week8/week8_homework/\">Refer to Catsoop.</a>"
      ]
    },
    {
      "metadata": {
        "id": "I-iTDCrHySde",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "eQGlJLxI__4A",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 5) MNIST (Digit Classification)\n",
        "\n",
        "In this section, we'll be looking at the MNIST data set seen already in problem 2. This time, we look at the *complete* MNIST problem where our networks will take an image of *any* digit from $0-9$ as input (recall that problem 2 only looked at digits $0$ and $1$) and try to predict that digit. Also, we will now use out-of-the-box neural network implementations using Keras and Tensorflow. State-of-the-art systems have error rates of less that one half of one percent on this data set (see <a href=\"http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354\">this list</a>).  We'll be happy with an error rate less than 2% since we don't have all year...\n",
        "<br>\n",
        "\n",
        "You can access the MNIST data for this problem using:\n",
        "<br><code>train, validation = get_MNIST_data()</code>\n",
        "<br>\n",
        "\n",
        "You can run the fully connected MNIST model, using:\n",
        "<br><code>run_keras_fc_mnist(train, validation, layers, epochs, split=0.1, trials=5)</code>\n",
        "<br>\n",
        "\n",
        "And, you can run the CNN MNIST test, using:\n",
        "<br><code>run_keras_cnn_mnist(train, validation, layers, epochs, split=0.1, trials=5)</code>\n",
        "<br>\n",
        "\n",
        "For all following experiments, please run for 5 trials (use `trials=5`) and report the average accuracy.\n",
        "<br>\n",
        "\n",
        "A word of warning, if you have a machine with a single core and/or very little RAM, you'll be better off running on an Athena workstation. If your solutions are not being accepted, and you are confident in your approach, try with more trials. Also, \n",
        "<br>\n",
        "\n",
        "You will need to design your own `layers` to feed to `run_keras_fc_mnist` and `run_keras_cnn_mnist`, which will be different than the ones specified by `archs()`. For instance, `layers=[Dense(input_dim=64, units=4, activation=\"softmax\")]` defines a single layer with 64 inputs, 4 output units, and softmax activation. Also, we advise you to use the option `verbose=True` when unsure about the progress made during training of your models.\n",
        "<br>"
      ]
    },
    {
      "metadata": {
        "id": "dx1jt6P9AUk1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b> 5A)</b> Look at the code and indicate what the difference is between <code>run_keras_fc_mnist</code> and <code>run_keras_cnn_mnist</code>? <a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week8/week8_homework/\">Refer to Catsoop.</a>"
      ]
    },
    {
      "metadata": {
        "id": "Q5moSfb7CcXd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def run_keras_fc_mnist(train, test, layers, epochs, split=0.1, verbose=True, trials=1):\n",
        "    (X_train, y1), (X_val, y2) = train, test\n",
        "    # Flatten the images\n",
        "    m = X_train.shape[1]\n",
        "    X_train = X_train.reshape((X_train.shape[0], m*m))\n",
        "    X_val = X_val.reshape((X_val.shape[0], m*m))\n",
        "    # Categorize the labels\n",
        "    num_classes = 10\n",
        "    y_train = np_utils.to_categorical(y1, num_classes)\n",
        "    y_val = np_utils.to_categorical(y2, num_classes)\n",
        "    # Train, use split for validation\n",
        "    val_acc, test_acc = 0, 0\n",
        "    for trial in range(trials):\n",
        "        # Reset the weights\n",
        "        # See https://github.com/keras-team/keras/issues/341\n",
        "        session = K.get_session()\n",
        "        for layer in layers:\n",
        "            for v in layer.__dict__:\n",
        "                v_arg = getattr(layer, v)\n",
        "                if hasattr(v_arg, 'initializer'):\n",
        "                    initializer_func = getattr(v_arg, 'initializer')\n",
        "                    initializer_func.run(session=session)\n",
        "        # Run the model\n",
        "        model, history, vacc, tacc = \\\n",
        "                run_keras(X_train, y_train, X_val, y_val, None, None, layers, epochs, split=split, verbose=verbose)\n",
        "        val_acc += vacc if vacc else 0\n",
        "        test_acc += tacc if tacc else 0\n",
        "    if val_acc:\n",
        "        print (\"\\nAvg. validation accuracy:\"  + str(val_acc/trials))\n",
        "    if test_acc:\n",
        "        print (\"\\nAvg. test accuracy:\"  + str(test_acc/trials))\n",
        "\n",
        "def run_keras_cnn_mnist(train, test, layers, epochs, split=0.1, verbose=True, trials=1):\n",
        "    # Load the dataset\n",
        "    (X_train, y1), (X_val, y2) = train, test\n",
        "    # Add a final dimension indicating the number of channels (only 1 here)\n",
        "    m = X_train.shape[1]\n",
        "    X_train = X_train.reshape((X_train.shape[0], m, m, 1))\n",
        "    X_val = X_val.reshape((X_val.shape[0], m, m, 1))\n",
        "    # Categorize the labels\n",
        "    num_classes = 10\n",
        "    y_train = np_utils.to_categorical(y1, num_classes)\n",
        "    y_val = np_utils.to_categorical(y2, num_classes)\n",
        "    # Train, use split for validation\n",
        "    val_acc, test_acc = 0, 0\n",
        "    for trial in range(trials):\n",
        "        # Reset the weights\n",
        "        # See https://github.com/keras-team/keras/issues/341\n",
        "        session = K.get_session()\n",
        "        for layer in layers:\n",
        "            for v in layer.__dict__:\n",
        "                v_arg = getattr(layer, v)\n",
        "                if hasattr(v_arg, 'initializer'):\n",
        "                    initializer_func = getattr(v_arg, 'initializer')\n",
        "                    initializer_func.run(session=session)\n",
        "        # Run the model\n",
        "        model, history, vacc, tacc = \\\n",
        "                run_keras(X_train, y_train, X_val, y_val, None, None, layers, epochs, split=split, verbose=verbose)\n",
        "        val_acc += vacc if vacc else 0\n",
        "        test_acc += tacc if tacc else 0\n",
        "    if val_acc:\n",
        "        print (\"\\nAvg. validation accuracy:\"  + str(val_acc/trials))\n",
        "    if test_acc:\n",
        "        print (\"\\nAvg. test accuracy:\"  + str(test_acc/trials))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4sGfqAbICbmE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b> 5B)</b> Using one epoch of training, what is the accuracy of a network **with no hidden units** (using the <code>run_keras_fc_mnist</code> method) on this data? Hint: this is expected to be terrible. If it's still not working, run for more trials. Remember to use 10 output units (the network predicts a digit from 0-9) and softmax activation!\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "J7Ce39lB3-Q6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# get mnist data\n",
        "train, validation = get_MNIST_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Gndf-ASDEe_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b> 5C)</b> When creating the keras layer, pass in the following argument to Dense:\n",
        "<code>kernel_initializer=VarianceScaling(scale=0.001, mode='fan_in', distribution='normal', seed=None)</code> and repeat the test.  What is the accuracy now?\n"
      ]
    },
    {
      "metadata": {
        "id": "H1VAxZ17DtPQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# fully connected flattens the 28*28 image so needs to have 28^2 numbers of units\n",
        "# 10 possible outputs (1 out of ten digits)\n",
        "layers = [Dense(input_dim=28*28, units=10, activation=\"softmax\")] # Your code\n",
        "layers_initialize = [Dense(input_dim=28*28, units=10, activation=\"softmax\", \n",
        "                          kernel_initializer=VarianceScaling(scale=0.001, mode='fan_in', distribution='normal', seed=None))] # Your code\n",
        "# this is a way of initializing the weights to adapt to scale of the weights\n",
        "#With distribution=\"normal\", samples are drawn from a truncated normal distribution centered on zero, with stddev = sqrt(scale / n) where n is:\n",
        "\n",
        "   # number of input units in the weight tensor, if mode = \"fan_in\"\n",
        "  #  number of output units, if mode = \"fan_out\"\n",
        "  #  average of the numbers of input and output units, if mode = \"fan_avg\"\n",
        "# seems to boost performance\n",
        "\n",
        "# suppose I linearly scale the data so pixel values are between 0 and 1, repeat test without variance scaling\n",
        "# each grayscale pixel is between 0 and 255 \n",
        "# to do this divide each by 255\n",
        "\n",
        "train_scaled  = train[0]/255.0\n",
        "validation_scaled  = validation[0]/255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xsQ31e1lDE6u",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b> 5D)</b> Now, linearly scale the data so that the pixel values are between 0 and 1 and repeat your test with the original layer (no VarianceScaling). What is the accuracy now?"
      ]
    },
    {
      "metadata": {
        "id": "KKh_-rQY4ixX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "4bd606f5-1cdc-4cca-a0e9-9fc390e982ec"
      },
      "cell_type": "code",
      "source": [
        "run_keras_fc_mnist(\n",
        "    (train_scaled, train[1]), (validation_scaled, validation[1]), layers, epochs=1, split=0.1, verbose=True, trials=5)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 3s 58us/step - loss: 0.4713 - acc: 0.8758 - val_loss: 0.3076 - val_acc: 0.9155\n",
            "\n",
            "Loss on validation set:0.3075888371527195 Accuracy on validation set: 0.9155\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.4683 - acc: 0.8769 - val_loss: 0.3063 - val_acc: 0.9172\n",
            "\n",
            "Loss on validation set:0.3062611770927906 Accuracy on validation set: 0.9172\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.4711 - acc: 0.8751 - val_loss: 0.3104 - val_acc: 0.9153\n",
            "\n",
            "Loss on validation set:0.3104010093510151 Accuracy on validation set: 0.9153\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 4s 61us/step - loss: 0.4658 - acc: 0.8788 - val_loss: 0.3075 - val_acc: 0.9160\n",
            "\n",
            "Loss on validation set:0.3074694989979267 Accuracy on validation set: 0.916\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 4s 62us/step - loss: 0.4679 - acc: 0.8773 - val_loss: 0.3114 - val_acc: 0.9143\n",
            "\n",
            "Loss on validation set:0.311397285425663 Accuracy on validation set: 0.9143\n",
            "\n",
            "Avg. validation accuracy:0.9156600000000001\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mHzyY-it4A0z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# check size of images\n",
        "len(train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2CP5WJQELJA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5b-RcZu1EPaj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "layers = None  # Your code\n",
        "train, validation = get_MNIST_data()\n",
        "\n",
        "# Scale the images\n",
        "train = (None, train[1])  # Your code\n",
        "validation = (None, validation[1])  # Your code\n",
        "\n",
        "run_keras_fc_mnist(train, validation, layers, 1, split=0.1, verbose=True, trials=5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S4aBsxAzDFBY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b> 5E)</b> What is happening? <a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week8/week8_homework/\">Refer to Catsoop.</a>\n"
      ]
    },
    {
      "metadata": {
        "id": "KrYGfcOLEr0f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Important: <b>Always scale the data like in 5D) for subsequent problems.</b>\n"
      ]
    },
    {
      "metadata": {
        "id": "mHoyqJdqDFH5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b> 5F)</b> Using this same architecture, evaluate validation accuracy for number of training epochs in [5, 10, 15]."
      ]
    },
    {
      "metadata": {
        "id": "q8PlbWS_EwTv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 6133
        },
        "outputId": "d7a259f0-0e75-4bbe-da27-dcebae199c73"
      },
      "cell_type": "code",
      "source": [
        "train, validation = get_MNIST_data()\n",
        "\n",
        "# Scale the images\n",
        "train = (train[0]/255.0, train[1])  # Your code\n",
        "validation = (validation[0]/255.0, validation[1])  # Your code\n",
        "\n",
        "for epochs in [5,10,15]:\n",
        "    layers = [Dense(input_dim=28*28, units=10, activation=\"softmax\")] # Your code\n",
        "    run_keras_fc_mnist(train, validation, layers, epochs, split=0.1, verbose=True, trials=5)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 4s 63us/step - loss: 0.4765 - acc: 0.8750 - val_loss: 0.3097 - val_acc: 0.9145\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.3047 - acc: 0.9153 - val_loss: 0.2809 - val_acc: 0.9218\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 0.2831 - acc: 0.9216 - val_loss: 0.2733 - val_acc: 0.9231\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 3s 50us/step - loss: 0.2731 - acc: 0.9237 - val_loss: 0.2714 - val_acc: 0.9228\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 3s 49us/step - loss: 0.2666 - acc: 0.9253 - val_loss: 0.2689 - val_acc: 0.9240\n",
            "\n",
            "Loss on validation set:0.2688548969298601 Accuracy on validation set: 0.924\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 4s 62us/step - loss: 0.4680 - acc: 0.8785 - val_loss: 0.3092 - val_acc: 0.9137\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.3044 - acc: 0.9155 - val_loss: 0.2805 - val_acc: 0.9206\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.2830 - acc: 0.9209 - val_loss: 0.2728 - val_acc: 0.9239\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.2731 - acc: 0.9247 - val_loss: 0.2718 - val_acc: 0.9239\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.2666 - acc: 0.9262 - val_loss: 0.2740 - val_acc: 0.9229\n",
            "\n",
            "Loss on validation set:0.2739608703225851 Accuracy on validation set: 0.9229\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 4s 64us/step - loss: 0.4720 - acc: 0.8765 - val_loss: 0.3106 - val_acc: 0.9153\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.3042 - acc: 0.9145 - val_loss: 0.2887 - val_acc: 0.9203\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.2839 - acc: 0.9199 - val_loss: 0.2779 - val_acc: 0.9230\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2729 - acc: 0.9237 - val_loss: 0.2676 - val_acc: 0.9263\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.2665 - acc: 0.9254 - val_loss: 0.2699 - val_acc: 0.9253\n",
            "\n",
            "Loss on validation set:0.26990511695444586 Accuracy on validation set: 0.9253\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 4s 64us/step - loss: 0.4709 - acc: 0.8756 - val_loss: 0.3070 - val_acc: 0.9165\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.3041 - acc: 0.9147 - val_loss: 0.2841 - val_acc: 0.9204\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.2835 - acc: 0.9201 - val_loss: 0.2735 - val_acc: 0.9248\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.2732 - acc: 0.9242 - val_loss: 0.2747 - val_acc: 0.9230\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.2670 - acc: 0.9258 - val_loss: 0.2722 - val_acc: 0.9244\n",
            "\n",
            "Loss on validation set:0.27222318689525127 Accuracy on validation set: 0.9244\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/5\n",
            "60000/60000 [==============================] - 4s 65us/step - loss: 0.4709 - acc: 0.8772 - val_loss: 0.3083 - val_acc: 0.9137\n",
            "Epoch 2/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.3040 - acc: 0.9148 - val_loss: 0.2819 - val_acc: 0.9219\n",
            "Epoch 3/5\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2838 - acc: 0.9213 - val_loss: 0.2787 - val_acc: 0.9214\n",
            "Epoch 4/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.2728 - acc: 0.9242 - val_loss: 0.2744 - val_acc: 0.9243\n",
            "Epoch 5/5\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.2666 - acc: 0.9260 - val_loss: 0.2655 - val_acc: 0.9258\n",
            "\n",
            "Loss on validation set:0.26552462839782237 Accuracy on validation set: 0.9258\n",
            "\n",
            "Avg. validation accuracy:0.92448\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4741 - acc: 0.8771 - val_loss: 0.3073 - val_acc: 0.9155\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.3048 - acc: 0.9152 - val_loss: 0.2818 - val_acc: 0.9233\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2837 - acc: 0.9206 - val_loss: 0.2746 - val_acc: 0.9252\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.2733 - acc: 0.9238 - val_loss: 0.2681 - val_acc: 0.9247\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2667 - acc: 0.9254 - val_loss: 0.2680 - val_acc: 0.9256\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2620 - acc: 0.9267 - val_loss: 0.2668 - val_acc: 0.9263\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.2579 - acc: 0.9287 - val_loss: 0.2663 - val_acc: 0.9266\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2553 - acc: 0.9287 - val_loss: 0.2636 - val_acc: 0.9262\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.2530 - acc: 0.9295 - val_loss: 0.2639 - val_acc: 0.9279\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2509 - acc: 0.9302 - val_loss: 0.2628 - val_acc: 0.9292\n",
            "\n",
            "Loss on validation set:0.2628103126972914 Accuracy on validation set: 0.9292\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4689 - acc: 0.8777 - val_loss: 0.3093 - val_acc: 0.9149\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.3033 - acc: 0.9160 - val_loss: 0.2823 - val_acc: 0.9222\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2831 - acc: 0.9209 - val_loss: 0.2742 - val_acc: 0.9238\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2728 - acc: 0.9237 - val_loss: 0.2720 - val_acc: 0.9233\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2665 - acc: 0.9258 - val_loss: 0.2674 - val_acc: 0.9265\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2622 - acc: 0.9270 - val_loss: 0.2699 - val_acc: 0.9251\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2581 - acc: 0.9280 - val_loss: 0.2658 - val_acc: 0.9272\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2553 - acc: 0.9297 - val_loss: 0.2628 - val_acc: 0.9269\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2527 - acc: 0.9302 - val_loss: 0.2676 - val_acc: 0.9260\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2507 - acc: 0.9304 - val_loss: 0.2659 - val_acc: 0.9282\n",
            "\n",
            "Loss on validation set:0.2659376858100295 Accuracy on validation set: 0.9282\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 4s 67us/step - loss: 0.4705 - acc: 0.8769 - val_loss: 0.3076 - val_acc: 0.9167\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.3035 - acc: 0.9158 - val_loss: 0.2792 - val_acc: 0.9226\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2836 - acc: 0.9207 - val_loss: 0.2764 - val_acc: 0.9247\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2733 - acc: 0.9231 - val_loss: 0.2714 - val_acc: 0.9245\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 3s 51us/step - loss: 0.2665 - acc: 0.9259 - val_loss: 0.2671 - val_acc: 0.9259\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2620 - acc: 0.9277 - val_loss: 0.2645 - val_acc: 0.9270\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2585 - acc: 0.9287 - val_loss: 0.2637 - val_acc: 0.9273\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2552 - acc: 0.9292 - val_loss: 0.2705 - val_acc: 0.9257\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2525 - acc: 0.9301 - val_loss: 0.2646 - val_acc: 0.9279\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2506 - acc: 0.9310 - val_loss: 0.2676 - val_acc: 0.9271\n",
            "\n",
            "Loss on validation set:0.26756411136686803 Accuracy on validation set: 0.9271\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.4646 - acc: 0.8795 - val_loss: 0.3082 - val_acc: 0.9143\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.3029 - acc: 0.9164 - val_loss: 0.2860 - val_acc: 0.9206\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2831 - acc: 0.9208 - val_loss: 0.2719 - val_acc: 0.9236\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2730 - acc: 0.9234 - val_loss: 0.2707 - val_acc: 0.9244\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2665 - acc: 0.9254 - val_loss: 0.2677 - val_acc: 0.9255\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2616 - acc: 0.9271 - val_loss: 0.2644 - val_acc: 0.9262\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2579 - acc: 0.9291 - val_loss: 0.2675 - val_acc: 0.9254\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2549 - acc: 0.9295 - val_loss: 0.2648 - val_acc: 0.9261\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2525 - acc: 0.9299 - val_loss: 0.2655 - val_acc: 0.9259\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2507 - acc: 0.9305 - val_loss: 0.2626 - val_acc: 0.9277\n",
            "\n",
            "Loss on validation set:0.2625809618920088 Accuracy on validation set: 0.9277\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/10\n",
            "60000/60000 [==============================] - 4s 69us/step - loss: 0.4693 - acc: 0.8770 - val_loss: 0.3092 - val_acc: 0.9131\n",
            "Epoch 2/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.3041 - acc: 0.9148 - val_loss: 0.2853 - val_acc: 0.9193\n",
            "Epoch 3/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2831 - acc: 0.9209 - val_loss: 0.2728 - val_acc: 0.9243\n",
            "Epoch 4/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2730 - acc: 0.9235 - val_loss: 0.2737 - val_acc: 0.9236\n",
            "Epoch 5/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2668 - acc: 0.9257 - val_loss: 0.2678 - val_acc: 0.9239\n",
            "Epoch 6/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2621 - acc: 0.9273 - val_loss: 0.2640 - val_acc: 0.9271\n",
            "Epoch 7/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2579 - acc: 0.9287 - val_loss: 0.2658 - val_acc: 0.9260\n",
            "Epoch 8/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2554 - acc: 0.9295 - val_loss: 0.2643 - val_acc: 0.9274\n",
            "Epoch 9/10\n",
            "60000/60000 [==============================] - 3s 52us/step - loss: 0.2527 - acc: 0.9304 - val_loss: 0.2637 - val_acc: 0.9277\n",
            "Epoch 10/10\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2510 - acc: 0.9304 - val_loss: 0.2635 - val_acc: 0.9272\n",
            "\n",
            "Loss on validation set:0.26351276998519896 Accuracy on validation set: 0.9272\n",
            "\n",
            "Avg. validation accuracy:0.92788\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/15\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.4642 - acc: 0.8790 - val_loss: 0.3070 - val_acc: 0.9153\n",
            "Epoch 2/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.3030 - acc: 0.9156 - val_loss: 0.2825 - val_acc: 0.9225\n",
            "Epoch 3/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2832 - acc: 0.9208 - val_loss: 0.2719 - val_acc: 0.9247\n",
            "Epoch 4/15\n",
            "60000/60000 [==============================] - 4s 58us/step - loss: 0.2734 - acc: 0.9227 - val_loss: 0.2687 - val_acc: 0.9247\n",
            "Epoch 5/15\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.2661 - acc: 0.9264 - val_loss: 0.2686 - val_acc: 0.9243\n",
            "Epoch 6/15\n",
            "60000/60000 [==============================] - 4s 59us/step - loss: 0.2615 - acc: 0.9270 - val_loss: 0.2660 - val_acc: 0.9259\n",
            "Epoch 7/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2580 - acc: 0.9287 - val_loss: 0.2632 - val_acc: 0.9270\n",
            "Epoch 8/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2551 - acc: 0.9295 - val_loss: 0.2645 - val_acc: 0.9268\n",
            "Epoch 9/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2528 - acc: 0.9299 - val_loss: 0.2642 - val_acc: 0.9283\n",
            "Epoch 10/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2506 - acc: 0.9303 - val_loss: 0.2652 - val_acc: 0.9277\n",
            "Epoch 11/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2489 - acc: 0.9313 - val_loss: 0.2653 - val_acc: 0.9261\n",
            "Epoch 12/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2474 - acc: 0.9315 - val_loss: 0.2640 - val_acc: 0.9277\n",
            "Epoch 13/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2462 - acc: 0.9321 - val_loss: 0.2653 - val_acc: 0.9287\n",
            "Epoch 14/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2448 - acc: 0.9325 - val_loss: 0.2703 - val_acc: 0.9256\n",
            "Epoch 15/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2435 - acc: 0.9325 - val_loss: 0.2677 - val_acc: 0.9267\n",
            "\n",
            "Loss on validation set:0.2676760985672474 Accuracy on validation set: 0.9267\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/15\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.4728 - acc: 0.8764 - val_loss: 0.3072 - val_acc: 0.9144\n",
            "Epoch 2/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.3032 - acc: 0.9150 - val_loss: 0.2820 - val_acc: 0.9230\n",
            "Epoch 3/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2829 - acc: 0.9206 - val_loss: 0.2721 - val_acc: 0.9228\n",
            "Epoch 4/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2726 - acc: 0.9242 - val_loss: 0.2711 - val_acc: 0.9243\n",
            "Epoch 5/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2664 - acc: 0.9255 - val_loss: 0.2723 - val_acc: 0.9225\n",
            "Epoch 6/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2616 - acc: 0.9276 - val_loss: 0.2674 - val_acc: 0.9262\n",
            "Epoch 7/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2577 - acc: 0.9287 - val_loss: 0.2652 - val_acc: 0.9259\n",
            "Epoch 8/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2550 - acc: 0.9295 - val_loss: 0.2636 - val_acc: 0.9263\n",
            "Epoch 9/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2529 - acc: 0.9300 - val_loss: 0.2636 - val_acc: 0.9260\n",
            "Epoch 10/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2504 - acc: 0.9310 - val_loss: 0.2621 - val_acc: 0.9272\n",
            "Epoch 11/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2489 - acc: 0.9312 - val_loss: 0.2667 - val_acc: 0.9264\n",
            "Epoch 12/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2473 - acc: 0.9318 - val_loss: 0.2658 - val_acc: 0.9267\n",
            "Epoch 13/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2459 - acc: 0.9323 - val_loss: 0.2668 - val_acc: 0.9279\n",
            "Epoch 14/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2447 - acc: 0.9324 - val_loss: 0.2683 - val_acc: 0.9275\n",
            "Epoch 15/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2438 - acc: 0.9332 - val_loss: 0.2661 - val_acc: 0.9278\n",
            "\n",
            "Loss on validation set:0.2660767931967974 Accuracy on validation set: 0.9278\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/15\n",
            "60000/60000 [==============================] - 4s 71us/step - loss: 0.4658 - acc: 0.8793 - val_loss: 0.3069 - val_acc: 0.9156\n",
            "Epoch 2/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.3042 - acc: 0.9153 - val_loss: 0.2832 - val_acc: 0.9216\n",
            "Epoch 3/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2831 - acc: 0.9210 - val_loss: 0.2748 - val_acc: 0.9229\n",
            "Epoch 4/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2732 - acc: 0.9241 - val_loss: 0.2712 - val_acc: 0.9243\n",
            "Epoch 5/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2665 - acc: 0.9256 - val_loss: 0.2679 - val_acc: 0.9264\n",
            "Epoch 6/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2617 - acc: 0.9276 - val_loss: 0.2676 - val_acc: 0.9249\n",
            "Epoch 7/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2581 - acc: 0.9291 - val_loss: 0.2646 - val_acc: 0.9271\n",
            "Epoch 8/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2551 - acc: 0.9290 - val_loss: 0.2656 - val_acc: 0.9257\n",
            "Epoch 9/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2527 - acc: 0.9297 - val_loss: 0.2624 - val_acc: 0.9278\n",
            "Epoch 10/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2510 - acc: 0.9310 - val_loss: 0.2656 - val_acc: 0.9262\n",
            "Epoch 11/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2489 - acc: 0.9314 - val_loss: 0.2643 - val_acc: 0.9283\n",
            "Epoch 12/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2474 - acc: 0.9316 - val_loss: 0.2678 - val_acc: 0.9269\n",
            "Epoch 13/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2460 - acc: 0.9325 - val_loss: 0.2657 - val_acc: 0.9274\n",
            "Epoch 14/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2448 - acc: 0.9326 - val_loss: 0.2704 - val_acc: 0.9263\n",
            "Epoch 15/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2433 - acc: 0.9331 - val_loss: 0.2674 - val_acc: 0.9268\n",
            "\n",
            "Loss on validation set:0.2673577934682369 Accuracy on validation set: 0.9268\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/15\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.4685 - acc: 0.8775 - val_loss: 0.3065 - val_acc: 0.9162\n",
            "Epoch 2/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.3041 - acc: 0.9151 - val_loss: 0.2805 - val_acc: 0.9226\n",
            "Epoch 3/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2833 - acc: 0.9207 - val_loss: 0.2763 - val_acc: 0.9222\n",
            "Epoch 4/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2726 - acc: 0.9242 - val_loss: 0.2682 - val_acc: 0.9256\n",
            "Epoch 5/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2666 - acc: 0.9251 - val_loss: 0.2694 - val_acc: 0.9260\n",
            "Epoch 6/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2616 - acc: 0.9265 - val_loss: 0.2623 - val_acc: 0.9276\n",
            "Epoch 7/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2582 - acc: 0.9292 - val_loss: 0.2690 - val_acc: 0.9241\n",
            "Epoch 8/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2549 - acc: 0.9293 - val_loss: 0.2678 - val_acc: 0.9257\n",
            "Epoch 9/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2531 - acc: 0.9303 - val_loss: 0.2630 - val_acc: 0.9257\n",
            "Epoch 10/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2507 - acc: 0.9307 - val_loss: 0.2635 - val_acc: 0.9272\n",
            "Epoch 11/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2490 - acc: 0.9311 - val_loss: 0.2689 - val_acc: 0.9244\n",
            "Epoch 12/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2473 - acc: 0.9321 - val_loss: 0.2671 - val_acc: 0.9270\n",
            "Epoch 13/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2458 - acc: 0.9319 - val_loss: 0.2736 - val_acc: 0.9250\n",
            "Epoch 14/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2446 - acc: 0.9321 - val_loss: 0.2708 - val_acc: 0.9253\n",
            "Epoch 15/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2433 - acc: 0.9324 - val_loss: 0.2659 - val_acc: 0.9268\n",
            "\n",
            "Loss on validation set:0.26585184873342516 Accuracy on validation set: 0.9268\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/15\n",
            "60000/60000 [==============================] - 4s 74us/step - loss: 0.4724 - acc: 0.8760 - val_loss: 0.3098 - val_acc: 0.9148\n",
            "Epoch 2/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.3045 - acc: 0.9145 - val_loss: 0.2807 - val_acc: 0.9214\n",
            "Epoch 3/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2836 - acc: 0.9204 - val_loss: 0.2737 - val_acc: 0.9239\n",
            "Epoch 4/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2728 - acc: 0.9234 - val_loss: 0.2682 - val_acc: 0.9249\n",
            "Epoch 5/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2665 - acc: 0.9258 - val_loss: 0.2686 - val_acc: 0.9259\n",
            "Epoch 6/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2615 - acc: 0.9279 - val_loss: 0.2682 - val_acc: 0.9264\n",
            "Epoch 7/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2579 - acc: 0.9282 - val_loss: 0.2684 - val_acc: 0.9258\n",
            "Epoch 8/15\n",
            "60000/60000 [==============================] - 3s 55us/step - loss: 0.2550 - acc: 0.9292 - val_loss: 0.2666 - val_acc: 0.9247\n",
            "Epoch 9/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2525 - acc: 0.9301 - val_loss: 0.2681 - val_acc: 0.9254\n",
            "Epoch 10/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2508 - acc: 0.9304 - val_loss: 0.2639 - val_acc: 0.9269\n",
            "Epoch 11/15\n",
            "60000/60000 [==============================] - 3s 53us/step - loss: 0.2491 - acc: 0.9313 - val_loss: 0.2649 - val_acc: 0.9255\n",
            "Epoch 12/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2472 - acc: 0.9318 - val_loss: 0.2659 - val_acc: 0.9269\n",
            "Epoch 13/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2460 - acc: 0.9317 - val_loss: 0.2657 - val_acc: 0.9285\n",
            "Epoch 14/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2450 - acc: 0.9323 - val_loss: 0.2671 - val_acc: 0.9271\n",
            "Epoch 15/15\n",
            "60000/60000 [==============================] - 3s 54us/step - loss: 0.2436 - acc: 0.9325 - val_loss: 0.2687 - val_acc: 0.9274\n",
            "\n",
            "Loss on validation set:0.26873935735076665 Accuracy on validation set: 0.9274\n",
            "\n",
            "Avg. validation accuracy:0.9271\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6zJk4u2-DFNi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b> 5G)</b> \n",
        "\n",
        "<b> 5G a)</b>With the validation accuracy that you just saw on per digit basis using $15$ epochs, and assuming each digit is read independently from the others, what is the probability of reading a 5 digit zip code correctly?<br>\n",
        "\n",
        "<b> 5G b)</b>Now, assume that the accuracy is 0.9985, what is the probability of reading a zip code correctly?<br>\n",
        "b.\n",
        "\n",
        "This is why people care about dropping the error rates to what at first sound like ridiculous values.\n",
        "\n",
        "<a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week8/week8_homework/\">Refer to Catsoop.</a>"
      ]
    },
    {
      "metadata": {
        "id": "vCPTuz-tDFTg",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b> 5H)</b> Using one epoch of training, try a single hidden layer with ReLU and gradually increase the units (128, 256, 512, 1024) units.  What are the accuracies?"
      ]
    },
    {
      "metadata": {
        "id": "bmpbP_VHFoh1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1913
        },
        "outputId": "3676b3f0-387e-4488-b897-0a928064b02b"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "for num in [128,256,512,1024]:\n",
        "    layers =  [Dense(input_dim=28*28, units=num, activation='relu'), Dense(units=10, activation=\"softmax\")] # Your code\n",
        "    run_keras_fc_mnist(train, validation, layers, 1, split=0.1, verbose=True, trials=5)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 10s 164us/step - loss: 0.2531 - acc: 0.9280 - val_loss: 0.1321 - val_acc: 0.9612\n",
            "\n",
            "Loss on validation set:0.13212243243679403 Accuracy on validation set: 0.9612\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 9s 153us/step - loss: 0.2529 - acc: 0.9279 - val_loss: 0.1336 - val_acc: 0.9620\n",
            "\n",
            "Loss on validation set:0.13358383609205485 Accuracy on validation set: 0.962\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 8s 140us/step - loss: 0.2566 - acc: 0.9271 - val_loss: 0.1374 - val_acc: 0.9598\n",
            "\n",
            "Loss on validation set:0.1373662525974214 Accuracy on validation set: 0.9598\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 9s 144us/step - loss: 0.2629 - acc: 0.9243 - val_loss: 0.1395 - val_acc: 0.9596\n",
            "\n",
            "Loss on validation set:0.1395069923326373 Accuracy on validation set: 0.9596\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 9s 154us/step - loss: 0.2580 - acc: 0.9254 - val_loss: 0.1361 - val_acc: 0.9596\n",
            "\n",
            "Loss on validation set:0.13606108719818294 Accuracy on validation set: 0.9596\n",
            "\n",
            "Avg. validation accuracy:0.96044\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.2270 - acc: 0.9347 - val_loss: 0.1083 - val_acc: 0.9680\n",
            "\n",
            "Loss on validation set:0.10831529248729348 Accuracy on validation set: 0.968\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 11s 178us/step - loss: 0.2270 - acc: 0.9339 - val_loss: 0.1150 - val_acc: 0.9645\n",
            "\n",
            "Loss on validation set:0.11500028242301195 Accuracy on validation set: 0.9645\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 11s 185us/step - loss: 0.2223 - acc: 0.9345 - val_loss: 0.1208 - val_acc: 0.9631\n",
            "\n",
            "Loss on validation set:0.120835185046494 Accuracy on validation set: 0.9631\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 11s 176us/step - loss: 0.2237 - acc: 0.9344 - val_loss: 0.1187 - val_acc: 0.9634\n",
            "\n",
            "Loss on validation set:0.11866493609175086 Accuracy on validation set: 0.9634\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 11s 177us/step - loss: 0.2269 - acc: 0.9348 - val_loss: 0.1147 - val_acc: 0.9654\n",
            "\n",
            "Loss on validation set:0.11467309250794351 Accuracy on validation set: 0.9654\n",
            "\n",
            "Avg. validation accuracy:0.96488\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 16s 264us/step - loss: 0.2010 - acc: 0.9407 - val_loss: 0.1005 - val_acc: 0.9675\n",
            "\n",
            "Loss on validation set:0.10047740837596357 Accuracy on validation set: 0.9675\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 16s 263us/step - loss: 0.2016 - acc: 0.9410 - val_loss: 0.1031 - val_acc: 0.9685\n",
            "\n",
            "Loss on validation set:0.10310966830551624 Accuracy on validation set: 0.9685\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 16s 264us/step - loss: 0.1981 - acc: 0.9419 - val_loss: 0.0976 - val_acc: 0.9718\n",
            "\n",
            "Loss on validation set:0.09759217167906463 Accuracy on validation set: 0.9718\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 16s 265us/step - loss: 0.2029 - acc: 0.9404 - val_loss: 0.1058 - val_acc: 0.9667\n",
            "\n",
            "Loss on validation set:0.10584345227889716 Accuracy on validation set: 0.9667\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 16s 269us/step - loss: 0.2020 - acc: 0.9408 - val_loss: 0.1106 - val_acc: 0.9675\n",
            "\n",
            "Loss on validation set:0.11062459153905511 Accuracy on validation set: 0.9675\n",
            "\n",
            "Avg. validation accuracy:0.9683999999999999\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 28s 473us/step - loss: 0.1870 - acc: 0.9432 - val_loss: 0.0986 - val_acc: 0.9685\n",
            "\n",
            "Loss on validation set:0.09864131823554635 Accuracy on validation set: 0.9685\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 28s 468us/step - loss: 0.1837 - acc: 0.9448 - val_loss: 0.0923 - val_acc: 0.9707\n",
            "\n",
            "Loss on validation set:0.09232271711304783 Accuracy on validation set: 0.9707\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 29s 479us/step - loss: 0.1890 - acc: 0.9432 - val_loss: 0.0979 - val_acc: 0.9706\n",
            "\n",
            "Loss on validation set:0.09791580752795562 Accuracy on validation set: 0.9706\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 29s 475us/step - loss: 0.1854 - acc: 0.9449 - val_loss: 0.0910 - val_acc: 0.9716\n",
            "\n",
            "Loss on validation set:0.09101180902887136 Accuracy on validation set: 0.9716\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 29s 481us/step - loss: 0.1882 - acc: 0.9433 - val_loss: 0.0869 - val_acc: 0.9730\n",
            "\n",
            "Loss on validation set:0.08693183492906392 Accuracy on validation set: 0.973\n",
            "\n",
            "Avg. validation accuracy:0.97088\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gEM5mZi5DFYS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b> 5I)</b> Now, try a network with two layers, again using one epoch, with 512 units in the first hidden layer and and 256 units in the second hidden layer.  What is the accuracy?"
      ]
    },
    {
      "metadata": {
        "id": "6cwp1VR7F06q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "f2dba382-1e24-4b07-f3a0-0ecb190da7bb"
      },
      "cell_type": "code",
      "source": [
        "layers =  [Dense(input_dim=28*28, units=512, activation='relu'), \n",
        "           Dense(input_dim=28*28, units=256, activation='relu'), \n",
        "           Dense(units=10, activation=\"softmax\")] # Your code\n",
        "run_keras_fc_mnist(train, validation, layers, 1, split=0.1, verbose=True, trials=5)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.1852 - acc: 0.9431 - val_loss: 0.1022 - val_acc: 0.9674\n",
            "\n",
            "Loss on validation set:0.10222592464890332 Accuracy on validation set: 0.9674\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 21s 349us/step - loss: 0.1852 - acc: 0.9445 - val_loss: 0.0988 - val_acc: 0.9699\n",
            "\n",
            "Loss on validation set:0.09878523978423327 Accuracy on validation set: 0.9699\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 22s 372us/step - loss: 0.1883 - acc: 0.9411 - val_loss: 0.0967 - val_acc: 0.9694\n",
            "\n",
            "Loss on validation set:0.09673928351644427 Accuracy on validation set: 0.9694\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 21s 354us/step - loss: 0.1855 - acc: 0.9432 - val_loss: 0.1127 - val_acc: 0.9637\n",
            "\n",
            "Loss on validation set:0.11268713978882879 Accuracy on validation set: 0.9637\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 21s 352us/step - loss: 0.1846 - acc: 0.9442 - val_loss: 0.1120 - val_acc: 0.9660\n",
            "\n",
            "Loss on validation set:0.11203005794268102 Accuracy on validation set: 0.966\n",
            "\n",
            "Avg. validation accuracy:0.96728\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TmnNUT2nDFdi",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b> 5J)</b> Build a convolutional network with the following structure:\n",
        "\n",
        "<ul>\n",
        "<li> A convolutional layer with 32 filters of size 3 × 3, with a ReLU activation\n",
        "<li> A max pooling layer with size 2 × 2\n",
        "<li> A convolutional layer with 64 filters of size 3 × 3, with ReLU activation\n",
        "<li> A max pooling layer with size 2 × 2\n",
        "<li> A flatten layer\n",
        "<li> A fully connected layer with 128 neurons, with ReLU activation\n",
        "<li> A dropout layer with drop probability 0.5\n",
        "<li> A fully-connected layer with 10 neurons with softmax\n",
        "</ul>\n",
        "Train it on MNIST for one epoch, using <code>run_keras_cnn_mnist</code>.  What is the accuracy on the validation set?\n",
        "\n",
        "If you have time to run the training for more epochs, try it, you should see improvement.\n"
      ]
    },
    {
      "metadata": {
        "id": "3-sPW8xKF7EZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        },
        "outputId": "b411d6e3-93df-4ddb-f453-82107bf61146"
      },
      "cell_type": "code",
      "source": [
        "# uses scaled train/validation\n",
        "# Conv1D, Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
        "\n",
        "layers = [Conv2D(input_shape=(28, 28, 1), filters=32, kernel_size=3, activation='relu'),\n",
        "          MaxPooling2D(pool_size=(2,2)),\n",
        "          Conv2D(filters=32, kernel_size=3, activation='relu'),\n",
        "          MaxPooling2D(pool_size=(2,2)),\n",
        "          Flatten(), \n",
        "          Dense(units=128, activation='relu'),\n",
        "          Dropout(rate=0.5),\n",
        "          Dense(units=10,  activation='softmax')    \n",
        "]  # Your code\n",
        "\n",
        "run_keras_cnn_mnist(train, validation, layers, 1, split=0.1, verbose=True, trials=5)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 59s 979us/step - loss: 0.2476 - acc: 0.9231 - val_loss: 0.0543 - val_acc: 0.9830\n",
            "\n",
            "Loss on validation set:0.054268948423699476 Accuracy on validation set: 0.983\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 61s 1ms/step - loss: 0.2207 - acc: 0.9326 - val_loss: 0.0549 - val_acc: 0.9812\n",
            "\n",
            "Loss on validation set:0.05485752918715589 Accuracy on validation set: 0.9812\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 61s 1ms/step - loss: 0.2304 - acc: 0.9294 - val_loss: 0.0493 - val_acc: 0.9852\n",
            "\n",
            "Loss on validation set:0.049320278084510935 Accuracy on validation set: 0.9852\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 60s 1ms/step - loss: 0.2389 - acc: 0.9264 - val_loss: 0.0518 - val_acc: 0.9836\n",
            "\n",
            "Loss on validation set:0.05180974739589728 Accuracy on validation set: 0.9836\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 61s 1ms/step - loss: 0.2214 - acc: 0.9311 - val_loss: 0.0476 - val_acc: 0.9852\n",
            "\n",
            "Loss on validation set:0.047613763809530064 Accuracy on validation set: 0.9852\n",
            "\n",
            "Avg. validation accuracy:0.98364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KjmqgGvIDFiS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b> 5K)</b> Now, let's compare the performance of a fully connected model and a CNN on data where the characters have been shifted randomly so that they are no longer centered.  \n",
        "\n",
        "You can build such a data set by calling: <code>train_20, validation_20 = get_MNIST_data(shift=20)</code>. Remember to scale it appropriately.\n",
        "\n",
        "<b>Note that each image is now 48x48, so you will need to change your layer definitions</b>.\n",
        "Run your two-hidden-layer FC architecture from above (problem 5I) on this data and then run the CNN architecture from above (problem 5J), both for one epoch. Report your results.\n"
      ]
    },
    {
      "metadata": {
        "id": "zwdj8cxPMQ74",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "83ccfd19-872e-4e42-a34f-c20734bceb92"
      },
      "cell_type": "code",
      "source": [
        "train_20, validation_20 = get_MNIST_data(shift=20) # Your code (fill in the shift)\n",
        "m = np.max(train_20[0][0])\n",
        "print(m)\n",
        "\n",
        "# Scale the images\n",
        "train_20 = (train_20[0]/m, train_20[1])  # Your code\n",
        "validation_20 = (validation_20[0]/m, validation_20[1])  # Your code\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "255.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "b3YFxy-NOT9Y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "64f2a6f9-e1f6-4ec4-d03e-feb52bcb4c75"
      },
      "cell_type": "code",
      "source": [
        "train_20[0].shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 48, 48)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "uvfiyrN9Gf7X",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "9d0c31d8-9261-4190-a16a-632e36bd7334"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "layers_fc = [Dense(input_dim=48*48, units=512, activation='relu'), \n",
        "           Dense(units=256, activation='relu'), \n",
        "           Dense(units=10, activation=\"softmax\")] \n",
        "\n",
        "run_keras_fc_mnist(train_20, validation_20, layers_fc, 1, split=0.1, verbose=True, trials=5)\n",
        "\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 50s 841us/step - loss: 0.7392 - acc: 0.7573 - val_loss: 0.3884 - val_acc: 0.8763\n",
            "\n",
            "Loss on validation set:0.3884438019275665 Accuracy on validation set: 0.8763\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 49s 814us/step - loss: 0.7463 - acc: 0.7542 - val_loss: 0.3963 - val_acc: 0.8709\n",
            "\n",
            "Loss on validation set:0.39629622364044187 Accuracy on validation set: 0.8709\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 49s 813us/step - loss: 0.7443 - acc: 0.7546 - val_loss: 0.3756 - val_acc: 0.8807\n",
            "\n",
            "Loss on validation set:0.375582682967186 Accuracy on validation set: 0.8807\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 48s 808us/step - loss: 0.7404 - acc: 0.7558 - val_loss: 0.3895 - val_acc: 0.8724\n",
            "\n",
            "Loss on validation set:0.38945188579559326 Accuracy on validation set: 0.8724\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 48s 805us/step - loss: 0.7519 - acc: 0.7517 - val_loss: 0.4388 - val_acc: 0.8580\n",
            "\n",
            "Loss on validation set:0.4388296055793762 Accuracy on validation set: 0.858\n",
            "\n",
            "Avg. validation accuracy:0.87166\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fPylEc6jOACM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "outputId": "bc11ac5b-a05b-4887-dba2-3bfd05219cbe"
      },
      "cell_type": "code",
      "source": [
        "layers_cnn = [Conv2D(input_shape=(48, 48, 1), filters=32, kernel_size=3, activation='relu'),\n",
        "          MaxPooling2D(pool_size=(2,2)),\n",
        "          Conv2D(filters=32, kernel_size=3, activation='relu'),\n",
        "          MaxPooling2D(pool_size=(2,2)),\n",
        "          Flatten(), \n",
        "          Dense(units=128, activation='relu'),\n",
        "          Dropout(rate=0.5),\n",
        "          Dense(units=10,  activation='softmax')    \n",
        "]  # Your code  \n",
        "\n",
        "run_keras_cnn_mnist(train_20, validation_20, layers_cnn, 1, split=0.1, verbose=True, trials=5)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 167s 3ms/step - loss: 0.7502 - acc: 0.7498 - val_loss: 0.1933 - val_acc: 0.9420\n",
            "\n",
            "Loss on validation set:0.19327147540748119 Accuracy on validation set: 0.942\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 175s 3ms/step - loss: 0.7450 - acc: 0.7492 - val_loss: 0.1943 - val_acc: 0.9419\n",
            "\n",
            "Loss on validation set:0.19434951719939708 Accuracy on validation set: 0.9419\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 174s 3ms/step - loss: 0.7204 - acc: 0.7575 - val_loss: 0.1574 - val_acc: 0.9555\n",
            "\n",
            "Loss on validation set:0.15740453015714884 Accuracy on validation set: 0.9555\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 174s 3ms/step - loss: 0.7876 - acc: 0.7335 - val_loss: 0.1827 - val_acc: 0.9496\n",
            "\n",
            "Loss on validation set:0.1827107978761196 Accuracy on validation set: 0.9496\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 184s 3ms/step - loss: 0.7156 - acc: 0.7601 - val_loss: 0.1715 - val_acc: 0.9483\n",
            "\n",
            "Loss on validation set:0.17152972524017096 Accuracy on validation set: 0.9483\n",
            "\n",
            "Avg. validation accuracy:0.9474599999999999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1iLmfPHaC2d8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "<b> 5L)</b> Some possible conclusions. <a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week8/week8_homework/\">Refer to Catsoop.</a>"
      ]
    },
    {
      "metadata": {
        "id": "y8u39r-848Ft",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# 5B\n",
        "train, validation = get_MNIST_data()\n",
        "run_keras_fc_mnist(train, validation, [\n",
        "    Dense(input_dim=28*28, units=10, activation=\"softmax\")\n",
        "], 1, split=0.1, verbose=False, trials=5)\n",
        "\n",
        "#5C Answer\n",
        "train, validation = get_MNIST_data()\n",
        "run_keras_fc_mnist(train, validation, [\n",
        "    Dense(input_dim=28*28, units=10, activation=\"softmax\",\n",
        "        kernel_initializer=VarianceScaling(\n",
        "            scale=0.001, mode='fan_in', distribution='normal', seed=None\n",
        "        )\n",
        "    )\n",
        "], 1, split=0.1, verbose=False, trials=5)\n",
        "\n",
        "# 5D answer\n",
        "train, validation = get_MNIST_data()\n",
        "train = train[0] / 255, train[1]\n",
        "validation = validation[0] / 255, validation[1]\n",
        "run_keras_fc_mnist(train, validation, [\n",
        "    Dense(input_dim=28*28, units=10, activation=\"softmax\")\n",
        "], 1, split=0.1, verbose=False, trials=5)\n",
        "\n",
        "# 5F answer\n",
        "train, validation = get_MNIST_data()\n",
        "train = train[0] / 255, train[1]\n",
        "validation = validation[0] / 255, validation[1]\n",
        "run_keras_fc_mnist(train, validation, [\n",
        "    Dense(input_dim=28*28, units=10, activation=\"softmax\")\n",
        "], 5, split=0.1, verbose=False, trials=5)\n",
        "run_keras_fc_mnist(train, validation, [\n",
        "    Dense(input_dim=28*28, units=10, activation=\"softmax\")\n",
        "], 10, split=0.1, verbose=False, trials=5)\n",
        "run_keras_fc_mnist(train, validation, [\n",
        "    Dense(input_dim=28*28, units=10, activation=\"softmax\")\n",
        "], 15, split=0.1, verbose=False, trials=5)\n",
        "\n",
        "# 5H answers\n",
        "train, validation = get_MNIST_data()\n",
        "train = train[0] / 255, train[1]\n",
        "validation = validation[0] / 255, validation[1]\n",
        "run_keras_fc_mnist(train, validation, [\n",
        "    Dense(input_dim=28*28, units=128, activation=\"relu\"), Dense(units=10, activation=\"softmax\")\n",
        "], 1, split=0.1, verbose=False, trials=5)\n",
        "run_keras_fc_mnist(train, validation, [\n",
        "    Dense(input_dim=28*28, units=256, activation=\"relu\"), Dense(units=10, activation=\"softmax\")\n",
        "], 1, split=0.1, verbose=False, trials=5)\n",
        "run_keras_fc_mnist(train, validation, [\n",
        "    Dense(input_dim=28*28, units=512, activation=\"relu\"), Dense(units=10, activation=\"softmax\")\n",
        "], 1, split=0.1, verbose=False, trials=5)\n",
        "run_keras_fc_mnist(train, validation, [\n",
        "    Dense(input_dim=28*28, units=1024, activation=\"relu\"), Dense(units=10, activation=\"softmax\")\n",
        "], 1, split=0.1, verbose=False, trials=5)\n",
        "\n",
        "# 5I \n",
        "train, validation = get_MNIST_data()\n",
        "train = train[0] / 255, train[1]\n",
        "validation = validation[0] / 255, validation[1]\n",
        "run_keras_fc_mnist(train, validation, [\n",
        "    Dense(input_dim=28*28, units=512, activation=\"relu\"),\n",
        "    Dense(units=256, activation=\"relu\"),\n",
        "    Dense(units=10, activation=\"softmax\")\n",
        "], 1, split=0.1, verbose=False, trials=5)\n",
        "\n",
        "# 5J\n",
        "train, validation = get_MNIST_data()\n",
        "train = train[0] / 255, train[1]\n",
        "validation = validation[0] / 255, validation[1]\n",
        "run_keras_cnn_mnist(train, validation, [\n",
        "    Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(units=128, activation=\"relu\"),\n",
        "    Dropout(0.5),\n",
        "    Dense(units=10, activation=\"softmax\")\n",
        "], 1, split=0.1, verbose=False, trials=5)\n",
        "\n",
        "# 5K\n",
        "\n",
        "\n",
        "train_20, validation_20 = get_MNIST_data(shift=20)\n",
        "train_20 = train_20[0] / 255, train_20[1]\n",
        "validation_20 = validation_20[0] / 255, validation_20[1]\n",
        "run_keras_fc_mnist(train_20, validation_20, [\n",
        "    Dense(input_dim=48*48, units=512, activation=\"relu\"),\n",
        "    Dense(units=256, activation=\"relu\"),\n",
        "    Dense(units=10, activation=\"softmax\")\n",
        "], 1, split=0.1, verbose=False, trials=5)\n",
        "run_keras_cnn_mnist(train_20, validation_20, [\n",
        "    Conv2D(32, (3, 3), activation=\"relu\", input_shape=(48, 48, 1)),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Conv2D(64, (3, 3), activation=\"relu\"),\n",
        "    MaxPooling2D((2, 2)),\n",
        "    Flatten(),\n",
        "    Dense(units=128, activation=\"relu\"),\n",
        "    Dropout(0.5),\n",
        "    Dense(units=10, activation=\"softmax\")\n",
        "], 1, split=0.1, verbose=False, trials=5)\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}