{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LRR 2  MIT 6.036 HW10 Colab Notebook",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lrraymond13/collab/blob/master/LRR_2_MIT_6_036_HW10_Colab_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "q58cS9antfCw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#MIT 6.036 Spring 2019: Homework 10#\n",
        "\n",
        "This colab notebook provides code and a framework for questions 2, 3, and 4 from [homework 10](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week10/week10_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab."
      ]
    },
    {
      "metadata": {
        "id": "OUEtSZRdtmI2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "3b2e9bac-312c-45dd-9396-3b4ae21affc5"
      },
      "cell_type": "code",
      "source": [
        "!rm -rf code_for_hw10* __MACOSX data .DS_Store\n",
        "!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw10/code_for_hw10.zip\n",
        "!unzip code_for_hw10.zip\n",
        "!mv code_for_hw10/* .\n",
        "\n",
        "import code_for_hw10 as code_for_hw10\n",
        "import mdp10 as mdp\n",
        "\n",
        "import numpy as np\n",
        "import math as m\n",
        "import random\n",
        "\n",
        "import pdb\n",
        "from dist import uniform_dist, delta_dist, mixture_dist, DDist\n",
        "from util import argmax_with_val, argmax\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import importlib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  code_for_hw10.zip\n",
            "   creating: code_for_hw10/\n",
            "  inflating: code_for_hw10/util.py   \n",
            "  inflating: code_for_hw10/mdp10.py  \n",
            "   creating: __MACOSX/\n",
            "   creating: __MACOSX/code_for_hw10/\n",
            "  inflating: __MACOSX/code_for_hw10/._mdp10.py  \n",
            "  inflating: code_for_hw10/code_for_hw10.py  \n",
            "  inflating: code_for_hw10/dist.py   \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "       table#id2, #id2 > tbody > tr > th, #id2 > tbody > tr > td {\n",
              "         border: 1px solid lightgray;\n",
              "         border-collapse:collapse;\n",
              "         \n",
              "        }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": [
              "outputarea_id2"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table id=id2><tr><td id=id2-0-0></td><td id=id2-0-1></td><td id=id2-0-2></td><td id=id2-0-3></td><td id=id2-0-4></td><td id=id2-0-5></td><td id=id2-0-6></td><td id=id2-0-7></td><td id=id2-0-8></td><td id=id2-0-9></td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": [
              "outputarea_id2"
            ]
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "3Zhptv005XBN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 2) Implement Q-Learning\n",
        "\n",
        "We'll work up to implementing the Q-learning algorithm by extending our code from HW9. In the next block, please copy and paste your implementations of the following functions from HW9."
      ]
    },
    {
      "metadata": {
        "id": "0cu9UMTm2l8x",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def value_iteration(mdp, q, eps = 0.01, max_iters = 1000):\n",
        "    def v(s):\n",
        "        return value(q,s)\n",
        "    for it in range(max_iters):\n",
        "        new_q = q.copy()\n",
        "        delta = 0\n",
        "        for s in mdp.states:\n",
        "            for a in mdp.actions:\n",
        "                new_q.set(s, a, mdp.reward_fn(s, a) + mdp.discount_factor * \\\n",
        "                          mdp.transition_model(s, a).expectation(v))\n",
        "                delta = max(delta, abs(new_q.get(s, a) - q.get(s, a)))\n",
        "        if delta < eps:\n",
        "            return new_q\n",
        "        q = new_q\n",
        "    return q\n",
        "\n",
        "def value(q, s):\n",
        "    # Your code here (COPY FROM HW9)\n",
        "    return max(q.get(s, a) for a in q.actions)\n",
        "\n",
        "def greedy(q, s):\n",
        "    \"\"\" Return pi*(s) based on a greedy strategy.\n",
        "\n",
        "    >>> q = TabularQ([0,1,2,3],['b','c'])\n",
        "    >>> q.set(0, 'b', 5)\n",
        "    >>> q.set(0, 'c', 10)\n",
        "    >>> q.set(1, 'b', 2)\n",
        "    >>> greedy(q, 0)\n",
        "    'c'\n",
        "    >>> greedy(q, 1)\n",
        "    'b'\n",
        "    \"\"\"\n",
        "    # solution is: return argmax(q.actions, lambda a: q.get(s, a))\n",
        "    # Your code here\n",
        "    possible_actions = q.actions\n",
        "    max_v = -float('inf')\n",
        "    best_a = None\n",
        "    for a in possible_actions:\n",
        "      if q.get(s,a) >= max_v:\n",
        "        max_v = q.get(s,a)\n",
        "        best_a = a\n",
        "    return best_a\n",
        "\n",
        "def epsilon_greedy(q, s, eps = 0.5):\n",
        "    \"\"\" Returns an action.\n",
        "\n",
        "    >>> q = TabularQ([0,1,2,3],['b','c'])\n",
        "    >>> q.set(0, 'b', 5)\n",
        "    >>> q.set(0, 'c', 10)\n",
        "    >>> q.set(1, 'b', 2)\n",
        "    >>> eps = 0.\n",
        "    >>> epsilon_greedy(q, 0, eps) #greedy\n",
        "    'c'\n",
        "    >>> epsilon_greedy(q, 1, eps) #greedy\n",
        "    'b'\n",
        "    \"\"\"\n",
        "    if random.random() < eps:  # True with prob eps, random action\n",
        "        return uniform_dist(q.actions).draw()\n",
        "    else:                   # False with prob 1-eps, greedy action\n",
        "        return greedy(q, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MyKQPeWk5zx1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the next code block to make sure what you need from HW9 is working."
      ]
    },
    {
      "metadata": {
        "id": "Fvz2c_Vs3JuN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "67c98fa2-b6f2-41f3-9d93-5b77a087ba5c"
      },
      "cell_type": "code",
      "source": [
        "mdp.value = value\n",
        "mdp.greedy = greedy\n",
        "mdp.epsilon_greedy = epsilon_greedy\n",
        "mdp.value_iteration = value_iteration\n",
        "\n",
        "importlib.reload(code_for_hw10)\n",
        "\n",
        "# Test: Value Iteration\n",
        "code_for_hw10.test_solve_play()\n",
        "\n",
        "# Expected output:\n",
        "# '''\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# Reward 100\n",
        "# '''"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "       table#id4, #id4 > tbody > tr > th, #id4 > tbody > tr > td {\n",
              "         border: 1px solid lightgray;\n",
              "         border-collapse:collapse;\n",
              "         \n",
              "        }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": [
              "outputarea_id4"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table id=id4><tr><td id=id4-0-0></td><td id=id4-0-1></td><td id=id4-0-2></td><td id=id4-0-3></td><td id=id4-0-4></td><td id=id4-0-5></td><td id=id4-0-6></td><td id=id4-0-7></td><td id=id4-0-8></td><td id=id4-0-9></td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": [
              "outputarea_id4"
            ]
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oCaiqNqB6D2-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.1) Q update\n",
        "\n",
        "First, we'll extend our implementation of the TabularQ class in HW 9 (Problem 5) to incorporate the crucial operation of Q-learning, which is to update the Q value for a given `(s, a)` entry and move it part of the way towards a \"target\" value *t*.\n",
        "\n",
        "> *Q(s,a) ← (1−α)Q(s,a) + αt*\n",
        "\n",
        "Note that this can also be written as:\n",
        "\n",
        "> *Q(s,a)← Q(s,a)+α(t−Q(s,a))*\n",
        "\n",
        "That is, move a small (*α*) step towards t."
      ]
    },
    {
      "metadata": {
        "id": "RL0934247JNK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let's define a new method for `TabularQ` that implements this, in a batched form. We will be given a list of `(s, a, t)` triples and have to do all the updates. Note that update is a method of the `TabularQ` class, so you can access the other methods and attributes.\n",
        "\n",
        "* `data` is a list of `(s, a, t)` tuples.\n",
        "* `lr` is a learning rate (*α* above)\n",
        "* We will have to update `self.q[(s,a)]` for all of the data.\n"
      ]
    },
    {
      "metadata": {
        "id": "8FZlzaFevNkD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class TabularQ:\n",
        "    def __init__(self, states, actions):\n",
        "        self.actions = actions\n",
        "        self.states = states\n",
        "        self.q = dict([((s, a), 0.0) for s in states for a in actions])\n",
        "    def copy(self):\n",
        "        q_copy = TabularQ(self.states, self.actions)\n",
        "        q_copy.q.update(self.q)\n",
        "        return q_copy\n",
        "    def set(self, s, a, v):\n",
        "        self.q[(s,a)] = v\n",
        "    def get(self, s, a):\n",
        "        return self.q[(s,a)]\n",
        "    def update(self, data, lr):\n",
        "      for (s, a, t) in data:\n",
        "        self.q[(s, a)] = (1 - lr) * self.q[(s, a)] + lr * t"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aeQekM6h7vuL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.2) Q_learn\n",
        "\n",
        "Complete the definition of the `Q_learn` function. It should update the entries in the `q` function, `(s, a)`, towards their estimated Q values. It should terminate after `iters` iterations and use learning rate `lr`. Use the `q.update` method, which you just wrote, to update the Q values."
      ]
    },
    {
      "metadata": {
        "id": "tzaOD9VY7_4-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "You will need to both simulate the agent's trajectory through the space as well as perform the updates to the Q function estimates. In this version, you should update the Q values after every transition, using a single `(s, a, t)` tuple. **The following methods and functions have already been defined for you.**\n",
        "\n",
        "\n",
        "* To start a new simulation, call `mdp.init_state()`. That will draw a state from the MDP's initial state distribution.\n",
        "* You can use the functions that we defined in HW 9: `epsilon_greedy` for action selection (epsilon_greedy takes `(q, s, eps = 0.5)` as input and returns an action) `value` takes `(q, s)` and returns the max Q value for a state.\n",
        "* To take a step in the simulation, starting in a given state, `s`, using action a, call `mdp.sim_transition(s,a)`. It will return a pair `(r, s_prime)` denoting the reward received by the agent at that step and the next state.\n",
        "* Be careful in treating terminal states. Recall that at a terminal state, there may be an immediate reward but the future expected value will be zero.\n",
        "* Return `q` so that the Tutor can test it."
      ]
    },
    {
      "metadata": {
        "id": "00YIgjEwvOab",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Q_learn(mdp, q, lr=.1, iters=100, eps=0.5, interactive_fn=None):\n",
        "    s = mdp.init_state()\n",
        "    for i in range(iters):\n",
        "        a = epsilon_greedy(q, s, eps)\n",
        "        r, s_prime = mdp.sim_transition(s, a)\n",
        "        future_val = 0 if mdp.terminal(s) else value(q, s_prime)\n",
        "        q.update([(s, a, (r + mdp.discount_factor * future_val))], lr)\n",
        "        s = s_prime\n",
        "        if interactive_fn: interactive_fn(q, i)\n",
        "    return q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4C3ZdmJ5NV3Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "1d440844-6292-42a8-9974-0a6372b01008"
      },
      "cell_type": "code",
      "source": [
        "mdp.TabularQ = TabularQ\n",
        "mdp.Q_learn = Q_learn\n",
        "importlib.reload(code_for_hw10)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "       table#id6, #id6 > tbody > tr > th, #id6 > tbody > tr > td {\n",
              "         border: 1px solid lightgray;\n",
              "         border-collapse:collapse;\n",
              "         \n",
              "        }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": [
              "outputarea_id6"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table id=id6><tr><td id=id6-0-0></td><td id=id6-0-1></td><td id=id6-0-2></td><td id=id6-0-3></td><td id=id6-0-4></td><td id=id6-0-5></td><td id=id6-0-6></td><td id=id6-0-7></td><td id=id6-0-8></td><td id=id6-0-9></td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": [
              "outputarea_id6"
            ]
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'code_for_hw10' from '/content/code_for_hw10.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "metadata": {
        "id": "UweE9iUL897r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the next code block to test your implementation of `Q_learn`."
      ]
    },
    {
      "metadata": {
        "id": "gzEz4q7y3R8a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "2ed95bdb-d191-4018-bd12-6a79b0f0c421"
      },
      "cell_type": "code",
      "source": [
        "def tinyTerminal(s):\n",
        "    return s==4\n",
        "def tinyR(s, a):\n",
        "    if s == 1: return 1\n",
        "    elif s == 3: return 2\n",
        "    else: return 0\n",
        "def tinyTrans(s, a):\n",
        "    if s == 0:\n",
        "        if a == 'a':\n",
        "            return DDist({1 : 0.9, 2 : 0.1})\n",
        "        else:\n",
        "            return DDist({1 : 0.1, 2 : 0.9})\n",
        "    elif s == 1:\n",
        "        return DDist({1 : 0.1, 0 : 0.9})\n",
        "    elif s == 2:\n",
        "        return DDist({2 : 0.1, 3 : 0.9})\n",
        "    elif s == 3:\n",
        "        return DDist({3 : 0.1, 0 : 0.5, 4 : 0.4})\n",
        "    elif s == 4:\n",
        "        return DDist({4 : 1.0})\n",
        "      \n",
        "def testQ():\n",
        "    tiny = mdp.MDP([0, 1, 2, 3, 4], ['a', 'b'], tinyTrans, tinyR, 0.9)\n",
        "    tiny.terminal = tinyTerminal\n",
        "    q = TabularQ(tiny.states, tiny.actions)\n",
        "    qf = Q_learn(tiny, q)\n",
        "    ret = list(qf.q.items())\n",
        "    expected = [((0, 'a'), 0.6649739221724159), ((0, 'b'), 0.1712369526453748), \n",
        "                ((1, 'a'), 0.7732751316011999), ((1, 'b'), 1.2034912054227331), \n",
        "                ((2, 'a'), 0.37197205380133874), ((2, 'b'), 0.45929063274463033), \n",
        "                ((3, 'a'), 1.5156163024818292), ((3, 'b'), 0.8776852768653631), \n",
        "                ((4, 'a'), 0.0), ((4, 'b'), 0.0)]\n",
        "    ok = True\n",
        "    for (s,a), v in expected:\n",
        "      qv = qf.get(s,a)\n",
        "      if abs(qv-v) > 1.0e-5:\n",
        "        print(\"Oops!  For (s=%s, a=%s) expected %s, but got %s\" % (s, a, v, qv))\n",
        "        ok = False\n",
        "    if ok:\n",
        "      print(\"Tests passed!\")\n",
        "\n",
        "random.seed(0)\n",
        "testQ()      "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Oops!  For (s=0, a=a) expected 0.6649739221724159, but got 0.8048428363707812\n",
            "Oops!  For (s=0, a=b) expected 0.1712369526453748, but got 0.19316690253968336\n",
            "Oops!  For (s=1, a=a) expected 0.7732751316011999, but got 0.48495610433794933\n",
            "Oops!  For (s=1, a=b) expected 1.2034912054227331, but got 1.3805655733690563\n",
            "Oops!  For (s=2, a=a) expected 0.37197205380133874, but got 0.24844196150564404\n",
            "Oops!  For (s=2, a=b) expected 0.45929063274463033, but got 0.5958300672555848\n",
            "Oops!  For (s=3, a=a) expected 1.5156163024818292, but got 0.5613052311726843\n",
            "Oops!  For (s=3, a=b) expected 0.8776852768653631, but got 1.744480863854994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7PXWcgn99NfK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2.3) Batch Q_learn\n",
        "\n",
        "Assume your previous update method has been defined.\n",
        "\n",
        "In the standard Q-learning algorithm, we make one epsilon-greedy transition based on the current Q estimate and then update the Q values. You can think of this as being like stochastic gradient descent. We can also define a version that is more like batch gradient descent, where we generate one or more \"episodes\" (sequences of transitions) using the current Q values and then update the Q values based on all the observed results. We can also keep around old transitions and use them (all or a random subset) in the update as well. **Note that as our Q value estimate evolves, the target Q value computed from a previously observed transition can change.**\n",
        "\n",
        "Implement this version of batch Q-learning that (a) generates some specifed number of episodes of a given length (see `sim_episode` below), (b) adds these to the experiences we have seen previously, and (c) updates the Q estimates based on **all the experience so far**. Return `q` so that the Tutor can test it."
      ]
    },
    {
      "metadata": {
        "id": "8WMcWSfnE-_Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# evaluate this cell so you can use the definition in your code below\n",
        "\n",
        "def sim_episode(mdp, episode_length, policy, draw=False):\n",
        "    '''\n",
        "    Simulate an episode (sequence of transitions) of at most\n",
        "    episode_length, using policy function to select actions.  If we find\n",
        "    a terminal state, end the episode.  Return accumulated reward a list\n",
        "    of (s, a, r, s') where s' is None for transition from terminal state.\n",
        "    Also return an animation if draw=True, or None if draw=False\n",
        "    '''\n",
        "    episode = []\n",
        "    reward = 0\n",
        "    s = mdp.init_state()\n",
        "    all_states = [s]\n",
        "    for i in range(episode_length):\n",
        "        a = policy(s)\n",
        "        (r, s_prime) = mdp.sim_transition(s, a)\n",
        "        reward += r\n",
        "        if mdp.terminal(s):\n",
        "            episode.append((s, a, r, None))\n",
        "            break\n",
        "        episode.append((s, a, r, s_prime))\n",
        "        if draw: \n",
        "            mdp.draw_state(s)\n",
        "        s = s_prime\n",
        "        all_states.append(s)\n",
        "    animation = animate(all_states, mdp.n, episode_length) if draw else None\n",
        "    return reward, episode, animation"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2IHoMJaI-ZhC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Clarifications**\n",
        "\n",
        "* There should be a SINGLE call to `q.update` per iteration, not per episode or per experience. Just one call per iteration, with a lot of data.\n",
        "* Let's understand the distinction between experiences `(s, a, r, s')` and Q targets `(s, a, t)`. Note that experiences don't depend on the current estimated Q values (only on the environment we are acting in), but the \"t\" in the Q targets depends on the current Q values. So, it makes sense to store experiences across iterations, but not to store Q targets, since the Q targets change when we update our Q values. Thus, you want to continuously aggregate the experience and then, in each iteration, re-compute the Q targets under the current estimated Q values, then do the update with all of these Q targets. Here's pseudocode:\n",
        "\n",
        "\n",
        "```\n",
        "all_experiences = []\n",
        "Loop over n_iterations:\n",
        "    Loop over n_episodes:\n",
        "        Generate an episode of length episode_length, append this experience to all_experiences\n",
        "    all_q_targets = []\n",
        "    Loop over all_experiences:\n",
        "        Append Q target from one experience to all_q_targets\n",
        "        Remember to handle terminal states (where s' = None)\n",
        "    q.update(all_q_targets, lr)\n",
        "return q\n",
        "```\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "8uHjoRWPvScc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def Q_learn_batch(mdp, q, lr=.1, iters=100, eps=0.5,\n",
        "                               episode_length=10, n_episodes=2,\n",
        "                  interactive_fn=None):\n",
        "    all_experiences = []\n",
        "    explore = lambda s: epsilon_greedy(q,s,eps)\n",
        "    for i in range(iters):\n",
        "        for e in range(n_episodes):\n",
        "            _, episode, _ = sim_episode(mdp, episode_length, explore)\n",
        "            all_experiences += episode\n",
        "        all_q_targets = []\n",
        "        for (s, a, r, s_prime) in all_experiences:\n",
        "            future_val = 0 if s_prime is None else value(q, s_prime)\n",
        "            all_q_targets.append((s, a, (r + mdp.discount_factor * future_val)))\n",
        "        q.update(all_q_targets, lr)\n",
        "        if interactive_fn: interactive_fn(q, i)\n",
        "    return q"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lv9oygbu9Sz1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the next code block to test your implementation of `Q_learn_batch`."
      ]
    },
    {
      "metadata": {
        "id": "pFvMhZuL3-Y9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "aecd3f85-2b94-4ae7-85e1-17c76d43e4ec"
      },
      "cell_type": "code",
      "source": [
        "def testBatchQ():\n",
        "    tiny = mdp.MDP([0, 1, 2, 3, 4], ['a', 'b'], tinyTrans, tinyR, 0.9)\n",
        "    tiny.terminal = tinyTerminal\n",
        "    q = TabularQ(tiny.states, tiny.actions)\n",
        "    qf = Q_learn_batch(tiny, q)\n",
        "    ret = list(qf.q.items())\n",
        "    expected = [((0, 'a'), 4.7566600197286535), ((0, 'b'), 3.993296047838986), \n",
        "                ((1, 'a'), 5.292467934685342), ((1, 'b'), 5.364014782870985), \n",
        "                ((2, 'a'), 4.139537149779127), ((2, 'b'), 4.155347555640753), \n",
        "                ((3, 'a'), 4.076532544818926), ((3, 'b'), 4.551442974149778), \n",
        "                ((4, 'a'), 0.0), ((4, 'b'), 0.0)]\n",
        "\n",
        "    ok = True\n",
        "    for (s,a), v in expected:\n",
        "      qv = qf.get(s,a)\n",
        "      if abs(qv-v) > 1.0e-5:\n",
        "        print(\"Oops!  For (s=%s, a=%s) expected %s, but got %s\" % (s, a, v, qv))\n",
        "        ok = False\n",
        "    if ok:\n",
        "      print(\"Tests passed!\")\n",
        "      \n",
        "      return list(qf.q.items())\n",
        "\n",
        "random.seed(0)\n",
        "testBatchQ()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Oops!  For (s=0, a=a) expected 4.7566600197286535, but got 4.719373048773068\n",
            "Oops!  For (s=0, a=b) expected 3.993296047838986, but got 3.9894502668007856\n",
            "Oops!  For (s=1, a=a) expected 5.292467934685342, but got 5.229379765859235\n",
            "Oops!  For (s=1, a=b) expected 5.364014782870985, but got 5.387604894868419\n",
            "Oops!  For (s=2, a=a) expected 4.139537149779127, but got 4.719721562235436\n",
            "Oops!  For (s=2, a=b) expected 4.155347555640753, but got 4.75645231638034\n",
            "Oops!  For (s=3, a=a) expected 4.076532544818926, but got 5.054037583555434\n",
            "Oops!  For (s=3, a=b) expected 4.551442974149778, but got 4.16591736629545\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sj5q0rup9eq0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3) NN Q\n",
        "\n",
        "Now we will combine neural networks with Q-learning. There are a wide variety of ways to do this, some of which are referred to as \"deep Q-learning\". They all hinge on the following ideas, which are explained in depth in the notes:\n",
        "\n",
        "* We would like to operate in large or continuous state and/or action spaces so it is not possible (or effective) to store the *Q* values in a table; instead, we will \"store\" them by training a neural network to do regression for us, taking *s, a* as input and generating (an approximation of) *Q∗(s,a)* as output.\n",
        "* To train the network, we will use *squared Bellman error* as the loss function (see [homework 10](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week10/week10_homework/) for equation)."
      ]
    },
    {
      "metadata": {
        "id": "IO0ScUsIDdUZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will explore two different algorithms and one input representation. There are many choices of neural network architecture. We are going to work on a problem with a very small set (size 3) of actions and we will:\n",
        "\n",
        "* Make one neural network for each possible action *a*.\n",
        "* Design that network with 2 **hidden** layers, with ReLU units, and a single linear output unit; and\n",
        "* Use mean squared error (mse) as the loss function since we are doing regression\n",
        "* Use `Adam()` as your optimizer."
      ]
    },
    {
      "metadata": {
        "id": "lqcuigrnLfY0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# In Keras, we would create such a network using the following function\n",
        "# please evaluate this cell so you can use it in your code\n",
        "\n",
        "def make_nn(state_dim, num_hidden_layers, num_units):\n",
        "    '''\n",
        "    state_dim =\t(int) number of states\n",
        "    num_hidden_layers =\t(int) number of\tfully connected hidden layers\n",
        "    num_units =\t(int) number of\tdense relu units to use\tin hidden layers\n",
        "    '''  \n",
        "    model = Sequential()\n",
        "    model.add(Dense(num_units, input_dim = state_dim, activation='relu'))\n",
        "    for i in range(num_hidden_layers-1):\n",
        "        model.add(Dense(num_units, activation='relu'))\n",
        "    model.add(Dense(1, activation='linear'))\n",
        "    model.compile(loss='mse', optimizer=Adam())\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tx9u0Gb7-9Ev",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1) Basic neural network Q-learning\n",
        "\n",
        "We'll call the first algorithm *basic neural network Q-learning*. It was the first strategy to be used to combine Q-learning and neural networks, and suffers from the *correlated experience* problem described in the notes. However, it is relatively easy to implement and works okay on very simple problems.\n",
        "\n",
        "In basic NN Q-learning, just as in the version of Q-learning we implemented above, you simulate a trajectory through state space, using *ϵ*-greedy action selection, and making an update to your Q functions after each piece of experience *(s, a, r, s′)*. You do this by doing one epoch of training on your network for action *a*, on a data set made up of a single data point, where the input is *s* and the desired output is *r + γ max_a' Q(s', a')*, using the current estimates of *Q*.\n",
        "\n",
        "This might sound very familiar. In fact, it is exactly `Q_learn` except that instead of using a tabular representation of the the Q function, we are using a neural net to approximate the Q values for the state. In particular, the update method corresponds to training the neural net with the target Q value.\n",
        "\n",
        "To use a neural net implementation of Q values, for a given action, we will need to have a mapping from states to fixed-length row vectors. We will assume that the `MDP` class has a `state2vec` method that maps states to vectors. For the simple MDPs we have seen so far, this simply returns a one-hot representation of the state.\n",
        "\n",
        "Now, all we need is to write a new class to implement the Q function and then we should be able to use the `Q_learn` directly to do Neural Net Q-learning (NNQ). **The arguments to the `get` and `update` methods are the same as those in `TabularQ`; please review those now.**"
      ]
    },
    {
      "metadata": {
        "id": "2tXUtkSTE7fk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "We will assume that we have a small number of actions and we will use a neural net to learn the Q values for each action. Keep a dictionary of these NN models in `self.models` using the action as the key. Note that you have access to `make_nn` which was previously described.\n",
        "\n",
        "For each `update` call, create one training set for each action *a*:\n",
        "\n",
        "1. Extract the tuples from your data set that contain action aa. Recall that the `data` tuples are of the form `(s, a, t)`, where t is a target Q value.\n",
        "2. Let the *X* values of your training set be all of the *s* values from your data tuples with action *a* and the *Y* values be the target Q values given in the data tuples.\n",
        "\n",
        "Train the network for action *a* for the specified number of epochs, using `self.models[a].fit(X, Y, epochs=epochs)`. In Basic Q-Learning, we will always use `epochs=1`, but make this a general argument in creating an instance of the class. **Ignore the `lr` argument to the `update` method, we'll let Adam pick learning rates.**"
      ]
    },
    {
      "metadata": {
        "id": "EKE6GBaxveRi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class NNQ:\n",
        "    def __init__(self, states, actions, state2vec, num_layers, num_units, epochs=1):\n",
        "        self.actions = actions\n",
        "        self.states = states\n",
        "        self.epochs = epochs\n",
        "        self.state2vec = state2vec\n",
        "        state_dim = state2vec(states[0]).shape[1] # a row vector\n",
        "        self.models = {a:make_nn(state_dim, num_layers, num_units) for a in actions}\n",
        "    def get(self, s, a):\n",
        "        return self.models[a].predict(self.state2vec(s))\n",
        "    def update(self, data, lr):\n",
        "        for a in self.actions:\n",
        "            if [s for (s, at, t) in data if a==at]:\n",
        "                X = np.vstack([self.state2vec(s) for (s, at, t) in data if a==at])\n",
        "                Y = np.vstack([t for (s, at, t) in data if a==at])\n",
        "                self.models[a].fit(X, Y, epochs = self.epochs, verbose = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mjf3R9wYNcaS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "f4c7ce2a-30a1-4fe4-c55f-103d9868cebb"
      },
      "cell_type": "code",
      "source": [
        "make_nn = mdp.make_nn\n",
        "mdp.NNQ = NNQ\n",
        "importlib.reload(code_for_hw10)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "       table#id8, #id8 > tbody > tr > th, #id8 > tbody > tr > td {\n",
              "         border: 1px solid lightgray;\n",
              "         border-collapse:collapse;\n",
              "         \n",
              "        }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": [
              "outputarea_id8"
            ]
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<table id=id8><tr><td id=id8-0-0></td><td id=id8-0-1></td><td id=id8-0-2></td><td id=id8-0-3></td><td id=id8-0-4></td><td id=id8-0-5></td><td id=id8-0-6></td><td id=id8-0-7></td><td id=id8-0-8></td><td id=id8-0-9></td></tr></table>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": [
              "outputarea_id8"
            ]
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<module 'code_for_hw10' from '/content/code_for_hw10.py'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "metadata": {
        "id": "UJ1PYVqe9Vq0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the next code block to test your implementation of `NNQ`."
      ]
    },
    {
      "metadata": {
        "id": "WYNYWo7BvfIC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9aaad895-b29b-4baa-9658-1d635b1c15bf"
      },
      "cell_type": "code",
      "source": [
        "def test_NNQ(data):\n",
        "    tiny = mdp.MDP([0, 1, 2, 3, 4], ['a', 'b'], tinyTrans, tinyR, 0.9)\n",
        "    tiny.terminal = tinyTerminal\n",
        "    q = NNQ(tiny.states, tiny.actions, tiny.state2vec, 2, 10)\n",
        "    q.update(data, 1)\n",
        "    ret =  [q.get(s,a) for s in q.states for a in q.actions]\n",
        "    expect = [np.array([[-0.07211456]]), np.array([[-0.19553234]]), \n",
        "              np.array([[-0.21926211]]), np.array([[0.01699455]]), \n",
        "              np.array([[-0.26390356]]), np.array([[0.06374809]]), \n",
        "              np.array([[0.0340214]]), np.array([[-0.18334733]]), \n",
        "              np.array([[-0.438375]]), np.array([[-0.13844737]])]\n",
        "    cnt = 0\n",
        "    ok = True\n",
        "    for s in q.states:\n",
        "      for a in q.actions:\n",
        "        if not np.all(np.abs(ret[cnt]-expect[cnt]) < 1.0e0):\n",
        "          print(\"Oops, for s=%s, a=%s expected %s but got %s\" % (s, a, expect[cnt], ret[cnt]))\n",
        "          ok = False\n",
        "        cnt += 1\n",
        "    if ok:\n",
        "      print(\"Output looks generally ok\")\n",
        "    return q\n",
        "  \n",
        "test_NNQ([(0,'a',0.3),(1,'a',0.1),(0,'a',0.1),(1,'a',0.5)])"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output looks generally ok\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.NNQ at 0x7fcad87f3c18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "metadata": {
        "id": "quq-Uh44_Bbo",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2) Fitted Q iteration\n",
        "\n",
        "*Fitted Q iteration (FQ)* suffers less from the correlated experience problem and is generally more stable (and sometimes slower) than NNQ.\n",
        "\n",
        "FQ initializes the Q networks and an empty data set, then operates in a loop:\n",
        "\n",
        "1. Use *ϵ*-greedy exploration to generate *k* steps of experience, of the form *(s,a,r,s′)* and add them to the data set.\n",
        "2. Create one training set for each action *a*:\n",
        "\n",
        "> 1. Extract all the tuples from your data set that contain action *a*,\n",
        "> 2. Let the *X* values of your training set be all of the *s* values from your data tuples with action *a* and the *Y* values be the *r + γ max_a' Q(s', a')* values computed for each data tuple, using the Q estimates from the current network.\n",
        "\n",
        "3. Train the network for action *a* for several epochs until it has done a good job of representing this data.\n",
        "\n",
        "So, this is basically `Q_learn_batch` using `NNQ` (training with multiple epochs) to implement the Q function."
      ]
    },
    {
      "metadata": {
        "id": "0kW_URSm9mOI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Run the next code block to test your implementation of `NNQ` with batching."
      ]
    },
    {
      "metadata": {
        "id": "SY6H72oV4z5O",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        },
        "outputId": "ff3e5456-877a-45cd-b16b-4cf72b96b30e"
      },
      "cell_type": "code",
      "source": [
        "# Test: NN Batch Q-learn (Fitted Q-learn)\n",
        "code_for_hw10.test_learn_play(iters=100000, tabular=False, batch=False)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "score (0, 9.9)\n",
            "score (10000, 12.4)\n",
            "score (20000, 100.0)\n",
            "score (30000, 66.3)\n",
            "score (40000, 100.0)\n",
            "score (50000, 100.0)\n",
            "score (60000, 75.1)\n",
            "score (70000, 100.0)\n",
            "score (80000, 100.0)\n",
            "score (90000, 91.7)\n",
            "String to upload (incude quotes): \"286c70300a4930300a614930300a61286c70310a284c304c0a46392e390a7470320a61284c31303030304c0a4631322e340a7470330a61284c32303030304c0a463130302e300a7470340a61284c33303030304c0a4636362e330a7470350a61284c34303030304c0a463130302e300a7470360a61284c35303030304c0a463130302e300a7470370a61284c36303030304c0a4637352e310a7470380a61284c37303030304c0a463130302e300a7470390a61284c38303030304c0a463130302e300a747031300a61284c39303030304c0a4639312e370a747031310a61612e\"\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3XtYVWW+B/AviJqViamIgQgIKBc1\nNGPqOA3Kg8dRB05KmVmZWjpZqZm3uZxp5kyTt8lLZdNQNlFndHK6qCc8TKk4Nl5GU097e79hKhiT\ngCYqctm/88cvCBV0s29rre338zw+PCw3a/1Ye/Pda7/vu943QEQERETktwKNLoCIiLyLQU9E5OcY\n9EREfo5BT0Tk5xj0RER+jkFPROTnGPRERH7uukE/duxYhISEICkpqW5baWkp0tPTERsbi/T0dJSV\nlQEARASTJk1CTEwMevbsiZ07d3qvciIicsp1g/7xxx9HXl7eZdvmzJmDtLQ0HDp0CGlpaZgzZw4A\n4H//939x6NAhHDp0CNnZ2Xjqqae8UzURETktwJk7Y48dO4ahQ4di9+7dAIBu3bphw4YN6NSpE06d\nOoXU1FQcOHAAEyZMQGpqKkaOHHnV467lBz/4AbZu3eqBX8cPORzA/fcDq1d/vy0jA/j4YyCQLW+X\naehcDRoE5OQYd64cDmD0aKD+xZKJnr8tW7bgnnvuMboMcz53tXWZ+PlzVpArP1RcXFwX3qGhoSgu\nLgYAFBYWonPnznWPCw8PR2Fh4XWDvrbphxqQl3f5ix/Q7/PygMGDjanJrBo6V3l5QMeOxtTTGBM9\nf5cuXTK6BGWV5w4w1fPnLJeCvr6AgAAEBAQ0+eeys7ORnZ0NQIN+w4YN7pbil7p8+CGiGth+9KOP\ncPzmm31ej5k1dq5O33MPyvr29Xk9ANB22za0b+DTqlmev/LyclP87ZnxuQMaf/5OLF2KIyZ4/lJT\nU517oDihoKBAEhMT676Pi4uToqIiEREpKiqSuLg4EREZP368LFu2rMHHXUvtz1MDcnNFgKv/5eYa\nXZn5fPKJ+c6VyZ+//Px8o0tQZj1PjdUVECAydqzI8ePG1ucklxqZMjIykJOTAwDIyclBZmZm3fZ3\n330XIoKtW7eiTZs21222oesYNEjbBOuLiNDtdLkWLa7elpFh7Llq6Plr3x749383ph6z6tDh6m1G\nP3dAw8/foEHA5MnAf/83EBsLPP88UFJiTH3Out47wUMPPSShoaESFBQkYWFh8tZbb8np06dlwIAB\nEhMTI2lpaVJSUiIiIg6HQyZOnCjR0dGSlJQk27dvd+rdhlf011FZKdKihZQlJooMGSLSvLlIYaHR\nVZlPerpIx44iH38sR8aN06uxmhqjq9IacnNFXnxR5Kmn9IrwvfeMrkpETHJFX1Ul0ru3Pnfvv6/n\nySzPnUjd83fVa+rYMZHHHxcJDBS57TaR3/5W5Nw5Y2tthFNNN97GoL+OAwdEANk3c6bIkSP6wpo+\n3eiqzGXHDg3QOXNExCQB1pDqapGUFJH27UVOnza6GnOcp4UL9bl7/32jK7mmRs/V7t0i//Ef+juE\nhIi8+qrIpUs+re16rDM+6EZmtwMAyqOigOho4IEHgDfeAM6eNbgwE5k/H2jdGpgwwehKrq1ZMyA7\nGygrA2bONLoa4x0/DvzylzqC5YEHjK7GNYmJOtxyyxYgPh549lmge3dt2nE4jK4OAKdAsAabDQgM\nxIXISP1+5kzg3DkNewKOHgVWrAB++lMgONjoaq6vZ09t1126FNi40ehqjCMCPPOMfl2yBHBh9J6p\n/OAHQH6+Dr1s0wZ49FEgORnIzdXf0UAMeiuw2YDYWDhattTvk5OB9HRg0SLALOOgjbRggV4pT55s\ndCXOe+EFIDJSP4HcqM/hypXA//wP8Jvf6LnwBwEB2tG+YwewfDlw/jwwdChw333Apk2GlcWgtwK7\nXa8C65sxA/j6a+C994ypySy++QZ4+229egoLM7oa5918M/CHPwD79wNz5xpdje99+602cfTqZa03\naGcFBgIPPQTs26fP8+HDQL9+OoLnu6ZYn5bj8yNS05SXA0eOAD16XL49LQ3o3Vvbpk3SDmiI114D\nLl4Epk0zupKmGzRIw+B3vwMOHjS6Gt/65S+BoiLtr2je3OhqvKd5c21SPHwYeOklbarr1Qt47DGg\noMBnZTDozW7PHv165RV9QIBe1R88CKxa5fu6zOD8eQ36zEztBLOihQuBVq00DAxux/WZbdv0eXv6\naeDuu42uxjduuQX42c+0P2n6dOCvfwW6ddNPM//6l9cPz6A3O5tNv14Z9AAwfLiOwpk798YJifqW\nLgVKS/UNz6pCQ/X5y88H3n3X6Gq8r7oaGD8e6NRJP8ncaG6/XZ/vw4eBMWO0Ezo6Wvtsvv3Wa4dl\n0JudzQbceivQpcvV/xcUpKM3/vlP4PPPfV+bkaqqgJdf1nbPe+81uhr3PPmk/g7PPw+cPm10Nd61\naBHw5ZfAq68Ct91mdDXGCQsD/vhH/cQ+eDDwX/+lgb9wIVBR4fHDMejNzm7X9vnGpkQdM0ZvH583\nz7d1GW3FCh2DbeWr+VqBgfpHf/asNfsanHXsmF65/uQnOiUxafPNihXA9u3a5zZ1qm575x2gpsZj\nh2HQm5mIXtFf2RFbX6tWOnohNxf4br0Avyeib2wJCcCQIUZX4xlJSfqmlZOjzTj+RkTb5AMCtH3e\n6mPmPe2uu4BPPwXWrgVCQvQCrmdPHYLqgWZZBr2ZFRXpHZQNtc/X9/TTOlxv/nzf1GW0v/1N3wCn\nT7fU4g/X9ctf6sf3CRO88vHdUB98AKxZA/z2tzopHzUsLU07qz/4QK/o779fm/X+/ne3dutHfyV+\n6FodsfXdfru28y5bps0Z/m7uXG3jfPhhoyvxrFat9G7nQ4eA2bONrsZzzp4FJk3SpolnnzW6GvML\nCNCBFrt3A2+9BZw4AaSmAj/+MbBrlz7G4dA3zhdfdGqXDHozqw36eguzN2rqVP2It2iRd2sy2rZt\nwIYNwHPPNTwtsdWlpwOjRmnQ799vdDWe8bOf6RDC7GwdQEDOCQoCxo3TN/7583XQRe/eeu/FwIHa\nbPmf/+nUrhj0Zma3A507A23bXv+xERHAyJH6x1Ra6v3ajDJvns5nM3680ZV4z4IFOtJqwgTr3wy3\nZYt+Snn2WaBPH6OrsaZWrbST/uhR4Be/0Hb7deuatAsGvZldryP2SjNm6E1Ef/iD92oy0qFDwEcf\nARMn6kyV/iokRK/gNm7U0RdWVVWlb8hhYdo2T+4JDtamGhemjGDQm1VlpX50v177fH09emg73uLF\nOi2Av/n977W5ZtIkoyvxvjFjgB/+UK/kfHDnpFcsWKDtzK+95t9vzL72ox81+UcY9GZ14IBeETUl\n6AGdwvibb3SYnj/5+mv9nR5/HOjY0ehqvK92bH15ud5IZTVHj+qslPffr1NUkOc0tLzhdTDozaq2\nI7YpTTeATod699169evBGy4M98or+inHiqHnqvh4YNYsXcBi7Vqjq3GeiDavNWumzxt5VmCgLnSS\nm8tRN5Znt+vMd926Ne3nAgL0qv7IEW3P9gfnzgGvv65DzmJjja7Gt37+c/2df/pT6zTHvf++3uvw\nu98B4eFGV+OfAgN16oRf/MK5h3u5HHKVzaZXdK5M4ZqZqeHgL5OdZWfrWGx/mO6gqW66SUetHDli\njUnAysq0s/Cuu/RGPjIFBr1ZNbTYiLOaNdO7RnfsANav92xdvlZZqRM99e8P9O1rdDXGGDBA5y+f\nO/f7aavNatYsoKRE35ybNTO6GvoOg96MSkuBkyddD3pAV1zq2NH6k50tWwYUFt6YV/P1vfyyrkNq\n5rH1//iHBvyUKbrcJZkGg96Mapcaa2pHbH033aR/cJ9++v1t01bjcOgbVc+eug7njax9e+1g37RJ\n5+E3m8pKfROKiAB+/Wujq6ErMOjNqDbo3bmiB7QDr3Vr6052lpura27OmMHZDgFg9Gid86R2vWAz\n+f3vgb17dSGNW281uhq6AoPejGw2naisUyf39hMcrFdZ77/v0/UpPWbuXF1w5cEHja7EHAICtGP2\nwgWd28gsDh/WhTOysoChQ42uhhrAoDej2o5YT1zFTpminWILFri/L1/atEn/TZ3q34tHN1W3bjrk\ncvlyIC/P6Gp0VNdTTwEtW+od2WRKDHqzcTjcG3FzpbAw4JFHtF33m288s09fmDdPP9WMG2d0JeYz\na5YG/sSJenVvpD//WW/mmj0buOMOY2uhRjHozaagQCcmc6cj9krTp+vNNkuWeG6f3rRvH7B6NfDM\nM8Attxhdjfm0bKnTIxQUGDtZWEmJThedkqJNhGRaDHqz8VRHbH3x8To3xquv6puI2c2fr1OzPvOM\n0ZWY149+BIwdq52gta8ZX5s5U2+Q4ph502PQm43Npm3ziYme3e/MmTo+/+23PbtfTyss1Lldxo7V\nRc+pcfXn5vf12PqNG7U58PnnPXtRQl7BoDcbux3o2tXzTRb33gv827/pjTdVVZ7dtyctWqSTsd1I\nk5e5ql077WTfulWbcnzl0iVtqomMBH71K98dl1zGoDcbm817V0gzZwJffQX89a/e2b+7zpzRwHrw\nQSAqyuhqrOGRR3RB6VmzgFOnfHPMuXN1rYTXX2cfikUw6M3kwgVdRcmTHbH1DRkCJCToR34zTnb2\nxhs6U+WNPt1BUwQE6Ipily65tPJQkx08qJOrjRihi9yQJTDozWTvXg1gb13RBwbqCJwvv9SpEcyk\nokLHYQ8cyHlSmio2VheJ/utf9W5ibxHRu61btfL/Rej9DIPeTFxdbKQpHn5Yx9bPneu9Y7jivff0\ntn5ezbtm+nQdXfX0094bWfXuu0B+vr52QkO9cwzyCga9mdjtwM03A9HR3jtGixY69jk/H9i+3XvH\naYqaGh0m2KePTslLTdeihQ5z/Oor70wqdvq0dpDfey/w5JOe3z95FYPeTGw2ICnJ+2OSn3xSp7w1\nyxTGq1Zp2y8nL3NPv3763C5cCPzf/3l239Om6eIvf/yjNgGSpfAZMwsRDXpvNtvUuu02vX3+ww91\nQiojiWhTQHS0LhVI7pk7V4ddjh/vuTWD8/N1Yfbp0/VChCyHQW8WxcX68dhXN59MmqQf93//e98c\nrzEbNwLbtukVI++udF/bttpRun27jsZxV0WFjpmPjtYOX7Ikt4J+4cKFSExMRFJSEkaOHImKigoU\nFBQgJSUFMTExGDFiBCorKz1Vq3/zRUdsfaGhOr/5O+/om4xR5s3TO2Aff9y4GvzNQw/p6KWf/1zv\nNHbH7Nk65PeNN3S0DVmSy0FfWFiIV155BV988QV2796Nmpoa/OUvf8HMmTPx3HPP4fDhw2jbti2W\nmnE1HDPyxKpSTfX887oy0Cuv+O6Y9dntwJo1+umCIeI5tWPrq6r03Lpq3z4N+lGjgPR0z9VHPufW\nFX11dTUuXryI6upqXLhwAZ06dcL69euRlZUFABg9ejRWrlzpkUL9ns2m07y2b++7Y8bFAcOG6R2O\n58757ri15s/XOysnTvT9sf1ddDTwwgvARx/pTKBN5XBok82tt1pvLQO6SpCrPxgWFoZp06YhIiIC\nrVq1wsCBA9GnTx8EBwcjKEh3Gx4ejsJGPjpmZ2cjOzsbAFBWVoYNGza4Wopf6LN5M6rCwmC7xnko\nLy/3+HlqnZaGPh9+iMMzZ+KkD1dyallcjJRly1B4//04Utts5UHeOFdWE3DXXegTFYWgJ5/E9ubN\nUdPAp6bGzlPomjXo/vnnODBtGk7t3as3893gzPiaSk1Nde6B4qLS0lLp37+//Otf/5LKykrJzMyU\n9957T7p27Vr3mOPHj0tiYuJ19xUXF+dqGf6hqkqkRQuR6dOv+bD8/HzvHD81VSQsTOTSJe/svyGT\nJ4sEBYl89ZVXdu+1c2U1mzeLACLPPdfgfzd4noqLRdq2FfnhD0Vqarxbn4VY+TXlctPN2rVrERUV\nhQ4dOqB58+YYNmwYNm3ahDNnzqC6uhoAcPLkSYSFhbl6iBvHwYPaVu7L9vn6ZszQTrvly31zvJIS\n4M03gZEjgYgI3xzzRnXPPTptweLFwM6dzv3M888D5eUcM+9HXH4WIyIisHXrVly4cAEignXr1iEh\nIQH9+/fHBx98AADIyclBZmamx4r1W95YbKQpBg3SY8+b55t5zV9/XSdwmz7d+8ci7VANCdGx9d9d\nhDXqs890PYBZs3RKBfILLgd9SkoKsrKy0Lt3b/To0QMOhwPjx4/H3LlzsWDBAsTExKCkpATjuObn\n9dlsQFAQ0L27MccPCNCr+r17dRSMN128qKN8Bg827hPMjSY4WK/od+y49nKSFy/qQt8xMTo0k/yH\n0W1HImyjl5/8RMSJvgyvthFWVopERIj06+e9Y4iILFmibcZ//7tXD2Pl9lSvcDhEfvxjkVtvFTl+\nvG7zZefp5z/X52btWt/XZwFWfk2xAc4MvLnYiLOaN9e22X/8A9i82TvHqK7WFa5SUoAf/tA7x6CG\nBQRok1lNDfDss1f//5492nT32GO6kAn5FQa90c6e1RkHzdCMMW4ccPvt3pvs7MMPgaNHdaUrTl7m\ne5GRwG9+o5PI1b+/pXbMfJs2xk+JQV7BoDfa7t361egrekBvXnrmGQ2C/fs9u28RfQOJiwMyMjy7\nb3LelClAr176PH/7rW576y1g0yYNeS7I7pcY9Ebz9Rw31/PMMzodwfz5nt3vunU6vG/6dE5eZqTm\nzXXYZGEh8OijiM7OBqZOBe67T+c+Ir/EoDea3a4fmTt3NroS1aEDMHasrvhUVOS5/c6bpxOpPfqo\n5/ZJrunbV5txVq9GxPLluiJVUJA51xEmj2DQG622I9ZMbdZTp2qnnafWBd25U8dnT5kCtGzpmX2S\n6/LygGPHLt+2fr1uJ7/EoDeSiF7Rm6XZplZ0NPDggzo17dmz7u9v/nygdWu9Q5OM19gdsrt2+bYO\n8hkGvZGOH9cOMTN0xF5pxgyd0fKNN9zbz9GjwIoVGvJt2nimNnJP794Nb09O9m0d5DMMeiOZrSO2\nvuRknYN80SLg0iXX97NggXa+TpniudrIPYMGXT3yKSNDt5NfYtAbqXaOG7OuwzljBvD119ox64pv\nvgHefls7YO+4w7O1kesCA4GPPwZyc3F03DggN1e/5wRmfovPrJFsNiAqShfrNqO0NP2YP3++a5Od\nvfaazp/CycvMJzAQGDwYxx95ROcdYsj7NT67RrLZzNlsU6t2srODB/UmqqY4f16DPjPTuMnaiAgA\ng944FRUaoGbsiK1v+HAdhTN3btPGWS9dCpSW6nQHRGQoBr1R9u3TsepmvqIH9Eaa558H/vlP4PPP\nnfuZqiqdvKxfP134gogMxaA3itGLjTTFmDF6x6yzk52tWKFDR3k1T2QKDHqj2GzATTfpIg9m16qV\nTm2bm/v9JGyNqZ28LCFBO/mIyHAMeqPYbBqGQUFGV+Kcp58Gbr75+lf1eXn6u82YwZEcRCbBv0Sj\n2O3WaLapdfvtwJNP6gLix483/rh584DwcF34m4hMgUFvhG++0RuRzN4Re6WpU/XrwoUN//+2bcCG\nDcBzzwEtWvisLCK6Nga9EazUEVtfRIReqb/5pg6dvNK8eboQ9ZNP+r42ImoUg94ItXPcWC3oAb3L\n9fx5XX+0voMHgY8+AiZO1Jkqicg0GPRGsNmAkBD9ZzU9euhomlde0ekNar38sjbXTJpkXG1E1CAG\nvRGs1hF7pRkztJ/hnXf0+6+/BnJygMcfBzp2NLIyImoAg97Xamp0LLrVOmLru+8+ICVFF5OuqdGr\n+8pKYNo0oysjogYw6H3tyBGd58bKV/S1k50dPaojcRYsAO69V+fEISLTYdD7mpU7Yuv7yU+AW27R\nq/lLl4BNm4D773dtOmMi8ioGva/ZbHrHaHy80ZW457PPdPRNfatXc4FpIhNi0Pua3Q7Exen8MVbG\nBaaJLINB72tmX2zEWVxgmsgyGPS+VF6uHZhWb58HuMA0kYVYZOpEP1E7xa8/BH3tAtN5edpck5ys\nIc8ZK4lMh0HvS7Ujbvyh6QaoW2Ca884TmRsvv3zJbtd5YLp0MboSIrqBMOh9yWYDkpLYvEFEPsXE\n8RUR689xQ0SWxKD3lcJCoKyMQU9EPseg9xV/64glIstg0PtK7apSDHoi8jG3gv7MmTPIyspC9+7d\nER8fjy1btqC0tBTp6emIjY1Feno6ysrKPFWrtdlsQOfOutQeEZEPuRX0kydPxqBBg7B//358+eWX\niI+Px5w5c5CWloZDhw4hLS0Nc+bM8VSt1saOWCIyiMtBf/bsWWzcuBHjxo0DALRo0QLBwcFYtWoV\nRo8eDQAYPXo0Vq5c6ZlKrayyEti3j0FPRIZwOegLCgrQoUMHjBkzBsnJyXjiiSdw/vx5FBcXo1On\nTgCA0NBQFBcXe6xYyzpwAKiuZvs8ERnC5SkQqqursXPnTrz66qtISUnB5MmTr2qmCQgIQEBAQIM/\nn52djezsbABAWVkZNmzY4Gopphfy2WdIALCtogIX3Pg9y8vL/fo8eRLPlXN4npxnxnOVmprq3APF\nRadOnZIuXbrUfb9x40YZPHiwxMXFSVFRkYiIFBUVSVxc3HX35cxjLG3GDJHmzUUqK93aTX5+vmfq\nuQHwXDmH58l5Vj5XLjfdhIaGonPnzjhw4AAAYN26dUhISEBGRgZycnIAADk5OcjMzHT1EP7DbgcS\nEoDmzY2uhIhuQG7NXvnqq69i1KhRqKysRHR0NP70pz/B4XDgwQcfxNKlS9GlSxesWLHCU7Val80G\n9O9vdBVEdINyK+jvvPNOfPHFF1dtX7dunTu79S+lpTr9AUfcEJFBeGest9XeEcugJyKDMOi9jXPc\nEJHBGPTeZrcD7doB391bQETkawx6b7PZ9Gq+kfsJiIi8jUHvTQ6HLgjO9nkiMhCD3psKCoDz5xn0\nRGQoBr03sSOWiEyAQe9Ndru2zScmGl0JEd3AGPTeZLMBXbsCt9xidCVEdANj0HsTFxshIhNg0HvL\nhQvAoUMMeiIyHIPeW/bsAUTYEUtEhmPQewvnuCEik2DQe4vNBtx8MxAdbXQlRHSDY9B7i90OJCUB\ngTzFRGQsppA3iABffslmGyIyBQa9N3z9NVBSwo5YIjIFBr03sCOWiEyEQe8NnOOGiEyEQe8Ndjtw\nxx264AgRkcEY9N5gs7HZhohMg0HvaVVVwN69bLYhItNg0HvaoUNAZSWv6InINBj0nsaOWCIyGQa9\np9ntQFAQ0L270ZUQEQFg0HuezaYh37Kl0ZUQEQFg0HuezcZmGyIyFQa9J509Cxw/zo5YIjIVBr0n\n1U59wCt6IjIRBr0ncY4bIjIhBr0n2WxAcDAQHm50JUREdRj0nlTbERsQYHQlRER1GPSeIqJNN2y2\nISKTYdB7yldfAefOsSOWiEyHQe8p7IglIpNi0HtK7Rw3SUnG1kFEdAUGvafYbEBUFNC6tdGVEBFd\nhkHvKeyIJSKTYtB7QkUFcPAgO2KJyJTcDvqamhokJydj6NChAICCggKkpKQgJiYGI0aMQGVlpdtF\nmt6+fUBNDa/oiciU3A76xYsXIz4+vu77mTNn4rnnnsPhw4fRtm1bLF261N1DmF9tRyyDnohMyK2g\nP3nyJHJzc/HEE08AAEQE69evR1ZWFgBg9OjRWLlypftVmp3dDtx0ExATY3QlRERXcSvop0yZgnnz\n5iEwUHdTUlKC4OBgBAUFAQDCw8NRWFjofpVmZ7MBiYlAs2ZGV0JEdJUgV3/wk08+QUhICPr06YMN\nGzY0+eezs7ORnZ0NACgrK3NpH2Zx7xdfoCQlBQe8/DuUl5db+jz5Es+Vc3ienGfGc5WamurcA8VF\ns2bNkrCwMOnSpYt07NhRWrVqJQ8//LC0a9dOqqqqRERk8+bNMnDgwOvuKy4uztUyjFdcLAKILFjg\n9UPl5+d7/Rj+gufKOTxPzrPyuXK56Wb27Nk4efIkjh07hr/85S8YMGAA/vznP6N///744IMPAAA5\nOTnIzMx09RDWwMVGiMjkPD6Ofu7cuViwYAFiYmJQUlKCcePGefoQ5sI5bojI5Fxuo68vNTW1rq0o\nOjoa27Zt88RurcFmAzp2BEJCjK6EiKhBvDPWXbWLjRARmRSD3h01NcCePWy2ISJTY9C74/BhneeG\nV/REZGIMenewI5aILIBB7w6bDQgMBBISjK6EiKhRDHp32GxAXJzOc0NEZFIMendwsREisgAGvavO\nnQOOHmVHLBGZHoPeVXv26Fde0RORyTHoXcXFRojIIhj0rrLZgNatgS5djK6EiOiaGPSustu1fT4g\nwOhKiIiuiUHvChHOcUNElsGgd0VhIXDmDNvnicgSGPSuYEcsEVkIg94VtUGflGRsHURETmDQu8Ju\nByIigOBgoyshIrouBr0r2BFLRBbCoG+qykpg/362zxORZTDom2r/fqC6mkFPRJbBoG+q2o5YNt0Q\nkUUw6JvKbgdatNB56ImILIBB31Q2GxAfDzRvbnQlREROYdA3FRcbISKLYdA3RUmJTn/AoCciC2HQ\nN4Xdrl/ZEUtEFsKgb4raoOcVPRFZCIO+KWw2oF07IDTU6EqIiJzGoG+K2o5YLjZCRBbCoHeWw8ER\nN0RkSQx6Zx09Cly4wI5YIrIcBr2z2BFLRBbFoHeWzaZt84mJRldCRNQkDHpn2e1ATAxw881GV0JE\n1CQMemfZbGy2ISJLYtA748IF4PBhdsQSkSUx6J2xZw8gwit6IrIkBr0zuNgIEVkYg94Zdrt2wkZH\nG10JEVGTuRz0J06cQP/+/ZGQkIDExEQsXrwYAFBaWor09HTExsYiPT0dZWVlHivWMDYbkJQEBPJ9\nkYisx+XkCgoKwssvv4y9e/di69atWLJkCfbu3Ys5c+YgLS0Nhw4dQlpaGubMmePJen1PhCNuiMjS\nXA76Tp06oXfv3gCA1q1bIz4+HoWFhVi1ahVGjx4NABg9ejRWrlzpmUqN8vXXuuAIg56ILCpARMTd\nnRw7dgz33Xcfdu/ejYiICJw5cwYAICJo27Zt3ff1ZWdnIzs7GwBw/PhxrFixwt0yvKLttm3oNXMm\n/m/hQpy5805DaykvL8ett96uZj5EAAAJhklEQVRqaA1WwXPlHJ4n55nxXKWmpjr3QHHTuXPnpHfv\n3vLhhx+KiEibNm0u+//g4ODr7iMuLs7dMrxn/nwRQOT0aaMrkfz8fKNLsAyeK+fwPDnPyufKrd7F\nqqoqDB8+HKNGjcKwYcMAAB07dsSpU6cAAKdOnUJISIg7hzCezQbccYcuOEJEZEEuB72IYNy4cYiP\nj8fUqVPrtmdkZCAnJwcAkJOTg8zMTPerNBLnoCcii3M56Ddt2oT33nsP69evx5133ok777wTa9as\nwaxZs/DZZ58hNjYWa9euxaxZszxZr29VVQF79zLoicjSglz9wX79+kEa6cddt26dywWZysGDQGUl\n74glIkvjHUDXwsVGiMgPMOivxWYDgoKA7t2NroSIyGUM+mux2zXkW7QwuhIiIpcx6K+FUx8QkR9g\n0DfmzBng+HF2xBKR5THoG7N7t37lFT0RWRyDvjFcbISI/ASDvjF2OxAcDISHG10JEZFbGPSNqe2I\nDQgwuhIiIrcw6Bsiolf0bLYhIj/AoL+SwwG88w5w7pxezTscRldEROQWBn19Dgdw//3A2LH6/Wuv\n6fcMeyKyMJcnNbO86mrg6FFg3z6doXLfPmDLFuDw4csft3o1kJcHDB5sTJ1ERG7y/6C/eBE4cECD\nvP6/Q4d0Zspad9wB3HRTw/vYtYtBT0SW5T9BX1Z2dZjv2wccO6adqwAQGAhERwPx8cCQIfo1Pl7n\ns2nTBlizRrdfKTnZp78KEZEnmSLoW5SUaMgOGqRh3BgRoKio4UAvLv7+cS1bAt26AXffDYwe/X2g\nx8Y2ftUO6PEzMrS5plZGhm4nIrIoUwR9y5ISvZLOyAA+/lgDvbb9vP6//fuBb7/9/gfbtNEAHzz4\n+zCPjwciI4FmzZpeSGCgHj8vT5trkpOv/+ZDRGRypgj6OqtXa9PKqVOXt5936qQB/uijlwd6aKjn\nb2gKDNQ3DrbJE5GfMFfQA9q0Mnny5e3nwcFGV0VEZFnmC/oFC3g1TUTkQQHS2ArfPtQ+IACRgLa5\nx8QYXI15ffPNN+jQoYPRZVgCz5VzeJ6cZ8Zz1b59e+Tl5V33caa4oo/s0wdffPGF0WWY3l133cXz\n5CSeK+fwPDnPyueKw0mIiPwcg56IyM81+/Wvf/1ro4sAgD59+hhdgiXwPDmP58o5PE/Os+q5MkVn\nLBEReQ+bboiI/JzhQZ+Xl4du3bohJiYGc+bMMbocnzhx4gT69++PhIQEJCYmYvHixQCA0tJSpKen\nIzY2Funp6SgrKwMAiAgmTZqEmJgY9OzZEzt37qzbV05ODmJjYxEbG4ucnJy67Tt27ECPHj0QExOD\nSZMmwcof3GpqapCcnIyhQ4cCAAoKCpCSkoKYmBiMGDECld/dRX3p0iWMGDECMTExSElJwbFjx+r2\nMXv2bMTExKBbt27429/+VrfdX15/Z86cQVZWFrp37474+Hhs2bKFr6dGLFy4EImJiUhKSsLIkSNR\nUVHh/68pMVB1dbVER0fLkSNH5NKlS9KzZ0/Zs2ePkSX5RFFRkezYsUNERL799luJjY2VPXv2yPTp\n02X27NkiIjJ79myZMWOGiIjk5ubKoEGDxOFwyJYtW+Tuu+8WEZGSkhKJioqSkpISKS0tlaioKCkt\nLRURkb59+8qWLVvE4XDIoEGDZM2aNQb8pp7x8ssvy8iRI2XIkCEiIvLAAw/I8uXLRURkwoQJ8vrr\nr4uIyJIlS2TChAkiIrJ8+XJ58MEHRURkz5490rNnT6moqJCjR49KdHS0VFdX+9Xr77HHHpM333xT\nREQuXbokZWVlfD014OTJkxIZGSkXLlwQEX0t/elPf/L715ShQb9582YZOHBg3fcvvfSSvPTSSwZW\nZIyMjAz59NNPJS4uToqKikRE3wzi4uJERGT8+PGybNmyusfXPm7ZsmUyfvz4uu21jysqKpJu3brV\nbb/ycVZy4sQJGTBggKxbt06GDBkiDodD2rVrJ1VVVSJy+Wto4MCBsnnzZhERqaqqknbt2onD4bjq\ndVX7OH95/Z05c0YiIyPF4XBctp2vp6udPHlSwsPDpaSkRKqqqmTIkCGSl5fn968pQ5tuCgsL0blz\n57rvw8PDUVhYaGBFvnfs2DHs2rULKSkpKC4uRqdOnQAAoaGhKP5u6uXGztO1toeHh1+13YqmTJmC\nefPmIfC7GURLSkoQHByMoCC916/+71b/fAQFBaFNmzYoKSlp8vmzmoKCAnTo0AFjxoxBcnIynnji\nCZw/f56vpwaEhYVh2rRpiIiIQKdOndCmTRv06dPH719ThrfR38jKy8sxfPhwLFq0CLfddttl/xcQ\nEIAAT8/MaTGffPIJQkJCLDukzVeqq6uxc+dOPPXUU9i1axduueWWq9qG+XpSZWVlWLVqFQoKClBU\nVITz5887NYWA1Rka9GFhYThx4kTd9ydPnkRYWJiBFflOVVUVhg8fjlGjRmHYsGEAgI4dO+LUqVMA\ngFOnTiEkJARA4+fpWttPnjx51Xar2bRpE1avXo3IyEg89NBDWL9+PSZPnowzZ86guroawOW/W/3z\nUV1djbNnz6Jdu3ZNPn9WEx4ejvDwcKSkpAAAsrKysHPnTr6eGrB27VpERUWhQ4cOaN68OYYNG4ZN\nmzb5/2vKyHajqqoqiYqKkqNHj9Z1XOzevdvIknzC4XDIo48+KpMnT75s+7Rp0y7rPJs+fbqIiHzy\nySeXdZ717dtXRLTzLDIyUkpLS6W0tFQiIyOlpKRERK7uPMvNzfXhb+h5+fn5dZ2xWVlZl3WcLVmy\nREREXnvttcs6zh544AEREdm9e/dlHWdRUVFSXV3tV6+/fv36yf79+0VE5IUXXpBp06bx9dSArVu3\nSkJCgpw/f14cDoc89thj8sorr/j9a8rQoBfREQCxsbESHR0tL774otHl+MTnn38uAKRHjx7Sq1cv\n6dWrl+Tm5srp06dlwIABEhMTI2lpaXV/ZA6HQyZOnCjR0dGSlJQk27dvr9vX0qVLpWvXrtK1a1d5\n++2367Zv375dEhMTJTo6Wp5++umrOuqspn7QHzlyRPr27Stdu3aVrKwsqaioEBGRixcvSlZWlnTt\n2lX69u0rR44cqfv5F198UaKjoyUuLu6yESP+8vrbtWuX9OnTR3r06CGZmZlSWlrK11MjfvWrX0m3\nbt0kMTFRHnnkEamoqPD71xTvjCUi8nPsjCUi8nMMeiIiP8egJyLycwx6IiI/x6AnIvJzDHoiIj/H\noCci8nMMeiIiP/f/zCH9/mAKRyAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 100\n",
            "Reward 30\n",
            "Reward 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-d4QW3g0ww5H",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#4) No Exit\n",
        "\n",
        "Please read the instructions in the [homework](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week10/week10_homework/) to set up for the game. You may use this space to display the game in Colab."
      ]
    },
    {
      "metadata": {
        "id": "zRbeO95qxJf4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "code_for_hw10.test_solve_play(draw = True)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}