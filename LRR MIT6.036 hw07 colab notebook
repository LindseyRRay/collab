{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LRR MIT6.036 hw07 colab notebook","version":"0.3.2","provenance":[{"file_id":"14pGqETHUQpCCVlkAU4rB_ueh5RjzcqzC","timestamp":1553379812293},{"file_id":"1EsEYYI5Ts2fcYDOY122fa96C_tAs0sgU","timestamp":1552923293807},{"file_id":"1-Bv9wNU6i8Qtv-oDyzh9Ux5PDvSzDDO5","timestamp":1551228788278},{"file_id":"1kIWGi2TEZ0Agd_qQt1KiD2uZw_5AJFHT","timestamp":1550587496262},{"file_id":"1j2fQR-NQxnazY5MSHqxwlS8z96n8YFMg","timestamp":1549860446001}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"_xIaEwCD406A","colab_type":"text"},"cell_type":"markdown","source":["#MIT 6.036 Spring 2019: Homework 7#\n","\n","This colab notebook provides code and a framework for problem 2 of [the homework](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week7/week7_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n","\n","## <section>**Setup**</section>\n","\n","First, download the code distribution for this homework that contains test cases and helper functions.\n","\n","Run the next code block to download and import the code for this lab.\n"]},{"metadata":{"id":"fgBQvO6jJqg5","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"2YM-_zLf9Bp-","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"7019d09d-0a48-4252-92ba-2f73b6117093","executionInfo":{"status":"ok","timestamp":1553443750591,"user_tz":240,"elapsed":7077,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["!rm -rf code_for_hw7*\n","!rm -rf mnist\n","!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw07/code_for_hw7.zip\n","!unzip code_for_hw7.zip\n","!mv code_for_hw7/* .\n","\n","from code_for_hw7 import *\n","import numpy as np\n","import modules_disp as disp"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Archive:  code_for_hw7.zip\n","   creating: code_for_hw7/\n","  inflating: code_for_hw7/code_for_hw7.py  \n","  inflating: code_for_hw7/expected_results.py  \n","  inflating: code_for_hw7/modules_disp.py  \n"],"name":"stdout"}]},{"metadata":{"id":"xFxhrJ5XDlvb","colab_type":"text"},"cell_type":"markdown","source":["# 2) Implementing Neural Networks\n","\n","This homework considers neural networks with multiple layers. Each layer has multiple inputs and outputs, and can be broken down into two parts:\n","\n","<br>\n","A linear module that implements a linear transformation:     $ z_j = (\\sum^{m}_{i=1} x_i W_{i,j}) + {W_0}_jz$\n","\n","specified by a weight matrix $W$ and a bias vector $W_0$. The output is $[z_1, \\ldots, z_n]^T$\n","\n","<br>\n","An activation module that applies an activation function to the outputs of the linear module for some activation function $f$, such as Tanh or ReLU in the hidden layers or Softmax (see below) at the output layer. We write the output as: $[f(z_1), \\ldots, f(z_m)]^T$, although technically, for some activation functions such as softmax, each output will depend on all the $z_i$."]},{"metadata":{"id":"MjQgtwPHj08n","colab_type":"text"},"cell_type":"markdown","source":["We'll use the modular implementation that we guided you through in the previous problem, which leads to clean code. The basic framework for SGD training is given below. We can construct a network and train it as follows:\n","\n","```\n","# build a 3-layer network\n","net = Sequential([Linear(2,3), Tanh(),\n","                  Linear(3,3), Tanh(),\n","    \t          Linear(3,2), SoftMax()])\n","# train the network on data and labels\n","net.sgd(X, Y)\n","```\n","Please fill in any unimplemented methods below:"]},{"metadata":{"id":"cEwpgsbnho9K","colab_type":"text"},"cell_type":"markdown","source":["## Linear Modules: ##\n","Each linear module has a forward method that takes in a batch of activations A (from the previous layer) and returns a batch of pre-activations Z.\n","\n","Each linear module has a backward method that takes in dLdZ and returns dLdA. This module also computes and stores dLdW and dLdW0, the gradients with respect to the weights."]},{"metadata":{"id":"Xh60OQHmLM-v","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"5c3d359a-b6ee-4c3a-afcf-e4525b9f66f6","executionInfo":{"status":"ok","timestamp":1553443750595,"user_tz":240,"elapsed":7053,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["A = np.ones((2,3))\n","W = np.array([[3, 3, 3], \n","             [-1, -1, 0]])\n","inn = np.exp(W)/np.sum(np.exp(W), axis=0)\n","np.argmax(inn, axis=0)"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0, 0, 0])"]},"metadata":{"tags":[]},"execution_count":2}]},{"metadata":{"id":"AiMkeDCgYJBP","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"5b5fd28f-49f1-4493-b474-e54a8f9a16b8","executionInfo":{"status":"ok","timestamp":1553443750602,"user_tz":240,"elapsed":7040,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["np.sum(W, axis=1, keepdims=True)"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[ 9],\n","       [-2]])"]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"LeKPhRo5Xlvk","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"9c5d2fe8-d87f-4b1d-8ba1-2e278f3e623d","executionInfo":{"status":"ok","timestamp":1553443751026,"user_tz":240,"elapsed":7440,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["A\n","denom = np.sum(np.exp(A), axis=0)\n","print(np.exp(A)/denom)\n"],"execution_count":4,"outputs":[{"output_type":"stream","text":["[[0.5 0.5 0.5]\n"," [0.5 0.5 0.5]]\n"],"name":"stdout"}]},{"metadata":{"id":"HgOAMYbxYEoG","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"hudp5iHhLgUo","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"0cadc943-2bf1-4802-c307-1f4d06f4244c","executionInfo":{"status":"ok","timestamp":1553443751034,"user_tz":240,"elapsed":7407,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["r = np.dot(W.T, A)\n","print(W.shape, A.shape, r.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","text":["(2, 3) (2, 3) (3, 3)\n"],"name":"stdout"}]},{"metadata":{"id":"QdW-3fNXLR0t","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"624e0b15-7f77-4939-e5dc-eb81f055a1dd","executionInfo":{"status":"ok","timestamp":1553443751037,"user_tz":240,"elapsed":7384,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["r"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2., 2., 2.],\n","       [2., 2., 2.],\n","       [3., 3., 3.]])"]},"metadata":{"tags":[]},"execution_count":6}]},{"metadata":{"id":"-VsYLAxCfy7U","colab_type":"code","colab":{}},"cell_type":"code","source":["class Linear(Module):\n","    def __init__(self, m, n):\n","        self.m, self.n = (m, n)  # (in size, out size)\n","        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n","        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n","\n","    def forward(self, A):\n","        self.A = A   # (m x b)  Hint: make sure you understand what b stands for - b is the number of data points in a batch\n","\n","        Z = np.dot(self.W.T, self.A) + self.W0 # this broadcasts shape to the last dimension\n","        return Z  # Your code (n x b)\n","\n","    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n","        self.dLdW = np.dot(self.A, np.transpose(dLdZ))      # Your code A is mxb and dLdA is nxb want dLdW to be mxn\n","        # self.dLdW0 = np.dot(dLdZ, np.ones((dLdZ.shape[1], 1)))   # Your code dLdZ is nxb, want it to be nx1 \n","        self.dLdW0 = np.sum(dLdZ, axis=1, keepdims=True)\n","        # need to sum up gradients over b items in batch\n","        dLdA = np.dot(self.W, dLdZ)   # self.W is m x n, dLdZ is nxb\n","        return dLdA            # Your code: return dLdA (m x b)\n","\n","    def sgd_step(self, lrate):  # Gradient descent step\n","        self.W = self.W - lrate*self.dLdW           # Your code\n","        self.W0 = self.W0 - lrate*self.dLdW0         # Your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"cqZ7_kZYr5s5","colab_type":"text"},"cell_type":"markdown","source":[" You are encouraged to make your own tests for each module. A unit test method and an example test case are given below for your reference:"]},{"metadata":{"id":"aY3yePY0r4eA","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":86},"outputId":"50e456fa-662d-4e50-ff94-041080eef5f7","executionInfo":{"status":"ok","timestamp":1553443751044,"user_tz":240,"elapsed":7360,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["np.random.seed(0)\n","\n","# data\n","X, Y = super_simple_separable()\n","\n","# module\n","linear_1 = Linear(2, 3)\n","\n","#hyperparameters\n","lrate = 0.005\n","\n","# test case\n","# forward\n","z_1 = linear_1.forward(X)\n","exp_z_1 =  np.array([[10.41750064, 6.91122168, 20.73366505, 22.8912344],\n","                     [7.16872235, 3.48998746, 10.46996239, 9.9982611],\n","                     [-2.07105455, 0.69413716, 2.08241149, 4.84966811]])\n","unit_test(\"linear_forward\", exp_z_1, z_1)\n","\n","# backward\n","dL_dz1 = np.array([[1.69467553e-09, -1.33530535e-06, 0.00000000e+00, -0.00000000e+00],\n","                                     [-5.24547376e-07, 5.82459519e-04, -3.84805202e-10, 1.47943038e-09],\n","                                     [-3.47063705e-02, 2.55611604e-01, -1.83538094e-02, 1.11838432e-04]])\n","exp_dLdX = np.array([[-2.40194628e-02, 1.77064845e-01, -1.27021626e-02, 7.74006953e-05],\n","                                    [2.39827939e-02, -1.75870737e-01, 1.26832126e-02, -7.72828555e-05]])\n","dLdX = linear_1.backward(dL_dz1)\n","unit_test(\"linear_backward\", exp_dLdX, dLdX)\n","\n","# sgd step\n","linear_1.sgd_step(lrate)\n","exp_linear_1_W = np.array([[1.2473734,  0.28294514,  0.68940437],\n","                           [1.58455079, 1.32055711, -0.69218045]]),\n","unit_test(\"linear_sgd_step_W\",  exp_linear_1_W,  linear_1.W)\n","\n","exp_linear_1_W0 = np.array([[6.66805339e-09],\n","                            [-2.90968033e-06],\n","                            [-1.01331631e-03]]),\n","unit_test(\"linear_sgd_step_W0\", exp_linear_1_W0, linear_1.W0)\n"],"execution_count":8,"outputs":[{"output_type":"stream","text":["linear_forward: OK\n","linear_backward: OK\n","linear_sgd_step_W: OK\n","linear_sgd_step_W0: OK\n"],"name":"stdout"}]},{"metadata":{"id":"2ETL01mPsBz4","colab_type":"text"},"cell_type":"markdown","source":["The following datasets are defined for your use:\n","*  `super_simple_separable_through_origin()`\n","*  `super_simple_separable()`\n","*  `xor()`\n","*  `xor_more()`\n","*  `hard()`\n","\n","Further, a plotting function is defined for your usage in modules_disp.py, and can be called in the colab notebook as `disp.plot_nn()`.\n","```\n","def plot_nn(X, Y, nn):\n","    \"\"\" Plot output of nn vs. data \"\"\"\n","    def predict(x):\n","        return nn.modules[-1].class_fun(nn.forward(x))[0]\n","    xmin, ymin = np.min(X, axis=1)-1\n","    xmax, ymax = np.max(X, axis=1)+1\n","    nax = plot_objective_2d(lambda x: predict(x), xmin, xmax, ymin, ymax)\n","    plot_data(X, Y, nax)\n","    plt.show()```\n"]},{"metadata":{"id":"4s70beWJh09h","colab_type":"text"},"cell_type":"markdown","source":["## Activation functions: ##\n","Each activation module has a forward method that takes in a batch of pre-activations Z and returns a batch of activations A.\n","\n","Each activation module has a backward method that takes in dLdA and returns dLdZ, with the exception of SoftMax, where we assume dLdZ is passed in."]},{"metadata":{"id":"kwaNAtLnhenT","colab_type":"text"},"cell_type":"markdown","source":["### Tanh: ###"]},{"metadata":{"id":"ff6eD3dnftiR","colab_type":"code","colab":{}},"cell_type":"code","source":["class Tanh(Module):            # Layer activation\n","    def forward(self, Z):  # Z is nxb\n","        self.A = np.tanh(Z)  # self.A is nxb\n","        return self.A\n","\n","    def backward(self, dLdA):    # Uses stored self.A # dLdA is n x b self.A is nxb \n","        # deriv of tanh is 1-a^2\n","        tanh_deriv = np.ones_like(self.A)-np.square(self.A)\n","        dLdZ = dLdA*tanh_deriv # this is elementwise multiplication\n","        return dLdZ             # Your code: return dLdZ (n, b)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2FW7ocKRhcgY","colab_type":"text"},"cell_type":"markdown","source":["### ReLU: ###"]},{"metadata":{"id":"1fm2KsLUfqdp","colab_type":"code","colab":{}},"cell_type":"code","source":["class ReLU(Module):              # Layer activation\n","    def forward(self, Z):\n","        self.A = np.maximum(0, Z)            # Your code: (n, b)\n","        return self.A  # code shape is also (nxb)\n","\n","    def backward(self, dLdA):    # uses stored self.A nxb\n","        # return dLdA*(self.A!=0)\n","        dLdZ = np.where(self.A<=0, 0, 1)*dLdA # dLdA should also be nxb where n is outputs per layer\n","        return dLdZ              # Your code: return dLdZ (n, b)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZKtXuTQ0hSNO","colab_type":"text"},"cell_type":"markdown","source":["###SoftMax: ###\n","For `SoftMax.class_fun()`, given the column vector of class probabilities for each point (computed by Softmax), return a vector of the classes (integers) with the highest probability for each point."]},{"metadata":{"id":"fqK-CJrnfn22","colab_type":"code","colab":{}},"cell_type":"code","source":["class SoftMax(Module):           # Output activation\n","    def forward(self, Z):  # Z is nxb\n","      # code up softmax on vector z\n","      r = np.exp(Z)/np.sum(np.exp(Z), axis=0)       # Your code: (n, b) - same shape as input\n","      # print('SM shape', r.shape)\n","      return r\n","\n","    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n","        return dLdZ\n","\n","    def class_fun(self, Ypred):  # Return class indices of highest prediction\n","        return np.argmax(Ypred, axis=0) # find argmax over rows              # Your code: (1, b)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CZc7HnMSh4fn","colab_type":"text"},"cell_type":"markdown","source":["## Loss Functions:##\n","Each loss module has a forward method that takes in a batch of predictions Ypred (from the previous layer) and labels Y and returns a scalar loss value.\n","\n","The NLL module has a backward method that returns dLdZ, the gradient with respect to the preactivation to SoftMax (note: not the activation!), since we are always pairing SoftMax activation with NLL loss"]},{"metadata":{"id":"l4uy0pHVhNd8","colab_type":"text"},"cell_type":"markdown","source":["### NLL: ###"]},{"metadata":{"id":"17Fb8mimflgb","colab_type":"code","colab":{}},"cell_type":"code","source":["class NLL(Module):       # Loss\n","    def forward(self, Ypred, Y): # Y is nxb, so is #ypred\n","        self.Ypred = Ypred\n","        self.Y = Y\n","        # this is the sum of Y and actual\n","        # ln_sum = np.diagonal(np.dot(self.Y.T, np.log(self.Ypred)))\n","        return -1.0*np.sum(self.Y*np.log(self.Ypred)) # Your code - should return a scalar\n","\n","    def backward(self):  # Use stored self.Ypred, self.Y\n","        # should return dLdZ - for NNL with SM this is simply guess - actual\n","        return self.Ypred - self.Y   # Your code"],"execution_count":0,"outputs":[]},{"metadata":{"id":"rvONMaSrZgPS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":69},"outputId":"d52ed8e4-e24e-4016-ea15-2c1cc9e54ef5","executionInfo":{"status":"ok","timestamp":1553444568711,"user_tz":240,"elapsed":468,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["np.sum(np.dot(np.array([[3], [2], [1]]), np.array([[-1, 0, 1]])))\n","print(W)\n","np.diagonal(W)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["[[ 3  3  3]\n"," [-1 -1  0]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([ 3, -1])"]},"metadata":{"tags":[]},"execution_count":32}]},{"metadata":{"id":"y1EffzDFkqMX","colab_type":"text"},"cell_type":"markdown","source":["## Activation and Loss Test Cases: ##\n","Run Test 1 and Test 2 below and compare your outputs with the expected outputs.\n"]},{"metadata":{"id":"9DJFzpahkvcD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":330},"outputId":"cf2c292d-e3c0-4f39-c841-00f0a1ec57a6","executionInfo":{"status":"ok","timestamp":1553444570479,"user_tz":240,"elapsed":474,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["# TEST 1: sgd_test for Tanh activation and SoftMax output\n","np.random.seed(0)\n","sgd_test(Sequential([Linear(2,3), Tanh(), Linear(3,2), SoftMax()], NLL()), test_1_values)"],"execution_count":33,"outputs":[{"output_type":"stream","text":["linear_1.W: OK\n","linear_1.W0: OK\n","linear_2.W: OK\n","linear_2.W0: OK\n","z_1: OK\n","a_1: OK\n","z_2: OK\n","a_2: OK\n","loss: OK\n","dloss: OK\n","dL_dz2: OK\n","dL_da1: OK\n","dL_dz1: OK\n","dL_dX: OK\n","updated_linear_1.W: OK\n","updated_linear_1.W0: OK\n","updated_linear_2.W: OK\n","updated_linear_2.W0: OK\n"],"name":"stdout"}]},{"metadata":{"id":"Bd0dXg-Qk05_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":330},"outputId":"9e800231-056a-430a-dc02-5555c4bf26e6","executionInfo":{"status":"ok","timestamp":1553444571167,"user_tz":240,"elapsed":489,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["# TEST 2: sgd_test for ReLU activation and SoftMax output\n","np.random.seed(0)\n","sgd_test(Sequential([Linear(2,3), ReLU(), Linear(3,2), SoftMax()], NLL()), test_2_values)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["linear_1.W: OK\n","linear_1.W0: OK\n","linear_2.W: OK\n","linear_2.W0: OK\n","z_1: OK\n","a_1: OK\n","z_2: OK\n","a_2: OK\n","loss: OK\n","dloss: OK\n","dL_dz2: OK\n","dL_da1: OK\n","dL_dz1: OK\n","dL_dX: OK\n","updated_linear_1.W: OK\n","updated_linear_1.W0: OK\n","updated_linear_2.W: OK\n","updated_linear_2.W0: OK\n"],"name":"stdout"}]},{"metadata":{"id":"-l5JgBU2iBCZ","colab_type":"text"},"cell_type":"markdown","source":["## Neural Network: ##"]},{"metadata":{"id":"eXMGcdnXgiF3","colab_type":"text"},"cell_type":"markdown","source":["Implement SGD. Randomly pick a data point Xt, Yt by using np.random.randint to choose a random index into the data. Compute the predicted output Ypred for Xt with the forward method. Compute the loss for Ypred relative to Yt. Use the backward method to compute the gradients. Use the sgd_step method to change the weights. Repeat.\n","\n","We will (later) be generalizing SGD to operate on a \"mini-batch\" of data points instead of a single point. You should strive for an implementation of the forward, backward, and `class_fun` methods that works with batches of data. Note that when $b$ is mentioned as part of the shape of a matrix in the code, this $b$ refers to the number of points."]},{"metadata":{"id":"ejO15Vr7fhKB","colab_type":"code","colab":{}},"cell_type":"code","source":[" class Sequential:\n","    def __init__(self, modules, loss):            # List of modules, loss module\n","        self.modules = modules\n","        self.loss = loss\n","\n","    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n","        D, N = X.shape\n","        for it in range(iters):\n","            rand_int = np.random.randint(N)\n","            Y_pred = self.forward(X[:, rand_int:rand_int+1]) # compute forward predictions\n","            self.loss.forward(Y_pred, Y[:, rand_int:rand_int+1])\n","            dLoss = self.loss.backward() # compute loss dl dZ\n","            self.backward(dLoss) # update gradients on Y for all of the things\n","            self.sgd_step(lrate) # take a sgd step using the updated gradients\n","\n","    def forward(self, Xt):                        # Compute Ypred\n","        for m in self.modules: Xt = m.forward(Xt)\n","        return Xt\n","\n","    def backward(self, delta):                    # Update dLdW and dLdW0\n","        # Note reversed list of modules\n","        for m in self.modules[::-1]: delta = m.backward(delta)\n","\n","    def sgd_step(self, lrate):                    # Gradient descent step\n","        for m in self.modules: m.sgd_step(lrate)\n","\n","    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n","        # Utility method to print accuracy on full dataset, should\n","        # improve over time when doing SGD. Also prints current loss,\n","        # which should decrease over time. Call this on each iteration\n","        # of SGD!\n","        if it % every == 1:\n","            cf = self.modules[-1].class_fun\n","            acc = np.mean(cf(self.forward(X)) == cf(Y))\n","            print('Iteration =', it, '\\tAcc =', acc, '\\tLoss =', cur_loss, flush=True)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"HUojaXqphDjh","colab_type":"text"},"cell_type":"markdown","source":["## Neural Network / SGD Test Cases: ##\n","Use Test 3 and Test 4 to help you debug."]},{"metadata":{"colab_type":"code","id":"wmupM8OScodw","colab":{"base_uri":"https://localhost:8080/","height":379},"outputId":"9d9f95fb-d229-43e8-dafd-d50cf34ccc18","executionInfo":{"status":"ok","timestamp":1553444586993,"user_tz":240,"elapsed":9904,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["# TEST 3: you should achieve 100% accuracy on the hard dataset (note that we provided plotting code)\n","X, Y = hard()\n","nn = Sequential([Linear(2, 10), ReLU(), Linear(10, 10), ReLU(), Linear(10,2), SoftMax()], NLL())\n","disp.classify(X, Y, nn, it=100000)"],"execution_count":36,"outputs":[{"output_type":"stream","text":["-3.46493986 -3.41956036 3.39710997 2.0597278\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAcAAAAFHCAYAAAA2kJwTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJUAREXElJUAREXElJUAREXGl/w+jMbXGWB4HrQAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 576x396 with 2 Axes>"]},"metadata":{"tags":[]}},{"output_type":"execute_result","data":{"text/plain":["<__main__.Sequential at 0x7f2971796710>"]},"metadata":{"tags":[]},"execution_count":36}]},{"metadata":{"id":"MaWfgC7Qe3ar","colab_type":"code","colab":{}},"cell_type":"code","source":["# TEST 4: try calling these methods that train with a simple dataset\n","def nn_tanh_test():\n","    np.random.seed(0)\n","    nn = Sequential([Linear(2, 3), Tanh(), Linear(3, 2), SoftMax()], NLL())\n","    X, Y = super_simple_separable()\n","    nn.sgd(X, Y, iters=1, lrate=0.005)\n","    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n","            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n","\n","\n","def nn_relu_test():\n","    np.random.seed(0)\n","    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n","    X, Y = super_simple_separable()\n","    nn.sgd(X, Y, iters=2, lrate=0.005)\n","    return [np.vstack([nn.modules[0].W, nn.modules[0].W0.T]).tolist(),\n","            np.vstack([nn.modules[2].W, nn.modules[2].W0.T]).tolist()]\n","\n","\n","def nn_pred_test():\n","    np.random.seed(0)\n","    nn = Sequential([Linear(2, 3), ReLU(), Linear(3, 2), SoftMax()], NLL())\n","    X, Y = super_simple_separable()\n","    nn.sgd(X, Y, iters=1, lrate=0.005)\n","    Ypred = nn.forward(X)\n","    print(Ypred, Y)\n","    return Ypred, Y\n","    return nn.modules[-1].class_fun(Ypred).tolist(), [nn.loss.forward(Ypred, Y)]\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"_dx-zM2y3R0z","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"87e2e60e-dc4d-47da-e26d-ce7c884e6c79","executionInfo":{"status":"ok","timestamp":1553444592532,"user_tz":240,"elapsed":538,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["\n","\n","# Expected output:\n","'''\n","[[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n","  [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n","  [-8.47337764291184e-12, 2.6227368810847106e-09, 0.00017353185263155828]],\n"," [[0.544808855557535, -0.08366117689965663],\n","  [-0.06331837550937104, 0.24078409926389266],\n","  [0.08677202043839037, 0.8360167748667923],\n","  [-0.0037249480614718, 0.0037249480614718]]]\n","'''\n","\n","nn_tanh_test()"],"execution_count":38,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[[1.2473733761848262, 0.2829538808226157, 0.6924193292712828],\n","  [1.5845507770278007, 1.320562932207846, -0.6901721567010647],\n","  [-8.47337764291184e-12, 2.6227368810847106e-09, 0.00017353185263155828]],\n"," [[0.544808855557535, -0.08366117689965663],\n","  [-0.06331837550937104, 0.24078409926389266],\n","  [0.08677202043839037, 0.8360167748667923],\n","  [-0.0037249480614718, 0.0037249480614718]]]"]},"metadata":{"tags":[]},"execution_count":38}]},{"metadata":{"id":"WmYT9IWk3TQL","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"f51c3537-f19a-44b7-bf84-29b4766a41ef","executionInfo":{"status":"ok","timestamp":1553444595967,"user_tz":240,"elapsed":724,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["\n","\n","# Expected output:\n","'''\n","[[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n","  [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n","  [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\n"," [[0.501769700845158, -0.040622022187279644],\n","  [-0.09260786974986723, 0.27007359350438886],\n","  [0.08364438851530624, 0.8391444067898763],\n","  [-0.004252310922204504, 0.004252310922204505]]]\n","'''\n","nn_relu_test()"],"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[[1.2421914999646917, 0.2851239946607419, 0.6905003767490479],\n","  [1.5695659964519526, 1.3273884281993562, -0.6920877418422037],\n","  [-0.0027754917572235106, 0.001212351486908601, -0.0005239629389906042]],\n"," [[0.501769700845158, -0.040622022187279644],\n","  [-0.09260786974986723, 0.27007359350438886],\n","  [0.08364438851530624, 0.8391444067898763],\n","  [-0.004252310922204504, 0.004252310922204505]]]"]},"metadata":{"tags":[]},"execution_count":39}]},{"metadata":{"id":"uo_woDFh3a2v","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":139},"outputId":"d62ec621-1f30-4a4b-afa2-4af21ac89c86","executionInfo":{"status":"ok","timestamp":1553444599889,"user_tz":240,"elapsed":544,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["# Expected output:\n","'''\n","([0, 0, 0, 0], [8.56575061835767])\n","'''\n","nn_pred_test()\n","#np.sum(-1.0*b*np.log(a))\n","\n"],"execution_count":40,"outputs":[{"output_type":"stream","text":["[[0.94480955 0.86147005 0.99595762 0.99128933]\n"," [0.05519045 0.13852995 0.00404238 0.00871067]] [[0 1 0 1]\n"," [1 0 1 0]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["(array([[0.94480955, 0.86147005, 0.99595762, 0.99128933],\n","        [0.05519045, 0.13852995, 0.00404238, 0.00871067]]),\n"," array([[0, 1, 0, 1],\n","        [1, 0, 1, 0]]))"]},"metadata":{"tags":[]},"execution_count":40}]},{"metadata":{"id":"uin2ZKnjfckU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"ec577f5f-018d-40b3-cc30-dcfbbd663afc","executionInfo":{"status":"ok","timestamp":1553444257712,"user_tz":240,"elapsed":570,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["a = list(range(10))\n","print(a, a[::-1])\n"],"execution_count":23,"outputs":[{"output_type":"stream","text":["[0, 1, 2, 3, 4, 5, 6, 7, 8, 9] [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]\n"],"name":"stdout"}]},{"metadata":{"id":"dV2L-hemfh2I","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"35a5e21e-74b2-4ee8-92b1-8ff37ae73c0a","executionInfo":{"status":"ok","timestamp":1553444302534,"user_tz":240,"elapsed":442,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["a[::-2]"],"execution_count":26,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[9, 7, 5, 3, 1]"]},"metadata":{"tags":[]},"execution_count":26}]},{"metadata":{"id":"PZamNuuyfpsy","colab_type":"code","colab":{}},"cell_type":"code","source":["# SOLUTION CODE\n","class Module:\n","    def sgd_step(self, lrate): pass  # For modules w/o weights\n","\n","\n","class Linear(Module):\n","    def __init__(self, m, n):\n","        self.m, self.n = (m, n)  # (in size, out size)\n","        self.W0 = np.zeros([self.n, 1])  # (n x 1)\n","        self.W = np.random.normal(0, 1.0 * m ** (-.5), [m, n])  # (m x n)\n","\n","    def forward(self, A):\n","        self.A = A\n","        return np.dot(self.W.T, A) + self.W0  # (m x n)^T (m x b) = (n x b)\n","\n","    def backward(self, dLdZ):  # dLdZ is (n x b), uses stored self.A\n","        self.dLdW = np.dot(self.A, dLdZ.T)                  # (m x n)\n","        self.dLdW0 = dLdZ.sum(axis=1).reshape((self.n, 1))  # (n x 1)\n","        return np.dot(self.W, dLdZ)                         # (m x b)\n","\n","    def sgd_step(self, lrate):  # Gradient descent step\n","        self.W -= lrate*self.dLdW\n","        self.W0 -= lrate*self.dLdW0\n","\n","\n","class Tanh(Module):            # Layer activation\n","    def forward(self, Z):\n","        self.A = np.tanh(Z)\n","        return self.A\n","\n","    def backward(self, dLdA):    # Uses stored self.A\n","        return dLdA * (1.0 - (self.A ** 2))\n","\n","\n","class ReLU(Module):              # Layer activation\n","    def forward(self, Z):\n","        self.A = np.maximum(0, Z)\n","        return self.A\n","\n","    def backward(self, dLdA):    # uses stored self.A\n","        return dLdA * (self.A != 0)\n","\n","\n","class SoftMax(Module):           # Output activation\n","    def forward(self, Z):\n","        return np.exp(Z) / np.sum(np.exp(Z), axis=0)\n","\n","    def backward(self, dLdZ):    # Assume that dLdZ is passed in\n","        return dLdZ\n","\n","    def class_fun(self, Ypred):  # Return class indices\n","        return np.argmax(Ypred, axis=0)\n","\n","\n","class NLL(Module):       # Loss\n","    def forward(self, Ypred, Y):\n","        self.Ypred = Ypred\n","        self.Y = Y\n","        return float(np.sum(-Y * np.log(Ypred)))\n","\n","    def backward(self):  # Use stored self.Ypred, self.Y\n","        return self.Ypred - self.Y\n","\n","\n","class Sequential:\n","    def __init__(self, modules, loss):            # List of modules, loss module\n","        self.modules = modules\n","        self.loss = loss\n","\n","    def sgd(self, X, Y, iters=100, lrate=0.005):  # Train\n","        D, N = X.shape\n","        sum_loss = 0\n","        for it in range(iters):\n","            i = np.random.randint(N)\n","            Xt = X[:, i:i+1]\n","            Yt = Y[:, i:i+1]\n","            Ypred = self.forward(Xt)\n","            sum_loss += self.loss.forward(Ypred, Yt)\n","            err = self.loss.backward()\n","            self.backward(err)\n","            self.sgd_step(lrate)\n","\n","    def forward(self, Xt):                        # Compute Ypred\n","        for m in self.modules: Xt = m.forward(Xt)\n","        return Xt\n","\n","    def backward(self, delta):                    # Update dLdW and dLdW0\n","        # Note reversed list of modules\n","        for m in self.modules[::-1]: delta = m.backward(delta)\n","\n","    def sgd_step(self, lrate):                    # Gradient descent step\n","        for m in self.modules: m.sgd_step(lrate)\n","\n","    def print_accuracy(self, it, X, Y, cur_loss, every=250):\n","        # Utility method to print accuracy on full dataset, should\n","        # improve over time when doing SGD. Also prints current loss,\n","        # which should decrease over time. Call this on each iteration\n","        # of SGD!\n","        if it % every == 1:\n","            cf = self.modules[-1].class_fun\n","            acc = np.mean(cf(self.forward(X)) == cf(Y))\n","            print('Iteration =', it, '\tAcc =', acc, '\tLoss =', cur_loss)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TffPCJXiiXoo","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}