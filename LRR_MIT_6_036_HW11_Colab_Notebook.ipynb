{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LRR MIT 6.036 HW11 Colab Notebook",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lrraymond13/collab/blob/master/LRR_MIT_6_036_HW11_Colab_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "q58cS9antfCw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#MIT 6.036 Spring 2019: Homework 11#\n",
        "\n",
        "This colab notebook provides code and a framework for question 6 from [homework 11](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week11/week11_homework/).  You can work out your solutions here, then submit your results back on the homework page when ready.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, download the code distribution for this homework that contains test cases and helper functions.\n",
        "\n",
        "Run the next code block to download and import the code for this lab."
      ]
    },
    {
      "metadata": {
        "id": "OUEtSZRdtmI2",
        "colab_type": "code",
        "outputId": "0fd40bb4-8775-4813-f392-b9bb2661269c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "cell_type": "code",
      "source": [
        "!rm -rf code_for_hw11* __MACOSX data .DS_Store\n",
        "!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw11/code_for_hw11.zip\n",
        "!unzip code_for_hw11.zip\n",
        "!mv code_for_hw11/* .\n",
        "\n",
        "import code_for_hw11 as code_for_hw11\n",
        "from rnn_hw11 import *\n",
        "from util import *\n",
        "#from sm import *\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import math as m\n",
        "import random\n",
        "\n",
        "import pdb\n",
        "from util import argmax_with_val, argmax\n",
        "from keras.models import Sequential\n",
        "from keras.layers.core import Dense\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "import importlib"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  code_for_hw11.zip\n",
            "   creating: code_for_hw11/\n",
            "  inflating: code_for_hw11/rnn_hw11.py  \n",
            "  inflating: code_for_hw11/util.py   \n",
            "  inflating: code_for_hw11/sm.py     \n",
            "  inflating: code_for_hw11/.rnn_hw11.py.swp  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "8jnw4bt5QKLJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "P6) **BPTT in code**:\n",
        "\n",
        "Complete the following implementation of BPTT. This is a method in an RNN class.\n",
        "\n",
        "The instance (self) has attributes that define the RNN, in particular:\n",
        "\n",
        "The weight matrices and offsets: self.Wss, self.Wsx, self.Wo, self.Wss0 and self.Wo0.\n",
        "The activation functions and their derivatives: self.f1, self.df1, self.f2, self.df2\n",
        "The loss function and the derivative of the combined loss and final activation (as we did for feedforward networks): self.loss_fn and self.dloss_f2.\n",
        "The dimensions of states (hidden), inputs and outputs: self.hidden_dim, self.input_dim, self.output_dim.\n",
        "The initial state and, the current state: self.init_state and self.hidden_state\n",
        "The step size for gradient descent: self.step_size\n"
      ]
    },
    {
      "metadata": {
        "id": "aIn_1VNw8iRa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Back propgation through time\n",
        "# xs is matrix of inputs: l by k\n",
        "# dLdz2 is matrix of output errors:  1 by k\n",
        "# states is matrix of state values: m by k"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DfPqvLUsQXtQ",
        "colab_type": "code",
        "outputId": "18ad3af9-912c-4a27-b622-8295f36ec303",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "cell_type": "code",
      "source": [
        "def bptt(self, xs, dLtdz2, states):\n",
        "    dWsdef bptt(self, xs, dLtdz2, states):\n",
        "    dWsx = np.zeros_like(self.Wsx)\n",
        "    dWss = np.zeros_like(self.Wss)\n",
        "    dWo = np.zeros_like(self.Wo)\n",
        "    dWss0 = np.zeros_like(self.Wss0)\n",
        "    dWo0 = np.zeros_like(self.Wo0)\n",
        "    # Derivative of future loss (from t+1 forward) wrt state at time t\n",
        "    # initially 0;  will pass \"back\" through iterations\n",
        "    dFtdst = np.zeros((self.hidden_dim, 1))\n",
        "    k = xs.shape[1]\n",
        "    # Technically we are considering time steps 1..k, but we need\n",
        "    # to index into our xs and states with indices 0..k-1\n",
        "    for t in range(k-1, -1, -1):\n",
        "        # Get relevant quantities\n",
        "        xt = xs[:, t:t+1]\n",
        "        st = states[:, t:t+1]\n",
        "        stm1 = states[:, t-1:t] if t-1 >= 0 else self.init_state\n",
        "        dLtdz2t = dLtdz2[:, t:t+1]\n",
        "        # Compute gradients step by step\n",
        "        # ==> Use self.df1(st) to get dfdz1;\n",
        "        # ==> Use self.Wo, self.Wss, etc. for weight matrices\n",
        "        # derivative of loss at time t wrt state at time t\n",
        "        dLtdst = np.dot(self.Wo.T, dLtdz2t)        # Your code\n",
        "        # derivatives of loss from t forward\n",
        "        dFtm1dst = dLtdst + dFtdst             # Your code\n",
        "        dFtm1dz1t = self.df1(st)*dFtm1dst           # Your code\n",
        "        dFtm1dstm1 = np.dot(self.Wss.T, dFtm1dz1t)          # Your code\n",
        "        # gradienst wrt weights\n",
        "        dLtdWo = np.dot(dLtdz2t, st.T)              # Your code\n",
        "        dLtdWo0 = dLtdz2t             # Your code\n",
        "        dFtm1dWss = np.dot(dFtm1dz1t, stm1.T)           # Your code\n",
        "        dFtm1dWss0 = dFtm1dz1t          # Your code\n",
        "        dFtm1dWsx = np.dot(dFtm1dz1t, xt.T)            # Your code\n",
        "        # Accumulate updates to weights\n",
        "        dWsx += dFtm1dWsx\n",
        "        dWss += dFtm1dWss\n",
        "        dWss0 += dFtm1dWss0\n",
        "        dWo += dLtdWo\n",
        "        dWo0 += dLtdWo0\n",
        "        # pass delta \"back\" to next iteration\n",
        "        dFtdst = dFtm1dstm1\n",
        "    return dWsx, dWss, dWo, dWss0, dWo0x = np.zeros_like(self.Wsx)\n",
        "    dWss = np.zeros_like(self.Wss)\n",
        "    dWo = np.zeros_like(self.Wo)\n",
        "    dWss0 = np.zeros_like(self.Wss0)\n",
        "    dWo0 = np.zeros_like(self.Wo0)\n",
        "    # Derivative of future loss (from t+1 forward) wrt state at time t\n",
        "    # initially 0;  will pass \"back\" through iterations\n",
        "    dFtdst = np.zeros((self.hidden_dim, 1))\n",
        "    k = xs.shape[1]\n",
        "    # Technically we are considering time steps 1..k, but we need\n",
        "    # to index into our xs and states with indices 0..k-1\n",
        "    for t in range(k-1, -1, -1):\n",
        "        # Get relevant quantities\n",
        "        xt = xs[:, t:t+1]\n",
        "        st = states[:, t:t+1]\n",
        "        stm1 = states[:, t-1:t] if t-1 >= 0 else self.init_state\n",
        "        dLtdz2t = dLtdz2[:, t:t+1]\n",
        "        # Compute gradients step by step\n",
        "        # ==> Use self.df1(st) to get dfdz1;\n",
        "        # ==> Use self.Wo, self.Wss, etc. for weight matrices\n",
        "        # derivative of loss at time t wrt state at time t\n",
        "        dLtdst = np.dot(self.Wo.T, dLtdz2t)        # Your code\n",
        "        # derivatives of loss from t forward\n",
        "        dFtm1dst = dLtdst + dFtdst             # Your code\n",
        "        dFtm1dz1t = self.df1(st)*dFtm1dst           # Your code\n",
        "        dFtm1dstm1 = np.dot(self.Wss.T, dFtm1dz1t)          # Your code\n",
        "        # gradienst wrt weights\n",
        "        dLtdWo = np.dot(dLtdz2t, st.T)              # Your code\n",
        "        dLtdWo0 = dLtdz2t             # Your code\n",
        "        dFtm1dWss = np.dot(dFtm1dz1t, stm1.T)           # Your code\n",
        "        dFtm1dWss0 = dFtm1dz1t          # Your code\n",
        "        dFtm1dWsx = np.dot(dFtm1dz1t, xt.T)            # Your code\n",
        "        # Accumulate updates to weights\n",
        "        dWsx += dFtm1dWsx\n",
        "        dWss += dFtm1dWss\n",
        "        dWss0 += dFtm1dWss0\n",
        "        dWo += dLtdWo\n",
        "        dWo0 += dLtdWo0\n",
        "        # pass delta \"back\" to next iteration\n",
        "        dFtdst = dFtm1dstm1\n",
        "    return dWsx, dWss, dWo, dWss0, dWo0\n",
        "  \n",
        "\n",
        "def delay_num_test(bptt, delay = 1, num_epochs = 10000, num_seqs = 10000, seq_length = 10, step_size = .005):\n",
        "#test function provided to help you debug your implementation\n",
        "\n",
        "# In case we want to initialize.  Now just for delay = 1\n",
        "#Wsx = np.array([[1.], [0.]])\n",
        "#Wss = np.array([[0., 0.],\n",
        "#              [1., 0.]])\n",
        "#Wo = np.array([[0., 1.]])\n",
        "#Wss0 = np.array([[0.], [0.0]])\n",
        "#Wo0 = np.array([[0.]])\n",
        "  np.random.seed(0)\n",
        "  data = []\n",
        "  for _ in range(num_seqs):\n",
        "    vals = np.random.random((1, seq_length))\n",
        "    x = np.hstack([vals, np.zeros([1, delay])])\n",
        "    y = np.hstack([np.zeros((1, delay)), vals])\n",
        "    data.append((x, y))\n",
        "  m = (delay + 1) * 2\n",
        "  RNN.bptt = bptt\n",
        "  rnn = RNN(1, m, 1, quadratic_loss, lambda z: z, quadratic_linear_gradient, step_size, lambda z: z, lambda z: 1)\n",
        "# Wsx = Wsx, Wo = Wo, Wss = Wss, Wo0 = Wo0, Wss0 = Wss0)\n",
        "  rnn.train_seq_to_seq(data, num_epochs)\n",
        "  assert np.all(np.isclose(rnn.Wsx, np.array([[0.00856855],\n",
        "        [0.01936238],\n",
        "        [0.01382334],\n",
        "        [0.00771265]])))\n",
        "  assert np.all(np.isclose(rnn.Wss, np.array([[0.01505222, 0.02059889, 0.01594291, 0.00558242],\n",
        "        [0.00824307, 0.01741733, 0.01864768, 0.01079751],\n",
        "        [0.01577334, 0.0124164 , 0.0171516 , 0.0071486 ],\n",
        "        [0.00722321, 0.00497032, 0.00514737, 0.00979516]])))\n",
        "  assert np.all(np.isclose(rnn.Wo, np.array([[0.02522786, 0.01744193, 0.01995478, 0.0084726 ]])))\n",
        "  assert np.all(np.isclose(rnn.Wss0, np.array([[0.01502946],\n",
        "        [0.01181406],\n",
        "        [0.02051297],\n",
        "        [0.00716202]])))\n",
        "  assert np.all(np.isclose(rnn.Wo0, np.array([[0.42198824]])))\n",
        "  print(\"Test passed!\")\n",
        "  return (rnn.Wsx, rnn.Wss, rnn.Wo, rnn.Wss0, rnn.Wo0)\n",
        "\n",
        "delay_num_test(bptt)\n",
        "  \n",
        "  \n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training error 0.04945571303681596\n",
            "training error 0.048114092792055106\n",
            "training error 0.04850511140924504\n",
            "training error 0.04834714371014377\n",
            "training error 0.04845429552158349\n",
            "training error 0.048195163749529306\n",
            "training error 0.0490306116336328\n",
            "training error 0.048476636551466105\n",
            "training error 0.048258029080159005\n",
            "Test passed!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.00856855],\n",
              "        [0.01936238],\n",
              "        [0.01382334],\n",
              "        [0.00771265]]),\n",
              " array([[0.01505222, 0.02059889, 0.01594291, 0.00558242],\n",
              "        [0.00824307, 0.01741733, 0.01864768, 0.01079751],\n",
              "        [0.01577334, 0.0124164 , 0.0171516 , 0.0071486 ],\n",
              "        [0.00722321, 0.00497032, 0.00514737, 0.00979516]]),\n",
              " array([[0.02522786, 0.01744193, 0.01995478, 0.0084726 ]]),\n",
              " array([[0.01502946],\n",
              "        [0.01181406],\n",
              "        [0.02051297],\n",
              "        [0.00716202]]),\n",
              " array([[0.42198824]]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "USoY8uc3-FIJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "r4JIv3lO-Fyz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is the solution answer!"
      ]
    },
    {
      "metadata": {
        "id": "daJq989--DBV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Back propgation through time\n",
        "# xs is matrix of inputs: l by k\n",
        "# dLdz2 is matrix of output errors:  1 by k\n",
        "# states is matrix of state values: m by k\n",
        "def bptt(self, xs, dLtdz2, states):\n",
        "    dWsx = np.zeros_like(self.Wsx)\n",
        "    dWss = np.zeros_like(self.Wss)\n",
        "    dWo = np.zeros_like(self.Wo)\n",
        "    dWss0 = np.zeros_like(self.Wss0)\n",
        "    dWo0 = np.zeros_like(self.Wo0)\n",
        "    # Derivative of all future loss (from t+1 forward) with respect to state\n",
        "    #  at time t\n",
        "    # initially 0;  will pass \"back\" through iterations\n",
        "    dFtdst = np.zeros((self.hidden_dim, 1))\n",
        "    k = xs.shape[1]\n",
        "    # Technically we are considering time steps 1..k, but we need\n",
        "    # to index into our xs and states with indices 0..k-1\n",
        "    for t in range(k-1, -1, -1):\n",
        "        # Get relevant quantities\n",
        "        xt = xs[:, t:t+1]\n",
        "        st = states[:, t:t+1]\n",
        "        stm1 = states[:, t-1:t] if t-1 >= 0 else self.init_state\n",
        "        dLtdz2t = dLtdz2[:, t:t+1]\n",
        "        # Compute gradients step by step\n",
        "        # derivative of loss at time t wrt state at time t\n",
        "        dLtdst = np.dot(self.Wo.T, dLtdz2t)\n",
        "        # derivatives of loss from t forward\n",
        "        dFtm1dst = dLtdst + dFtdst\n",
        "        dFtm1dz1t = dFtm1dst * self.df1(st)\n",
        "        dFtm1dstm1 = np.dot(self.Wss.T, dFtm1dz1t)\n",
        "        # gradienst wrt weights\n",
        "        dLtdWo = np.dot(dLtdz2t, st.T)\n",
        "        dLtdWo0 = dLtdz2t\n",
        "        dFtm1dWss = np.dot(dFtm1dz1t, stm1.T)\n",
        "        dFtm1dWss0 = dFtm1dz1t\n",
        "        dFtm1dWsx = np.dot(dFtm1dz1t, xt.T)\n",
        "        # Accumulate updates to weights\n",
        "        dWsx += dFtm1dWsx\n",
        "        dWss += dFtm1dWss\n",
        "        dWss0 += dFtm1dWss0\n",
        "        dWo += dLtdWo\n",
        "        dWo0 += dLtdWo0\n",
        "        # pass delta \"back\" to next iteration\n",
        "        dFtdst = dFtm1dstm1\n",
        "    return dWsx, dWss, dWo, dWss0, dWo0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MyKQPeWk5zx1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "More instructions..."
      ]
    }
  ]
}