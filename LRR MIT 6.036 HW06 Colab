{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LRR MIT 6.036 HW06 Colab","version":"0.3.2","provenance":[{"file_id":"18I8zLoLsfYt0J2Bqs7LTO3lgGtassrYH","timestamp":1551467279533}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"Wmv3jlgr4_Ji","colab_type":"text"},"cell_type":"markdown","source":["# MIT 6.036 Spring 2019: Homework 4\n","This homework does not include provided Python code. Instead, we\n","encourage you to write your own code to help you answer some of these\n","problems, and/or test and debug the code components we do ask for.\n","Some of the problems below are simple enough that hand calculation\n","should be possible; your hand solutions can serve as test cases for\n","your code.  You may also find that including utilities written in\n","previous labs (like a `sd` or signed distance function) will be\n","helpful, as you build up additional functions and utilities for\n","calculation of margins, different loss functions, gradients, and other\n","functions needed for margin maximization and gradient descent."]},{"metadata":{"id":"N622h8-D5i-M","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jUS51a8m5rEI","colab_type":"text"},"cell_type":"markdown","source":["## 6) Implementing gradient descent\n","In this section we will implement generic versions of gradient descent and apply these to the SVM objective.\n","\n","<b>Note: </b> If you need a refresher on gradient descent,\n","you may want to reference\n","<a href=\"https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week4/gradient_descent/2\">this week's notes</a>.\n","\n","### 6.1) Implementing Gradient Descent\n","We want to find the $x$ that minimizes the value of the *objective\n","function* $f(x)$, for an arbitrary scalar function $f$.  The function\n","$f$ will be implemented as a Python function of one argument, that\n","will be a numpy column vector.  For efficiency, we will work with\n","Python functions that return not just the value of $f$ at $f(x)$ but\n","also return the gradient vector at $x$, that is, $\\nabla_x f(x)$.\n","\n","We will now implement a generic gradient descent function, `gd`, that\n","has the following input arguments:\n","\n","* `f`: a function whose input is an `x`, a column vector, and\n","  returns a scalar.\n","* `df`: a function whose input is an `x`, a column vector, and\n","  returns a column vector representing the gradient of `f` at `x`.\n","* `x0`: an initial value of $x$, `x0`, which is a column vector.\n","* `step_size_fn`: a function that is given the iteration index (an\n","  integer) and returns a step size.\n","* `max_iter`: the number of iterations to perform\n","\n","Our function `gd` returns a tuple:\n","\n","* `x`: the value at the final step\n","* `fs`: the list of values of `f` found during all the iterations (including `f(x0)`)\n","* `xs`: the list of values of `x` found during all the iterations (including `x0`)\n","\n","**Hint:** This is a short function!\n","\n","**Hint 2:** If you do `temp_x = x` where `x` is a vector\n","(numpy array), then `temp_x` is just another name for the same vector\n","as `x` and changing an entry in one will change an entry in the other.\n","You should either use `x.copy()` or remember to change entries back after modification.\n","\n","Some utilities you may find useful are included below."]},{"metadata":{"id":"fYOF0HS34YOj","colab_type":"code","colab":{}},"cell_type":"code","source":["def rv(value_list):\n","    return np.array([value_list])\n","\n","def cv(value_list):\n","    return np.transpose(rv(value_list))\n","\n","def f1(x):\n","    return float((2 * x + 3)**2)\n","\n","def df1(x):\n","    return 2 * 2 * (2 * x + 3)\n","\n","def f2(v):\n","    x = float(v[0]); y = float(v[1])\n","    return (x - 2.) * (x - 3.) * (x + 3.) * (x + 1.) + (x + y -1)**2\n","\n","def df2(v):\n","    x = float(v[0]); y = float(v[1])\n","    return cv([(-3. + x) * (-2. + x) * (1. + x) + \\\n","               (-3. + x) * (-2. + x) * (3. + x) + \\\n","               (-3. + x) * (1. + x) * (3. + x) + \\\n","               (-2. + x) * (1. + x) * (3. + x) + \\\n","               2 * (-1. + x + y),\n","               2 * (-1. + x + y)])\n","\n","import math\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"s03NFuxG6kvt","colab_type":"text"},"cell_type":"markdown","source":["The main function to implement is `gd`, defined below."]},{"metadata":{"id":"mNsLE3bg6jt9","colab_type":"code","colab":{}},"cell_type":"code","source":["def gd(f, df, x0, step_size_fn, max_iter):\n","    xs = list()\n","    fs = list()\n","    f_val=0\n","    x = x0.copy()\n","    for t in range(max_iter):\n","      xs.append(x.copy())\n","      f_val = f(x)\n","      fs.append(f_val)\n","      # then do gradient update\n","      grad_f = df(x)\n","      x_new = x - step_size_fn(t)*grad_f\n","      if np.sqrt(np.sum(np.dot(grad_f.T, grad_f))) < .000001:\n","        return x, fs, xs\n","      x = x_new.copy()\n","    return x, fs, xs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jXu60n-H5_Hz","colab_type":"text"},"cell_type":"markdown","source":["To evaluate results, we also use a simple `package_ans` function,\n","which checks the final `x`, as well as the first and last values in\n","`fs`, `xs`."]},{"metadata":{"id":"GJcClaqN4nE6","colab_type":"code","colab":{}},"cell_type":"code","source":["def package_ans(gd_vals):\n","    x, fs, xs = gd_vals\n","    return [x.tolist(), [fs[0], fs[-1]], [xs[0].tolist(), xs[-1].tolist()]]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"aN_XbacQ6Rue","colab_type":"text"},"cell_type":"markdown","source":["The test cases are provided below, but you should feel free (and are encouraged!) to write more of your own."]},{"metadata":{"id":"jq0OJLEf6Dan","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":72},"outputId":"83bdb963-035d-4360-8507-723eebefc904","executionInfo":{"status":"ok","timestamp":1551467896183,"user_tz":300,"elapsed":298,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["# Test case 1\n","ans=package_ans(gd(f1, df1, cv([0.]), lambda i: 0.1, 1000))\n","print(ans)\n","# Test case 2\n","ans=package_ans(gd(f2, df2, cv([0., 0.]), lambda i: 0.01, 1000))\n","print(ans)"],"execution_count":10,"outputs":[{"output_type":"stream","text":["[[[-1.49999616]], [9.0, 5.898239999929943e-11], [[[0.0]], [[-1.49999616]]]]\n","[[[-2.2058218474520825], [3.2057727782539893]], [19.0, -20.967239608835946], [[[0.0], [0.0]], [[-2.2058218474520825], [3.2057727782539893]]]]\n"],"name":"stdout"}]},{"metadata":{"id":"pbuSt5hY645k","colab_type":"text"},"cell_type":"markdown","source":["### 6.2) Numerical Gradient\n","Getting the analytic gradient correct for complicated functions is\n","tricky.  A very handy method of verifying the analytic gradient or\n","even substituting for it is to estimate the gradient at a point by\n","means of *finite differences*.\n","\n","Assume that we are given a function $f(x)$ that takes a column vector\n","as its argument and returns a scalar value.  In gradient descent, we\n","will want to estimate the gradient of $f$ at a particular $x_0.$\n","\n","The $i^{th}$ component of $\\nabla_x f(x_0)$ can be estimated as\n","$$\\frac{f(x_0+\\delta^{i}) - f(x_0-\\delta^{i})}{2\\delta}$$\n","where $\\delta^{i}$ is a column vector whose $i^{th}$ coordinate is\n","$\\delta$, a small constant such as 0.001, and whose other components\n","are zero.\n","Note that adding or subtracting $\\delta^{i}$ is the same as\n","incrementing or decrementing the $i^{th}$ component of $x_0$ by\n","$\\delta$, leaving the other components of $x_0$ unchanged.  Using\n","these results, we can estimate the $i^{th}$ component of the gradient.\n","\n","For example, if $x_0 = (1,1,\\dots,1)^T$ and $\\delta = 0.01$,\n","we may approximate the first component of $\\nabla_x f(x_0)$ as\n","$$\\frac{f((1,1,1,\\dots)^T+(0.01,0,0,\\dots)^T) - f((1,1,1,\\dots)^T-(0.01,0,0,\\dots)^T)}{2\\cdot 0.01}.$$\n","(We add the transpose so that these are column vectors.)\n","**This process should be done for each dimension independently,\n","and together the results of each computation are compiled to give the\n","estimated gradient, which is $d$ dimensional.**\n","\n","Implement this as a function `num_grad` that takes as arguments the\n","objective function `f` and a value of `delta`, and returns a new\n","**function** that takes an `x` (a column vector of parameters) and\n","returns a gradient column vector.\n","\n","**Note:** As in the previous part, make sure you do not modify your input vector."]},{"metadata":{"id":"WPVwGZ-l6XvW","colab_type":"code","colab":{}},"cell_type":"code","source":["def num_grad(f, delta=0.001):\n","    def df(x):\n","        n=x.shape[0]\n","        gradient = np.zeros_like(x)\n","        for i in range(n):\n","          delta_v = np.zeros_like(x)\n","          delta_v[i, 0] = delta\n","          gradient[i,0]=(1./(2*delta))*(f(x+delta_v) - f(x-delta_v))\n","        return gradient      \n","    return df"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kElTR0bL7cbG","colab_type":"text"},"cell_type":"markdown","source":["The test cases are shown below; these use the functions defined in the previous exercise.\n"]},{"metadata":{"id":"3D7BHu4S7Z8D","colab_type":"code","colab":{}},"cell_type":"code","source":["x = cv([0.])\n","ans=(num_grad(f1)(x).tolist(), x.tolist())\n","\n","x = cv([0.1])\n","ans=(num_grad(f1)(x).tolist(), x.tolist())\n","\n","x = cv([0., 0.])\n","ans=(num_grad(f2)(x).tolist(), x.tolist())\n","\n","x = cv([0.1, -0.1])\n","ans=(num_grad(f2)(x).tolist(), x.tolist())"],"execution_count":0,"outputs":[]},{"metadata":{"id":"WASaSsYu75sG","colab_type":"text"},"cell_type":"markdown","source":["A faster (one function evaluation per entry), though sometimes less\n","accurate, estimate is to use:\n","$$\\frac{f(x_0+\\delta^{i}) - f(x_0)}{\\delta}$$\n","for the $i^{th}$ component of $\\nabla_x f(x_0).$"]},{"metadata":{"id":"E31sdqyG78jD","colab_type":"text"},"cell_type":"markdown","source":["### 6.3) Using the Numerical Gradient\n","Recall that our generic gradient descent function takes both a function\n","`f` that returns the value of our function at a given point, and `df`,\n","a function that returns a gradient at a given point.  Write a function\n","`minimize` that takes only a function `f` and uses this function and\n","numerical gradient descent to return the local minimum.  We have\n","provided you with our implementations of `num_grad` and `gd`, so you\n","should not redefine them in the code box below.\n","You may use the default of `delta=0.001` for `num_grad`.\n","\n","**Hint:** Your definition of `minimize` should call `num_grad` exactly\n","once, to return a function that is called many times.\n","You should return the same outputs as `gd`."]},{"metadata":{"id":"CStwqDem76Bx","colab_type":"code","colab":{}},"cell_type":"code","source":["def minimize(f, x0, step_size_fn, max_iter):\n","    df = num_grad(f)\n","    return gd(f, df, x0, step_size_fn, max_iter)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"4gl0FTby8EQq","colab_type":"text"},"cell_type":"markdown","source":["The test cases are below."]},{"metadata":{"id":"UxBLWJFm8DnV","colab_type":"code","colab":{}},"cell_type":"code","source":["ans = package_ans(minimize(f1, cv([0.]), lambda i: 0.1, 1000))\n","\n","ans = package_ans(minimize(f2, cv([0., 0.]), lambda i: 0.01, 1000))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BH-1e98V8LtM","colab_type":"text"},"cell_type":"markdown","source":["## 7) Applying gradient descent to SVM objective\n","\n","**Note:** In this section,\n","you will code many individual functions, each of which depends on previous ones.\n","We **strongly recommend** that you test each of the components on your own to debug.\n","\n","### 7.1) Calculating the SVM objective\n","\n","Implement the single-argument hinge function, which computes $L_h$,\n","and use that to implement hinge loss for a data point and separator.\n","Using the latter function, implement the SVM objective.\n","Note that these functions should work for matrix/vector arguments,\n","so that we can compute the objective for a whole dataset with one call.\n","<pre> x is d x n, y is 1 x n, th is d x 1, th0 is 1 x 1, lam is a scalar </pre>\n","\n","Hint: Look at `np.where` for implementing `hinge`."]},{"metadata":{"id":"F_6E78BF8e2W","colab_type":"code","colab":{}},"cell_type":"code","source":["def hinge(v):\n","  ones = np.ones_like(v)\n","  zeros = np.zeros_like(v)\n","  return np.where(v<1, ones-v, zeros)\n","\n","# x is dxn, y is 1xn, th is dx1, th0 is 1x1\n","def hinge_loss(x, y, th, th0):\n","    v = y*(np.dot(th.T,x)+th0)\n","    loss = hinge(v)\n","    return np.mean(loss, axis=1)\n","\n","# x is dxn, y is 1xn, th is dx1, th0 is 1x1, lam is a scalar\n","def svm_obj(x, y, th, th0, lam):\n","    # hinge loss returns a 1x1 loss vector which is average over all points\n","    loss = hinge_loss(x, y, th, th0)\n","    mag = np.dot(th.T, th)[0,0]\n","    return loss[0] + lam*mag"],"execution_count":0,"outputs":[]},{"metadata":{"id":"muPDf4etYsoy","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":104},"outputId":"22bc59ae-37df-46e7-c7f1-e2c556c32759","executionInfo":{"status":"ok","timestamp":1551470949531,"user_tz":300,"elapsed":248,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["# add your tests here\n","t = np.array([[1,3, -1, 0]]).T\n","hinge(t)\n","print(t)\n","np.dot(t.T, t)"],"execution_count":39,"outputs":[{"output_type":"stream","text":["[[ 1]\n"," [ 3]\n"," [-1]\n"," [ 0]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[11]])"]},"metadata":{"tags":[]},"execution_count":39}]},{"metadata":{"id":"QY1NJEOP8jCC","colab_type":"text"},"cell_type":"markdown","source":["In the test cases for this problem, we'll use the following\n","`super_simple_separable` test dataset and test separator for some of\n","the tests.  A couple of the test cases are also shown below."]},{"metadata":{"id":"POFvK7zW8iYK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":191},"outputId":"adef602b-f91a-4c06-b0d8-83fe81dc819e","executionInfo":{"status":"ok","timestamp":1551470952126,"user_tz":300,"elapsed":390,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["def super_simple_separable():\n","    X = np.array([[2, 3, 9, 12],\n","                  [5, 2, 6, 5]])\n","    y = np.array([[1, -1, 1, -1]])\n","    return X, y\n","\n","sep_e_separator = np.array([[-0.40338351], [1.1849563]]), np.array([[-2.26910091]])\n","\n","# Test case 1\n","x_1, y_1 = super_simple_separable()\n","th1, th1_0 = sep_e_separator\n","ans = svm_obj(x_1, y_1, th1, th1_0, .1)\n","print(ans)\n","# Test case 2\n","ans = svm_obj(x_1, y_1, th1, th1_0, 0.0)\n","print(ans)"],"execution_count":40,"outputs":[{"output_type":"stream","text":["[[2.84891357 1.10933884 1.2101853  1.18492153]]\n","[[0. 0. 0. 0.]]\n","[0.]\n","1.5668396890496104\n","0.15668396890496106\n","[[2.84891357 1.10933884 1.2101853  1.18492153]]\n","[[0. 0. 0. 0.]]\n","[0.]\n","1.5668396890496104\n","0.0\n"],"name":"stdout"}]},{"metadata":{"id":"ZjBB0R4u8tF1","colab_type":"text"},"cell_type":"markdown","source":["### 7.2) Calculating the SVM gradient\n","\n","Define a function `svm_obj_grad` that returns the gradient of the SVM\n","objective function with respect to $\\theta$ and $\\theta_0$ in a single\n","column vector.  The last component of the gradient vector should be\n","the partial derivative with respect to $\\theta_0$.  Look at\n","`np.vstack` as a simple way of stacking two matrices/vectors\n","vertically.  We have broken it down into pieces that mimic steps in\n","the chain rule; this leads to code that is a bit inefficient but\n","easier to write and debug.  We can worry about efficiency later."]},{"metadata":{"id":"lAtDiGVK8vnt","colab_type":"code","colab":{}},"cell_type":"code","source":["# Returns the gradient of hinge(v) with respect to v.\n","def d_hinge(v):\n","    return np.where(v>=1, 0, -1)\n","\n","# Returns the gradient of hinge_loss(x, y, th, th0) with respect to th\n","def d_hinge_loss_th(x, y, th, th0):\n","  v = y*(np.dot(th.T, x)+th0)\n","  deriv_hinge = d_hinge(v)\n","  th_grad = y*deriv_hinge*x\n","  print(th_grad.shape)\n","  return th_grad\n","  \n","\n","# Returns the gradient of hinge_loss(x, y, th, th0) with respect to th0\n","def d_hinge_loss_th0(x, y, th, th0):\n","  v = y*(np.dot(th.T, x)+th0)\n","  deriv_hinge = d_hinge(v)\n","  th0_grad = y*deriv_hinge\n","  print(th0_grad.shape)\n","  return th0_grad\n","\n","# Returns the gradient of svm_obj(x, y, th, th0) with respect to th\n","def d_svm_obj_th(x, y, th, th0, lam):\n","    return np.mean(d_hinge_loss_th(x, y, th, th0), axis=1, keepdims=True) + 2*lam*th\n","\n","# Returns the gradient of svm_obj(x, y, th, th0) with respect to th0\n","def d_svm_obj_th0(x, y, th, th0, lam):\n","    return np.mean(d_hinge_loss_th0(x, y, th, th0), axis=1, keepdims=True)\n","\n","# Returns the full gradient as a single vector\n","def svm_obj_grad(X, y, th, th0, lam):\n","  th0_grad = d_svm_obj_th0(X, y, th, th0, lam)\n","  th_grad = d_svm_obj_th(X, y, th, th0, lam)\n","  return np.vstack((th_grad, th0_grad))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3amReUfg7ORe","colab_type":"code","colab":{}},"cell_type":"code","source":["np.vstack((np.array([[1, 2, 3, 9, 10]])))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"OPjFac-nY1Z4","colab_type":"code","colab":{}},"cell_type":"code","source":["# add your tests here"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LDP6H_2P80vm","colab_type":"text"},"cell_type":"markdown","source":["Some test cases that may be of use are shown below."]},{"metadata":{"id":"xNuF6-c38yji","colab_type":"code","colab":{}},"cell_type":"code","source":["X1 = np.array([[1, 2, 3, 9, 10]])\n","y1 = np.array([[1, 1, 1, -1, -1]])\n","th1, th10 = np.array([[-0.31202807]]), np.array([[1.834     ]])\n","X2 = np.array([[2, 3, 9, 12],\n","               [5, 2, 6, 5]])\n","y2 = np.array([[1, -1, 1, -1]])\n","th2, th20=np.array([[ -3.,  15.]]).T, np.array([[ 2.]])"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tZ9Q6k935tLY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"6b253b50-8936-4b7f-d6ce-902dae8a6ece","executionInfo":{"status":"ok","timestamp":1551471504392,"user_tz":300,"elapsed":363,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["d_hinge(np.array([[ 71.]])).tolist()\n","d_hinge(np.array([[ -23.]])).tolist()\n","d_hinge(np.array([[ 71, -23.]])).tolist()"],"execution_count":44,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[0, -1]]"]},"metadata":{"tags":[]},"execution_count":44}]},{"metadata":{"id":"5fcjaWEo5uWo","colab_type":"code","colab":{}},"cell_type":"code","source":["d_hinge_loss_th(X2[:,0:1], y2[:,0:1], th2, th20).tolist()\n","d_hinge_loss_th(X2, y2, th2, th20).tolist()\n","d_hinge_loss_th0(X2[:,0:1], y2[:,0:1], th2, th20).tolist()\n","d_hinge_loss_th0(X2, y2, th2, th20).tolist()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FkjrEb5y5xBU","colab_type":"code","colab":{}},"cell_type":"code","source":["d_svm_obj_th(X2[:,0:1], y2[:,0:1], th2, th20, 0.01).tolist()\n","d_svm_obj_th(X2, y2, th2, th20, 0.01).tolist()\n","d_svm_obj_th0(X2[:,0:1], y2[:,0:1], th2, th20, 0.01).tolist()\n","d_svm_obj_th0(X2, y2, th2, th20, 0.01).tolist()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"BB7axWab58Lp","colab_type":"code","colab":{}},"cell_type":"code","source":["svm_obj_grad(X2, y2, th2, th20, 0.01).tolist()\n","svm_obj_grad(X2[:,0:1], y2[:,0:1], th2, th20, 0.01).tolist()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"3vf6OFEU89pC","colab_type":"text"},"cell_type":"markdown","source":["### 7.3) Batch SVM minimize\n","\n","Putting it all together, use the functions you built earlier to write\n","a gradient descent minimizer for the SVM objective.  You do not need\n","to paste in your previous definitions; you can just call the ones\n","defined by the staff.  You will need to call `gd`, which is already\n","defined for you as well; your function `batch_svm_min` should return\n","the values that `gd` does.\n","\n","* Initialize all the separator parameters to zero,\n","* use the step size function provided below, and\n","* specify 10 iterations."]},{"metadata":{"id":"dIqWIYnq8_Nb","colab_type":"code","colab":{}},"cell_type":"code","source":["def batch_svm_min(data, labels, lam):\n","    def svm_min_step_size_fn(i):\n","       return 2/(i+1)**0.5\n","    ths = np.zeros((data.shape[0]+1, 1))\n","\n","    def f(x):\n","      th = x[:-1,:]\n","      th0 = x[-1:,:]\n","      return svm_obj(data, labels, th, th0, lam)\n","    def df(x):\n","      th = x[:-1,:]\n","      th0 = x[-1:,:]\n","      return svm_obj_grad(data, labels, th, th0, lam)\n","    return gd(f, df, ths, svm_min_step_size_fn, 10)\n","\n","  \n"," "],"execution_count":0,"outputs":[]},{"metadata":{"id":"aCqScUf_AKKx","colab_type":"code","colab":{}},"cell_type":"code","source":["def batch_svm_min(data, labels, lam):\n","    def svm_min_step_size_fn(i):\n","       return 2/(i+1)**0.5\n","    init = np.zeros((data.shape[0] + 1, 1))\n","\n","    def f(th):\n","      return svm_obj(data, labels, th[:-1, :], th[-1:,:], lam)\n","\n","    def df(th):\n","      return svm_obj_grad(data, labels, th[:-1, :], th[-1:,:], lam)\n","\n","    x, fs, xs = gd(f, df, init, svm_min_step_size_fn, 10)\n","    return x, fs, xs"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SP-lzwBg-0vF","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"e4fd933b-8e89-4e17-9d5f-dc0b3d559ae5","executionInfo":{"status":"ok","timestamp":1551473021059,"user_tz":300,"elapsed":381,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["t[-1:,:]"],"execution_count":60,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0]])"]},"metadata":{"tags":[]},"execution_count":60}]},{"metadata":{"id":"JH4xd7C-9BIm","colab_type":"text"},"cell_type":"markdown","source":["Test cases are shown below, where an additional separable test\n","data set has been specified."]},{"metadata":{"id":"HgOC_i879Acd","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2101},"outputId":"ef525428-0fda-43e5-edea-16a484b54da4","executionInfo":{"status":"ok","timestamp":1551473021624,"user_tz":300,"elapsed":412,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["def separable_medium():\n","    X = np.array([[2, -1, 1, 1],\n","                  [-2, 2, 2, -1]])\n","    y = np.array([[1, -1, 1, -1]])\n","    return X, y\n","sep_m_separator = np.array([[ 2.69231855], [ 0.67624906]]), np.array([[-3.02402521]])\n","\n","x_1, y_1 = super_simple_separable()\n","ans = package_ans(batch_svm_min(x_1, y_1, 0.0001))\n","\n","x_1, y_1 = separable_medium()\n","ans = package_ans(batch_svm_min(x_1, y_1, 0.0001))"],"execution_count":61,"outputs":[{"output_type":"stream","text":["[[ 0. -0.  0. -0.]]\n","[[1. 1. 1. 1.]]\n","[1.]\n","0.0\n","(1, 4)\n","(2, 4)\n","[[ 6.  2. -6. 14.]]\n","[[0. 0. 7. 0.]]\n","[1.75]\n","8.0\n","(1, 4)\n","(2, 4)\n","[[ 23.32241908 -12.14270131  35.72099715 -35.14788109]]\n","[[ 0.         13.14270131  0.         36.14788109]]\n","[12.3226456]\n","18.37903446948112\n","(1, 4)\n","(2, 4)\n","[[  3.97588063   5.46920449 -15.96001971  27.50265852]]\n","[[ 0.          0.         16.96001971  0.        ]]\n","[4.24000493]\n","14.315108812636531\n","(1, 4)\n","(2, 4)\n","[[16.2250407  -4.53184459 13.54312754 -7.25279725]]\n","[[0.         5.53184459 0.         8.25279725]]\n","[3.44616046]\n","13.75534554956713\n","(1, 4)\n","(2, 4)\n","[[  1.24048751   9.10897606 -26.48490724  41.27117059]]\n","[[ 0.          0.         27.48490724  0.        ]]\n","[6.87122681]\n","22.204548040443697\n","(1, 4)\n","(2, 4)\n","[[11.24229931  0.94259151 -2.39400189 12.8912436 ]]\n","[[0.         0.05740849 3.39400189 0.        ]]\n","[0.86285259]\n","16.43349295270971\n","(1, 4)\n","(2, 4)\n","[[17.28799841 -3.97105635 12.34694171 -4.49703834]]\n","[[0.         4.97105635 0.         5.49703834]]\n","[2.61702367]\n","17.73716854555026\n","(1, 4)\n","(2, 4)\n","[[  5.44148426   6.81291433 -19.29786354  33.86417119]]\n","[[ 0.          0.         20.29786354  0.        ]]\n","[5.07446589]\n","23.147811782194218\n","(1, 4)\n","(2, 4)\n","[[13.60734934  0.14541533  0.37130011 10.69306536]]\n","[[0.         0.85458467 0.62869989 0.        ]]\n","[0.37082114]\n","20.14999716613419\n","(1, 4)\n","(2, 4)\n","[[ 0. -0.  0. -0.]]\n","[[1. 1. 1. 1.]]\n","[1.]\n","0.0\n","(1, 4)\n","(2, 4)\n","[[ 4.   2.5  0.5 -2. ]]\n","[[0.  0.  0.5 3. ]]\n","[0.875]\n","2.5\n","(1, 4)\n","(2, 4)\n","[[ 1.87754829  0.37797255  2.62117892 -0.93877414]]\n","[[0.         0.62202745 0.         1.93877414]]\n","[0.6402004]\n","2.5632258146578053\n","(1, 4)\n","(2, 4)\n","[[ 1.87711468  1.5325858   1.46587305 -0.64988221]]\n","[[0.         0.         0.         1.64988221]]\n","[0.41247055]\n","2.321671204191923\n","(1, 4)\n","(2, 4)\n","[[0.62662379 1.03239475 1.4654644  0.10036324]]\n","[[0.37337621 0.         0.         0.89963676]]\n","[0.31825324]\n","1.8322491134305627\n","(1, 4)\n","(2, 4)\n","[[ 1.52079089  1.70317847  1.24144745 -0.34672031]]\n","[[0.         0.         0.         1.34672031]]\n","[0.33668008]\n","2.256653971428214\n","(1, 4)\n","(2, 4)\n","[[0.49978671 1.29478715 1.24110962 0.26584385]]\n","[[0.50021329 0.         0.         0.73415615]]\n","[0.30859236]\n","1.8600163096945446\n","(1, 4)\n","(2, 4)\n","[[ 1.25548415  1.86169405  1.0517838  -0.11200487]]\n","[[0.         0.         0.         1.11200487]]\n","[0.27800122]\n","2.2202199095214565\n","(1, 4)\n","(2, 4)\n","[[0.37127725 1.50802325 1.05148919 0.41848693]]\n","[[0.62872275 0.         0.         0.58151307]]\n","[0.30255895]\n","1.8778681770922359\n","(1, 4)\n","(2, 4)\n","[[1.03773331 2.00798328 0.88452122 0.0852589 ]]\n","[[0.         0.         0.11547878 0.9147411 ]]\n","[0.25755497]\n","2.196142627413645\n","(1, 4)\n","(2, 4)\n"],"name":"stdout"}]},{"metadata":{"id":"4tFslLvo5X9w","colab_type":"text"},"cell_type":"markdown","source":["### 7.4) Numerical SVM objective (Optional)\n","\n","Recall from the previous question that we were able to closely approximate gradients\n","with numerical estimates.\n","We may apply the same technique to optimize the SVM objective.\n","\n","Using your definition of `minimize` and `num_grad` from the previous problem,\n","implement a function that optimizes the SVM objective through numeric approximations.\n","\n","How well does this function perform, compared to the analytical result?\n","Consider both accuracy and runtime."]},{"metadata":{"id":"bLt1LwN05b3k","colab_type":"code","colab":{}},"cell_type":"code","source":["# your code here"],"execution_count":0,"outputs":[]}]}