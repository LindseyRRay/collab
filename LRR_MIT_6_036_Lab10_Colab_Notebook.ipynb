{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LRR MIT 6.036 Lab10 Colab Notebook",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lrraymond13/collab/blob/master/LRR_MIT_6_036_Lab10_Colab_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "q58cS9antfCw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#MIT 6.036 Spring 2019: Lab10#\n",
        "\n",
        "This colab notebook runs the companion code for question 3 in MIT 6.036 Lab 10. You can work out your solutions here, then submit your results back on the lab page when ready. If you have not used colab before, ask your partner or a TA for help.\n",
        "\n",
        "## <section>**Setup**</section>\n",
        "\n",
        "First, run the next code block to download and import the code for this lab."
      ]
    },
    {
      "metadata": {
        "id": "OUEtSZRdtmI2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!rm -rf code_for_lab10* __MACOSX data .DS_Store\n",
        "!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/labs/lab10/code_for_lab10.zip\n",
        "!unzip code_for_lab10.zip\n",
        "!mv code_for_lab10/* .\n",
        "\n",
        "from code_for_lab10 import *\n",
        "\n",
        "import numpy as np\n",
        "import math as m\n",
        "import random\n",
        "\n",
        "import importlib\n",
        "import tensorflow"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3Zhptv005XBN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# 3) A Wild Epsilon Chase\n",
        "\n",
        "$\\epsilon$ typically denotes an \"explore vs. exploit\" tradeoff in reinforcement learning, specifically Q-learning algorithm. As a reminder, when a Q-learning algorithm interacts with the environment, it takes a completely random move with probability $\\epsilon$ and the best move according to its current Q-table with probability $1-\\epsilon$.\n",
        "\n",
        "We will try to teach a computer how to play a simple game with a Q-learning algorithm. We will observe how the learning differs based on different values of $\\epsilon$."
      ]
    },
    {
      "metadata": {
        "id": "cKlbS6ZMv8c8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.1 Expectations\n",
        "\n",
        "Before you move on, write down how you expect the Q-learning algorithm to behave for different values of $\\epsilon$ in the set $\\{0, 0.5, 1\\}$.Think about whether they will ever converge to an optimal Q-value function or policy, and how fast."
      ]
    },
    {
      "metadata": {
        "id": "3M4mYROqdiGK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3.2 Versus Reality"
      ]
    },
    {
      "metadata": {
        "id": "DFo6mGJvdkct",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Once you run the code below wait patiently until you see a yellow and purple square moving around on a teal background.  Ignore everything else for now."
      ]
    },
    {
      "metadata": {
        "id": "MNN6wDVGdncW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try_tabular_batch_q_learning(iters=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-VHQyT54dszA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Click play in the button right below the square. This is a movie of a policy playing the game [No Exit](https://en.wikipedia.org/wiki/No_Exit). It’s kind of like Pong: the purple square is the “ball” and the yellow square is your “paddle”. The actions are to move the paddle up, down, or keep it still.\n",
        "\n",
        "The state is specified by the positions and velocities of the ball and paddle, with a special added “game over” state.\n",
        "\n",
        "The transition model is a very approximate physics model of the ball reflecting off walls and the paddle, except if the ball gets past the paddle in the positive x direction, the game is over.\n",
        "\n",
        "The agent gets a reward of +1 on every step it manages to survive.\n",
        "\n",
        "When watching the game play out, you’ll sometimes see that the purple square gets near the right-hand border and then suddenly it changes to a state with the purple square in the bottom left and the yellow one in the upper right -- this means that the game terminated and then reset to the initial state.\n",
        "\n",
        "Now we can go back and look at the other output in the notebook:\n",
        "\n",
        "* First, we show a table of what happens during learning: after every 10 iterations of batch Q learning, we take the current greedy policy and run it to see what its average score is. This score represents how long the episode ran before the ball ran off the map, or 100 if it lasted for that long.\n",
        "\n",
        "* Next is a plot of the score as a function of the amount of training.\n",
        "\n",
        "* Finally, we run the greedy policy with respect last Q-value function for 10 games and report the rewards achieved on each game. We also make a movie of these 10 games, which is what we started out looking at."
      ]
    },
    {
      "metadata": {
        "id": "GRwAr3O_wAEM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**3.2)** Run the code given on the notebook for different values of $\\epsilon$ in the set $\\{0, 0.5, 1\\}$. Does your observation match with what you wrote down?\n",
        "\n",
        "Remember that this is a small instance, so sometimes the random noise of the environment might prevent you from seeing any useful information. Run the notebook two or three times if something doesn't line up with your expectation, and then ask for help."
      ]
    },
    {
      "metadata": {
        "id": "Gb1bl_jCWVpY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try_tabular_batch_q_learning(iters=100, eps=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aEj9VmCsWc1o",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try_tabular_batch_q_learning(iters=100, eps=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0rL7_Z9sWffh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "try_tabular_batch_q_learning(iters=100, eps=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "keaYd0d4wG2_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**(Optional)** Once you are done with the check off, play with the number of iterations in the colab until you get all of the models to converge, and then observe the gameplay of the agents. You might expect the model that learned with $\\epsilon = 1$ has a more jittery gameplay than the agent that learned with $\\epsilon=0$ or $0.5$. Does that hold?\n"
      ]
    }
  ]
}