{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LRR MIT6.036 hw02 colab notebook","version":"0.3.2","provenance":[{"file_id":"1kIWGi2TEZ0Agd_qQt1KiD2uZw_5AJFHT","timestamp":1550174897070},{"file_id":"1j2fQR-NQxnazY5MSHqxwlS8z96n8YFMg","timestamp":1549860446001}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"_xIaEwCD406A","colab_type":"text"},"cell_type":"markdown","source":["#MIT 6.036 Spring 2019: Homework 2#\n","\n","This colab notebook provides code and a framework for problems 7-10 of [the homework](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week2/week2_homework/1).  You can work out your solutions here, then submit your results back on the homework page when ready.\n","\n","## <section>**Setup**</section>\n","\n","First, download the code distribution for this homework that contains test cases and helper functions (such as `positive`).\n","\n","Run the next code block to download and import the code for this lab.\n"]},{"metadata":{"id":"2YM-_zLf9Bp-","colab_type":"code","outputId":"c2b734f2-e7f7-41df-fce5-d2741d63d1a1","executionInfo":{"status":"ok","timestamp":1550174939402,"user_tz":300,"elapsed":8167,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}},"colab":{"base_uri":"https://localhost:8080/","height":243}},"cell_type":"code","source":["!rm -f code_for_hw02.py*\n","!wget --quiet https://introml.odl.mit.edu/cat-soop/_static/6.036/homework/hw02/code_for_hw02.py\n","from code_for_hw02 import *"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Importing code_for_hw02\n","New procedures added: tidy_plot, plot_separator, plot_data, plot_nonlin_sep, cv,\n","                      rv, y, positive, score\n","Data Sets: super_simple_separable_through_origin(), super_simple_separable(), xor(),\n","           xor_more()\n","Test data for problem 2.1: data1, labels1, data2, labels2\n","Test data for problem 2.2: big_data, big_data_labels, gen_big_data(), gen_lin_separable(),\n","                           big_higher_dim_separable(), gen_flipped_lin_separable()\n","Test functions: test_linear_classifier(), test_perceptron(), test_averaged_perceptron(),\n","                test_eval_classifier(), test_eval_learning_alg(), test_xval_learning_alg()\n","\n","For more information, use 'help', e.g. 'help tidy_plot'\n","Done with import of code_for_hw02\n"],"name":"stdout"}]},{"metadata":{"id":"2z1zuhqltjBy","colab_type":"code","outputId":"224f64bc-2c28-498d-a1ff-b87bc3c298e2","executionInfo":{"status":"ok","timestamp":1550158102068,"user_tz":300,"elapsed":353,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}},"colab":{"base_uri":"https://localhost:8080/","height":139}},"cell_type":"code","source":["help(tidy_plot)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Help on function tidy_plot in module code_for_hw02:\n","\n","tidy_plot(xmin, xmax, ymin, ymax, center=False, title=None, xlabel=None, ylabel=None)\n","    Set up axes for plotting\n","    xmin, xmax, ymin, ymax = (float) plot extents\n","    Return matplotlib axes\n","\n"],"name":"stdout"}]},{"metadata":{"id":"TNV0QHn2pWi1","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","import numpy as np"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gcdW07Fb-YKx","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":52},"outputId":"111530ab-0574-413e-d0ee-f27b2dd345cd","executionInfo":{"status":"ok","timestamp":1550180797015,"user_tz":300,"elapsed":406,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["np.zeros((2,1))"],"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.],\n","       [0.]])"]},"metadata":{"tags":[]},"execution_count":3}]},{"metadata":{"id":"_bhI4dQB1-UZ","colab_type":"text"},"cell_type":"markdown","source":["# <section>**7) Implement perceptron**</section>\n","\n","Implement [the perceptron algorithm](https://lms.mitx.mit.edu/courses/course-v1:MITx+6.036+2019_Spring/courseware/Week2/perceptron/2), where\n","\n","* `data` is a numpy array of dimension $d$ by $n$\n","* `labels` is numpy array of dimension $1$ by $n$\n","* `params` is a dictionary specifying extra parameters to this algorithm; your algorithm should run a number of iterations equal to $T$\n","* `hook` is either None or a function that takes the tuple `(th, th0)` as an argument and displays the separator graphically.  We won't be testing this in the Tutor, but it will help you in debugging on your own machine.\n","\n","It should return a tuple of $\\theta$ (a $d$ by 1 array) and $\\theta_0$ (a 1 by 1 array).\n","\n","We have given you some  data sets in the code file for you to test your implementation.\n","\n","Your function should initialize all parameters to 0, then run through the data, in the order it is given, performing an update to the parameters whenever the current parameters would make a mistake on that data point. Perform $T$ iterations through the data. "]},{"metadata":{"id":"VtYf8ysk-VQU","colab_type":"code","colab":{}},"cell_type":"code","source":["def perceptron(data, labels, params = {}, hook = None):\n","    # if T not in params, default to 100\n","    T = params.get('T', 100)\n","    n = data.shape[1]\n","    print(n)\n","    # Your implementation here\n","    theta = np.zeros((data.shape[0], 1))\n","    theta_0 = np.zeros((1,1))\n","    for t in range(T):\n","      mistakes = 0\n","      for i in range(n):\n","        # returns -1 if sign < 0, 0 if == 0 and + 1 if sign > 0\n","        # 1d array\n","        y_i = labels[:, i]\n","        # dx1 2 dim numpy array\n","        x_i = data[:, i:i+1]\n","        if np.sign(y_i*(np.dot(theta.T, x_i) + theta_0))< 1:\n","          # correct and update thetas\n","          theta += y_i*x_i\n","          theta_0 += y_i\n","          mistakes +=1\n","      if mistakes==0:\n","        return theta, theta_0\n","    return theta, theta_0\n","        \n","      "],"execution_count":0,"outputs":[]},{"metadata":{"id":"2r0Rog0g92ol","colab_type":"code","colab":{}},"cell_type":"code","source":["# solution implementation\n","import numpy as np\n","\n","\n","# x is dimension d by 1\n","# th is dimension d by 1\n","# th0 is dimension 1 by 1\n","# return 1 by 1 matrix of +1, 0, -1\n","def positive(x, th, th0):\n","   return np.sign(th.T@x + th0)\n","\n","# Perceptron algorithm with offset.\n","# data is dimension d by n\n","# labels is dimension 1 by n\n","# T is a positive integer number of steps to run\n","# Perceptron algorithm with offset.\n","# data is dimension d by n\n","# labels is dimension 1 by n\n","# T is a positive integer number of steps to run\n","def perceptron(data, labels, params = {}, hook = None):\n","    # if T not in params, default to 100\n","    T = params.get('T', 100)\n","    (d, n) = data.shape\n","\n","    theta = np.zeros((d, 1)); theta_0 = np.zeros((1, 1))\n","    for t in range(T):\n","        for i in range(n):\n","            x = data[:,i:i+1]\n","            y = labels[:,i:i+1]\n","            if y * positive(x, theta, theta_0) <= 0.0:\n","                theta = theta + y * x\n","                theta_0 = theta_0 + y\n","                if hook: hook((theta, theta_0))\n","    return theta, theta_0\n","#Note, the solution doesn't have to pretty; it's far better that it is understandable.\n","    "],"execution_count":0,"outputs":[]},{"metadata":{"id":"92r2oL42-yfM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"fedac0e6-0ba0-4954-b84b-47e04959d1b3","executionInfo":{"status":"ok","timestamp":1550182167288,"user_tz":300,"elapsed":400,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["test_perceptron(perceptron)"],"execution_count":25,"outputs":[{"output_type":"stream","text":["4\n","-----------Test Perceptron 0-----------\n","Passed! \n","\n","4\n","-----------Test Perceptron 1-----------\n","Passed! \n","\n"],"name":"stdout"}]},{"metadata":{"id":"hQMcSWlmB4-Y","colab_type":"text"},"cell_type":"markdown","source":["# <section>8) Implement averaged perceptron</section>\n","\n","Regular perceptron can be somewhat sensitive to the most recent examples that it sees. Instead, averaged perceptron produces a more stable output by outputting the average value of `th` and `th0` across all iterations.\n","\n","Implement averaged perceptron with the same spec as regular perceptron, and using the pseudocode below as a guide.\n","<pre>\n","procedure averaged_perceptron({(x^(i), y^(i)), i=1,...n}, T)\n","    th = 0 (d by 1); th0 = 0 (1 by 1)\n","    ths = 0 (d by 1); th0s = 0 (1 by 1)\n","    for t = 1,...,T do:\n","        for i = 1,...,n do:\n","\t        if y^(i)(th . x^(i) + th0) <= 0 then\n","              th = th + y^(i)x^(i)\n","              th0 = th0 + y^(i)\n","\t        ths = ths + th\n","\t        th0s = th0s + th0\n","    return ths/(nT), th0s/(nT)\n","</pre>"]},{"metadata":{"id":"XAwW00MU_FzS","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","# theta is dx1, theta0 is 1x1\n","# thetas is dx1, theta0s is 1x1\n","def averaged_perceptron(data, labels, params={}, hook=None):\n","    # if T not in params, default to 100\n","    T = params.get('T', 100)\n","    # Your implementation here\n","    # initialize thetas\n","    d, n = data.shape\n","    theta = np.zeros((d,1))\n","    thetas = np.zeros((d,1))\n","    theta0 = np.zeros((1,1))\n","    theta0s = np.zeros((1,1))\n","    for t in range(T):\n","      \n","      for i in range(n):\n","        # slice along both dimensions to preserve 2d array\n","        y_i = labels[:, i:i+1]\n","        x_i = data[:, i:i+1]\n","        if y_i*(np.dot(theta.T, x_i)+theta0) <=0:\n","          theta += y_i*x_i\n","          theta0 += y_i\n","          if hook:\n","            hook((theta, theta0))\n","        # after updating the valye of thetas add to averaged value\n","        thetas += theta\n","        theta0s += theta0\n","       \n","    return (thetas/(n*T)), (theta0s/(n*T))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ksVs6pbiG6Pi","colab_type":"code","colab":{}},"cell_type":"code","source":["# Solution Implementation\n","import numpy as np\n","\n","\n","# x is dimension d by 1\n","# th is dimension d by 1\n","# th0 is dimension 1 by 1\n","# return 1 by 1 matrix of +1, 0, -1\n","def positive(x, th, th0):\n","   return np.sign(th.T@x + th0)\n","\n","def averaged_perceptron(data, labels, params = {}, hook = None):\n","    T = params.get('T', 100)\n","    (d, n) = data.shape\n","\n","    theta = np.zeros((d, 1)); theta_0 = np.zeros((1, 1))\n","    theta_sum = theta.copy() \n","    theta_0_sum = theta_0.copy()\n","    for t in range(T):\n","        for i in range(n):\n","            x = data[:,i:i+1]\n","            y = labels[:,i:i+1]\n","            if y * positive(x, theta, theta_0) <= 0.0:\n","                theta = theta + y * x\n","                theta_0 = theta_0 + y\n","                if hook: hook((theta, theta_0))\n","            theta_sum = theta_sum + theta\n","            theta_0_sum = theta_0_sum + theta_0\n","    theta_avg = theta_sum / (T*n)\n","    theta_0_avg = theta_0_sum / (T*n)\n","    if hook: hook((theta_avg, theta_0_avg))\n","    return theta_avg, theta_0_avg"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kyLGH0_cBFSU","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":121},"outputId":"1dfc1816-c538-4e26-dee5-ea9db2949925","executionInfo":{"status":"ok","timestamp":1550182952547,"user_tz":300,"elapsed":439,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["test_averaged_perceptron(averaged_perceptron)"],"execution_count":34,"outputs":[{"output_type":"stream","text":["-----------Test Averaged Perceptron 0-----------\n","Passed! \n","\n","-----------Test Averaged Perceptron 1-----------\n","Passed! \n","\n"],"name":"stdout"}]},{"metadata":{"id":"NTfGq7LNGceQ","colab_type":"text"},"cell_type":"markdown","source":["# 9) Implement evaluation strategies\n","  \n","## 9.1)  Evaluating a classifier\n","\n","To evaluate a classifier, we are interested in how well it performs on data that it wasn't trained on. Construct a testing procedure that uses a training data set, calls a learning algorithm to get a linear separator (a tuple of $\\theta, \\theta_0$), and then reports the percentage correct on a new testing set as a float between 0. and 1..\n","\n","The learning algorithm is passed as a function that takes a data array and a labels vector.  Your evaluator should be able to interchangeably evaluate `perceptron` or `averaged_perceptron` (or future algorithms with the same spec), depending on what is passed through the `learner` parameter.\n","\n","The `eval_classifier` function should accept the following parameters:\n","\n","* <tt>learner</tt> - a function, such as perceptron or averaged_perceptron\n","* <tt>data_train</tt> - training data\n","* <tt>labels_train</tt> - training labels\n","* <tt>data_test</tt> - test data\n","* <tt>labels_test</tt> - test labels\n","\n","Assume that you have available the function `score` from HW 1, which takes inputs:\n","\n","* <tt>data</tt>: a <tt>d</tt> by <tt>n</tt> array of floats (representing <tt>n</tt> data points in <tt>d</tt> dimensions)\n","* <tt>labels</tt>: a <tt>1</tt> by <tt>n</tt> array of elements in <tt>(+1, -1)</tt>, representing target labels\n","* <tt>th</tt>: a <tt>d</tt> by <tt>1</tt> array of floats that together with\n","* <tt>th0</tt>: a single scalar or 1 by 1 array, represents a hyperplane\n","\n","and returns 1 by 1 matrix with an integer indicating number of data points correct for the separator."]},{"metadata":{"id":"uSip1lfHBKaT","colab_type":"code","colab":{}},"cell_type":"code","source":["def eval_classifier(learner, data_train, labels_train, data_test, labels_test):\n","    th, th0 = learner(data_train, labels_train)\n","    correct =  score(data_test, labels_test, th, th0)\n","    size_test = data_test.shape[1]\n","    # get percentage of points scored correctly\n","    return correct/size_test"],"execution_count":0,"outputs":[]},{"metadata":{"id":"beHMGAb6BTu1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"19972f09-e07a-4520-951b-9106be796f85","executionInfo":{"status":"ok","timestamp":1550183324344,"user_tz":300,"elapsed":454,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["test_eval_classifier(eval_classifier,perceptron)"],"execution_count":45,"outputs":[{"output_type":"stream","text":["16\n","-----------Test Eval Classifier 0-----------\n","Passed! \n","\n","19\n","-----------Test Eval Classifier 1-----------\n","Passed! \n","\n"],"name":"stdout"}]},{"metadata":{"id":"-WPStky3GiJb","colab_type":"text"},"cell_type":"markdown","source":["## <subsection>9.2) Evaluating a learning algorithm using a data source</subsection>\n","\n","Construct a testing procedure that takes a learning algorithm and a data source as input and runs the learning algorithm multiple times, each time evaluating the resulting classifier as above. It should report the overall average classification accuracy.\n","\n","You can use our implementation of `eval_classifier` as above.\n","\n","Write the function `eval_learning_alg` that takes:\n","\n","* <tt>learner</tt> - a function, such as perceptron or averaged_perceptron\n","* <tt>data_gen</tt> - a data generator, call it with a desired data set size; returns a tuple (data, labels)\n","* <tt>n_train</tt> - the size of the learning sets\n","* <tt>n_test</tt> - the size of the test sets\n","* <tt>it</tt> - the number of iterations to average over\n","\n","and returns the average classification accuracy as a float between 0. and 1..  \n","\n","** Note: Be sure to generate your training data and then testing data in that order, to ensure that the pseudorandomly generated data matches that in the test code. **"]},{"metadata":{"id":"6qytb8giBXZq","colab_type":"code","colab":{}},"cell_type":"code","source":["def eval_learning_alg(learner, data_gen, n_train, n_test, it):\n","    scores = 0\n","    for i in range(it):\n","      # gen train data\n","      data_train, labels_train = data_gen(n_train)\n","      # gen test data set\n","      data_test, labels_test = data_gen(n_test)\n","      scores += eval_classifier(learner, data_train, labels_train, data_test, labels_test)\n","    return scores/it"],"execution_count":0,"outputs":[]},{"metadata":{"id":"uCZojUBJBb06","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"35ac3d9e-d6db-4894-ca7f-927ef4c628ac","executionInfo":{"status":"ok","timestamp":1550183572367,"user_tz":300,"elapsed":402,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["test_eval_learning_alg(eval_learning_alg,perceptron)"],"execution_count":49,"outputs":[{"output_type":"stream","text":["5\n","2\n","3\n","9\n","9\n","-----------Test Eval Learning Algo-----------\n","Passed! \n","\n"],"name":"stdout"}]},{"metadata":{"id":"60u9G0QnGzv-","colab_type":"text"},"cell_type":"markdown","source":["## <subsection>9.3) Evaluating a learning algorithm with a fixed dataset</subsection>\n","\n","Cross-validation is a strategy for evaluating a learning algorithm, using a single training set of size $n$. Cross-validation takes in a learning algorithm $L$, a fixed data set $\\mathcal{D}$, and a parameter $k$. It will run the learning algorithm $k$ different times, then evaluate the accuracy of the resulting classifier, and ultimately return the average of the accuracies over each of the $k$ \"runs\" of $L$. It is structured like this:\n","\n","<pre><code>divide D into k parts, as equally as possible;  call them D_i for i == 0 .. k-1\n","# be sure the data is shuffled in case someone put all the positive examples first in the data!\n","for j from 0 to k-1:\n","    D_minus_j = union of all the datasets D_i, except for D_j\n","    h_j = L(D_minus_j)\n","    score_j = accuracy of h_j measured on D_j\n","return average(score0, ..., score(k-1))\n","</code></pre>\n","\n","So, each time, it trains on  $k−1$ of the pieces of the data set and tests the resulting hypothesis on the piece that was not used for training.\n","\n","When $k=n$, it is called *leave-one-out cross validation*.\n","\n","Implement cross validation **assuming that the input data is shuffled already** so that the positives and negatives are distributed randomly. If the size of the data does not evenly divide by k, split the data into n % k sub-arrays of size n//k + 1 and the rest of size n//k. (Hint: You can use <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.array_split.html\">numpy.array_split</a>\n","and <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html\">numpy.concatenate</a> with axis arguments to split and rejoin the data as you desire.)\n","\n","Note: In Python, n//k indicates integer division, e.g. 2//3 gives 0 and 4//3 gives 1."]},{"metadata":{"id":"W5_iixOmBgR7","colab_type":"code","colab":{}},"cell_type":"code","source":["def xval_learning_alg(learner, data, labels, k):\n","    # assume data  is shuffled randomly\n","    # split data in k parts\n","    scores= 0\n","    # split along the columns\n","    data_array = np.array_split(data, k, axis=1)\n","    print(data_array[0].shape)\n","    labels_array = np.array_split(labels, k, axis=1)\n","    print(labels_array[0].shape)\n","    for j in range(0, k):\n","      print(j)\n","      if j==0:\n","        to_concat_d = data_array[1:]\n","        to_concat_l = labels_array[1:]\n","      elif j==k-1:\n","        to_concat_d = data_array[:j]\n","        to_concat_l = labels_array[:j]\n","      else:\n","        to_concat_d = data_array[:j] + data_array[j+1:]\n","        to_concat_l = labels_array[:j] + labels_array[j+1:]\n","        \n","        \n","      data_train = np.concatenate(to_concat_d, axis=1)\n","      labels_train = np.concatenate(to_concat_l, axis=1)\n","      data_test = data_array[j]\n","      labels_test = labels_array[j]\n","      # train learning algo on train data, return percentage classified correctly on the test set\n","      scores += eval_classifier(learner, data_train, labels_train, data_test, labels_test)\n","    return scores/k"],"execution_count":0,"outputs":[]},{"metadata":{"id":"A-38mQFtQVuO","colab_type":"code","colab":{}},"cell_type":"code","source":["# solution code\n","\n","import numpy as np\n","def xval_learning_alg(learner, data, labels, k):\n","    s_data = np.array_split(data, k, axis=1)\n","    s_labels = np.array_split(labels, k, axis=1)\n","\n","    score_sum = 0\n","    for i in range(k):\n","        data_train = np.concatenate(s_data[:i] + s_data[i+1:], axis=1)\n","        labels_train = np.concatenate(s_labels[:i] + s_labels[i+1:], axis=1)\n","        data_test = np.array(s_data[i])\n","        labels_test = np.array(s_labels[i])\n","        score_sum += eval_classifier(learner, data_train, labels_train,\n","                                              data_test, labels_test)\n","    return score_sum/k"],"execution_count":0,"outputs":[]},{"metadata":{"id":"iUiUgtMHBiZX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":278},"outputId":"68188465-39cc-4b64-8b66-261ee91d353d","executionInfo":{"status":"ok","timestamp":1550185431975,"user_tz":300,"elapsed":801,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["test_xval_learning_alg(xval_learning_alg,perceptron)"],"execution_count":100,"outputs":[{"output_type":"stream","text":["(2, 20)\n","(1, 20)\n","0\n","11\n","1\n","12\n","2\n","10\n","3\n","13\n","4\n","15\n","-----------Test Cross-eval Learning Algo-----------\n","Passed! \n","\n"],"name":"stdout"}]},{"metadata":{"id":"RzDxaghNKwPX","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"82b103f7-f4f1-489e-e1b5-db5175828cd2","executionInfo":{"status":"ok","timestamp":1550184666101,"user_tz":300,"elapsed":414,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["np.array_split(np.array([[0,0, 1, 1]]), 2, axis=1)"],"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[array([[0, 0]]), array([[1, 1]])]"]},"metadata":{"tags":[]},"execution_count":65}]},{"metadata":{"id":"crF8flfB3hr1","colab_type":"text"},"cell_type":"markdown","source":["## 10) Testing\n","\n","In this section, we compare the effectiveness of perceptron and averaged perceptron on some data that are not necessarily linearly separable.\n","\n","Use your `eval_learning_alg` and the `gen_flipped_lin_separable` generator in the code file to evaluate the accuracy of `perceptron` vs. a`veraged_perceptron`. `gen_flipped_lin_separable` can be called with an integer to return a data set and labels. Note that this generates linearly separable data and then \"flips\" the labels with some specified probability (the argument pflip); so most of the results will not be linearly separable. You can also specifiy pflip in the call to the generator. You should use the default values of th and th_0 to retain consistency with the Tutor.\n","\n","Run enough trials so that you can confidently predict the accuracy of these algorithms on new data from that same generator; assume training/test sets on the order of 20 points. The Tutor will check that your answer is within 0.025 of the answer we got using the same generator."]},{"metadata":{"id":"kYrtCNreRt9o","colab_type":"code","colab":{}},"cell_type":"code","source":["def eval_learning_alg_training(learner, data_gen, n_train, n_test, it):\n","    scores = 0\n","    for i in range(it):\n","      # gen train data\n","      data_train, labels_train = data_gen(n_train)\n","      # gen test data set\n","      data_test, labels_test = data_gen(n_test)\n","      scores += eval_classifier(learner, data_train, labels_train, data_train, labels_train)\n","    return scores/it"],"execution_count":0,"outputs":[]},{"metadata":{"id":"CXFoptqiI6Aw","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":903},"outputId":"9d1f236c-396c-4883-a735-e071951b2a3a","executionInfo":{"status":"ok","timestamp":1550185970908,"user_tz":300,"elapsed":791,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["print(eval_learning_alg_training(perceptron, gen_flipped_lin_separable(pflip=.1), 20, 20, 50))"],"execution_count":120,"outputs":[{"output_type":"stream","text":["17\n","17\n","19\n","15\n","18\n","16\n","20\n","20\n","17\n","20\n","15\n","17\n","8\n","17\n","17\n","20\n","18\n","20\n","19\n","15\n","19\n","11\n","17\n","19\n","16\n","13\n","15\n","13\n","18\n","13\n","18\n","14\n","20\n","16\n","16\n","20\n","17\n","3\n","17\n","16\n","18\n","16\n","16\n","19\n","15\n","20\n","18\n","14\n","13\n","11\n","0.8159999999999998\n"],"name":"stdout"}]},{"metadata":{"id":"k1oXc9cfQkcM","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":903},"outputId":"da589d35-f25d-4a41-d0b4-5ec58186d9a3","executionInfo":{"status":"ok","timestamp":1550185964867,"user_tz":300,"elapsed":1144,"user":{"displayName":"Lindsey Raymond","photoUrl":"","userId":"12017110552411429068"}}},"cell_type":"code","source":["print(eval_learning_alg_training(averaged_perceptron, gen_flipped_lin_separable(pflip=.1), 20, 20, 50))"],"execution_count":119,"outputs":[{"output_type":"stream","text":["20\n","16\n","17\n","18\n","16\n","17\n","18\n","16\n","16\n","18\n","15\n","16\n","20\n","20\n","16\n","19\n","16\n","18\n","17\n","19\n","17\n","15\n","18\n","15\n","20\n","17\n","19\n","17\n","15\n","14\n","19\n","16\n","18\n","17\n","18\n","20\n","16\n","19\n","18\n","16\n","16\n","15\n","14\n","20\n","17\n","19\n","20\n","17\n","17\n","18\n","0.8650000000000001\n"],"name":"stdout"}]},{"metadata":{"id":"kUTSWMd4RTxo","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"3-x1ZCybQvw1","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}