{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow with GPU V2",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Lrraymond13/collab/blob/master/TensorFlow_with_GPU_V2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tMce8muBqXQP"
      },
      "source": [
        "# Generate ELMo Embeddings with GPU\n",
        "\n",
        "* this generates a series of ELMo embeddings reading/writing to google drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oM_8ELnJq_wd"
      },
      "source": [
        "## Enabling and testing the GPU\n",
        "\n",
        "First, you'll need to enable GPUs for the notebook:\n",
        "\n",
        "- Navigate to Edit→Notebook Settings\n",
        "- select GPU from the Hardware Accelerator drop-down\n",
        "\n",
        "* To avoid saving output, check the \"Omit code cell output when saving this notebook\" option\n",
        "\n",
        "- At the moment, elmo tensorflow hub module is not compatible with tensorflow 2 so need to specifically import tensorflow 1.x"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sXnDmXR7RDr2",
        "colab": {}
      },
      "source": [
        "# \n",
        "# tf.report_tensor_allocations_upon_oom(True)\n",
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "device_name = tf.test.gpu_device_name()\n",
        "# this checks that the GPU is being used\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qm5E27tHAXpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ioGQ4DoBPHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtWDKnOjM1Ew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "printm() "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5rg56Id_JFF",
        "colab_type": "text"
      },
      "source": [
        "Mount Google Drive Folder with Collab Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "73hubzX1_G5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vwHKmzuFfjz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = tf.ConfigProto()\n",
        "# this allows memory allocation to increase over time\n",
        "config.gpu_options.allow_growth = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25lsMREA_htb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HOME_ROOT_DIR = '/content/drive/My Drive/collab_data'\n",
        "PARSED_PATVIEW_DOWNLOADS_DIR = os.path.join(HOME_ROOT_DIR, 'parsed_api_downloads')\n",
        "\n",
        "print(PARSED_PATVIEW_DOWNLOADS_DIR)\n",
        "embeds_filename = 'elmo_embeds/{}/chunk_{}.csv'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wq4BHBF__DqG",
        "colab_type": "text"
      },
      "source": [
        "Import Code that Generates Elmo EMbeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB0MAuT0_C3P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def fetch_elmo_embeds(year, chunk_number, ids, list_sentences, column_prefix):\n",
        "    # list sentences should be a list of sentences ()\n",
        "    # these should be processed by the clean for embeddings fnc and then cut to a max length\n",
        "    elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable=True)\n",
        "      # We set the trainable parameter to True when creating the module so that the 4 scalar weights (as described in the paper) can be trained.\n",
        "      # In this setting, the module still keeps all other parameters fixed.\n",
        "      # default signature takes untokenized sentences as inputs\n",
        "      # The input tensor is a string tensor with shape [batch_size]. The module tokenizes each string by splitting on spaces.\n",
        "      # output dictionary option default: a fixed  mean - pooling of all contextualized word representations with shape[batch_size, 1024]\n",
        "    embeddings = elmo(\n",
        "        list_sentences, signature=\"default\", as_dict=True)[\"default\"]\n",
        "    with tf.Session(config=config) as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        sess.run(tf.tables_initializer())\n",
        "        embeds = sess.run(embeddings)\n",
        "    tf.keras.backend.clear_session()\n",
        "    # this backend line is absolutely crucial or tf will hold on to memory allocated in previous sessions and throw OOM\n",
        "    embeds_df = pd.DataFrame(\n",
        "        index=ids, data=embeds,\n",
        "        columns=['{}_{}'.format(column_prefix, create_column_name(i)) for i in np.arange(0, 1024)])\n",
        "    print(embeds.shape)\n",
        "    embeds_df.reset_index(drop=False).to_csv(os.path.join(HOME_ROOT_DIR, embeds_filename.format(str(year), chunk_number)))\n",
        "    del embeds\n",
        "    del embeds_df\n",
        "    return None\n",
        "\n",
        "\n",
        "def create_column_name(n):\n",
        "    if len(str(n)) == 1:\n",
        "        return '000{}'.format(str(n))\n",
        "    elif len(str(n)) == 2:\n",
        "        return '00{}'.format(str(n))\n",
        "    elif len(str(n)) == 3:\n",
        "        return '0{}'.format(str(n))\n",
        "    return str(n)\n",
        "\n",
        "\n",
        "def combine_title_abstract(x):\n",
        "    # this combines title and abstract\n",
        "    # this takes in a pandas series with index is column names and values being text strings\n",
        "    txt, abstr = x.values\n",
        "    if not pd.isnull(txt) and not pd.isnull(abstr):\n",
        "        return ' . '.join([txt, abstr])\n",
        "    if not pd.isnull(abstr):\n",
        "        return abstr\n",
        "    # this should not be hit - we only look at patents with abstracts\n",
        "    return txt\n",
        "\n",
        "\n",
        "def feed_text_to_elmo(yr, column_prefix='abstract_embeds'):\n",
        "    # for each of the US patents in the dataframe, feed the cleaned abstract to ELMO\n",
        "    # the prep text for the embedding function uses the first 300 words of the abstract\n",
        "    parsed_df_format4 = 'patview_citations_df4_{}.csv'\n",
        "    input_filename = os.path.join(PARSED_PATVIEW_DOWNLOADS_DIR, parsed_df_format4.format(str(yr)))\n",
        "    print('Reading in ', input_filename)\n",
        "    df = pd.read_csv(input_filename, usecols=['title_text', 'first_claim_text_cleaned',\n",
        "                                    'abstract_text_cleaned', 'patent_id_integer'])\n",
        "    abstract_colname = 'abstract_text_cleaned'\n",
        "    # abstract_colname = 'abstract_text'\n",
        "    df = df.sort_values('patent_id_integer').set_index('patent_id_integer')\n",
        "\n",
        "    df_text = df.loc[~pd.isnull(\n",
        "        df[abstract_colname]), ['title_text', abstract_colname, ]].copy()\n",
        "    # missing_text = df.loc[~df.index.isin(df_text.index),:].copy()\n",
        "    print('Original df ', df.shape, ' abstract text shape ', df_text.shape)\n",
        "    del df\n",
        "    df_text['title_plus_abstract'] = df_text[['title_text',\n",
        "                                              abstract_colname]].apply(combine_title_abstract, axis=1)\n",
        "    print('Text df shape ', df_text.shape)                                           \n",
        "    # currently doing from embeds 200 on (because already did first 200)\n",
        "    #starting_integer = 321\n",
        "    #text_splits = np.array_split(df_text['title_plus_abstract'], 500)[:starting_integer]\n",
        "    # needed to change this because np bug on the split function \n",
        "    text_splits = np.array_split(df_text[['title_plus_abstract']].values, 500)\n",
        "    index_splits = np.array_split(df_text[['title_plus_abstract']].index, 500)\n",
        "    #i = starting_integer\n",
        "    for i, r in enumerate(zip(index_splits, text_splits)):\n",
        "      chunk_index = r[0]\n",
        "      if len(r[1].shape) > 1:\n",
        "          # select the first dimension\n",
        "          chunk_text = r[1][:, 0]\n",
        "      else:\n",
        "          chunk_text = r[1]\n",
        "      print('On iteration ', i)\n",
        "      try:\n",
        "          # we need to feed this a 1-D numpy array of text\n",
        "          fetch_elmo_embeds(yr, i, chunk_index, chunk_text.tolist(), column_prefix)\n",
        "      except tf.errors.ResourceExhaustedError as e:\n",
        "          print('Chunk number OOM Error')\n",
        "          print(printm())\n",
        "      except ValueError as z:\n",
        "        print('Value Error from Dimension issue', 'Chunk shape ', chunk.shape, 'chunk strings', chunk.values)\n",
        "      # i+=1\n",
        "    print('Done with embeddings')\n",
        "    return None\n",
        "        # for counter, mini_chunk in enumerate(np.array_split(chunk, 3)):\n",
        "        #     if counter==0:\n",
        "        #         chunk_number = '{}_a'.format(str(i))\n",
        "        #     elif counter==1:\n",
        "        #         chunk_number = '{}_b'.format(str(i))\n",
        "        #     else:\n",
        "        #         chunk_number = '{}_c'.format(str(i))\n",
        "        #     # each chunk is a series\n",
        "        #     print('On iteration ', i)\n",
        "        #     try:\n",
        "        #       fetch_elmo_embeds(yr, chunk_number, mini_chunk.index, mini_chunk.values, embeds_filename)\n",
        "        #     except tf.errors.ResourceExhaustedError as e:\n",
        "        #       print('Chunk number OOM Error')\n",
        "        #       print(printm())\n",
        "              \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjfvHCAqqAJ7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "feed_text_to_elmo(2009)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SgJ0ZYFmsJhd",
        "colab": {}
      },
      "source": [
        "tf.keras.backend.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ar7zKe9WMPCg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "parsed_df_format4 = 'patview_citations_df4_{}.csv'\n",
        "yr = 2003\n",
        "input_filename = os.path.join(PARSED_PATVIEW_DOWNLOADS_DIR, parsed_df_format4.format(str(yr)))\n",
        "df = pd.read_csv(input_filename, usecols=['title_text', 'first_claim_text_cleaned',\n",
        "                                    'abstract_text_cleaned', 'patent_id_integer'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKqzaQxIkRTe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tmQEV4iFkQvH",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtVeUEhMjtfN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}